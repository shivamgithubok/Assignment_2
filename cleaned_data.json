{
    "CDP_Documentation": "scraped from: https:segment.comdocsgettingstarted02simpleinstall a basic segment installation on this page when you implement segment, you add segment code to your website, app, or server. this code generates messages based on specific triggers you define. in a basic implementation, the code can be a snippet of javascript that you copy and paste into the html of a website to track page views. it can also be as complex as segment calls embedded in a react mobile app to send messages when the app is opened or closed, when the user performs different actions, or when time based conditions are met for example ticket reservation expired or cart abandoned after 2 hours. the best way to learn about how segment works is to see it in action. this tutorial walks you through an installation using one of segments libraries: javascript, php, or the ios library. before you begin before you start your segment implementation, you need: tip! if you dont have any of those things, consider creating a simple github pages website. create separate dev and prod sources when you develop and test sources, segment recommends you to create and use separate sources for each of your environments production, development, staging to prevent testing and development activities from filling production systems with invalid data. you can give each source an environment label when you create it, and segment strongly suggests that you use these labels to sort your sources. when you create a source during the steps below, make sure you enter an environment label. doublecheck when you enter write keys for dev and production environments to make sure that you send the right data to the right place. create a segment source to create a segment source: demo test quickstart dev find your write key the write key is a unique identifier for a source that tells segment which source the data comes from, to which workspace the data belongs, and which destinations should receive the data. to find your write key: make note of or write down your write key, as youll need it in the next steps. any time you change a librarys settings in the segment app, the write key regenerates. cloudsources do not have write keys, as they use a token or key from your account with that service. cloudsources have other considerations and arent part of this tutorial. installing segment click a tab below to see the tutorial content for the specific library you chose. step 1: copy the snippet navigate connections sources javascript in the segment app and copy the snippet from the javascript source overview page and paste it into the tag of your site. that snippet loads analytics.js onto the page asynchronously, so it wont affect your page load speed. once the snippet runs on your site, you can turn on destinations from the destinations page in your workspace and data starts loading on your site automatically. note: if you only want the most basic google analytics setup you can stop reading right now. youre done! just toggle on google analytics from the segment app. the segment snippet version history available on github. segment recommends that you use the latest snippet version whenever possible. step 2: identify users the identify method is how you tell segment who the current user is. it includes a unique user id and any optional traits you know about them. you can read more about it in the identify method reference. identify note: you dont need to call identify for anonymous visitors to your site. segment automatically assigns them an anonymousid, so just calling page and track works just fine without identify. identify anonymousid page track identify heres an example of what a basic call to identify might look like: identify analytics.identifyf4ca124298, name: michael brown, email: mbrownexample.com ; this identifies michael by his unique user id in this case, f4ca124298, which is what you know him by in your database and labels him with name and email traits. when you put that code on your site, you need to replace those hardcoded trait values with the variables that represent the details of the currently loggedin user. to do that, segment recommends that you use a backend template to inject an identify call into the footer of every page of your site where the user is logged in. that way, no matter what page the user first lands on, they will always be identified. you dont need to call identify if your unique identifier userid is not known. depending on your templating language, your actual identify call might look something like this: f4ca124298 name email identify identify userid analytics.identify user.id , name: user.fullname, email: user.email ; with that call in your page footer, you successfully identify every user that visits your site. note: if you only want to use a basic crm set up, you can stop here. just enable salesforce, intercom, or any other crm system from your segment workspace, and segment starts sending all of your user data to it. step 3: track actions the track method is how you tell segment about the actions your users are performing on your site. every action triggers what segment calls an event, which can also have associated properties. you can read more about track in the track method reference. heres an example of what a call to track might look like when a user signs up: track track track analytics.tracksigned up, plan: enterprise ; this example shows that your user triggered the signed up event and chose your hypothetical enterprise plan. properties can be anything you want to record, for example: enterprise analytics.trackarticle bookmarked, title: snow fall, subtitle: the avalanche at tunnel creek, author: john branch ; if youre just getting started, some of the events you should track are events that indicate the success of your site, like signed up, item purchased or article bookmarked. segment recommends that you track a few important events as you can always add more later. once you add a few track calls, youre done with setting up segment. you successfully installed analytics.js tracking. now youre ready to turn on any destination you like from the segment app. track step 1: install the sdk to install analyticsios, segment recommends you to use cocoapods, because it allows you to create a build with specific bundled destinations, and because it makes it simple to install and upgrade. 1 add the analytics dependency to your podfile by adding the following line: analytics podfile pod analytics, 3.0 2 in your application delegates application:didfinishlaunchingwithoptions: method, set up the sdk like so: application:didfinishlaunchingwithoptions: seganalyticsconfiguration configuration seganalyticsconfiguration configurationwithwritekey:yourwritekey; configuration.trackapplicationlifecycleevents yes; enable this to record certain application events automatically! configuration.recordscreenviews yes; enable this to record screen views automatically! seganalytics setupwithconfiguration:configuration; you dont need to use initialization config parameters to track lifecycle events application opened, application installed, application updated and screen views automatically, but segment highly recommends that you do, so you can start off already tracking some important core events. application opened application installed application updated 3 import the sdk in the files that you use it by adding the following line: import to keep the segment sdk lightweight, the analytics pod only installs the segment library. this means all of the data goes first to segments servers, and is then forwarded to any destination tools which accept the data from segment. some destinations dont accept data from the segment servers, and instead require that you collect the data from the device. in these cases you must bundle some additional destination code with the segment sdk. this document skips over this part, but you can see the instructions on how to bundle the destination tools. now that the sdk is installed and set up, youre ready to start making calls. analytics step 2: identify users the identify method is how you tell segment who the current user is. it takes a unique user id, and any optional traits you know about them. you can read more about it in the identify reference. heres an example of what a basic call to identify might look like: identify identify seganalytics sharedanalytics identify:f4ca124298 traits: name: michael brown, email: mbrownexample.com ; this call identifies michael by his unique user id f4ca124298, which is the one you know him by in your database and labels him with name and email traits. f4ca124298 name email note: when you put that code in your ios app, you need to replace those hardcoded trait values with the variables that represent the details of the currently loggedin user. step 3: track actions the track method is how you tell segment about the actions your users are performing in your app. every action triggers what we call an event, which can also have associated properties. you can read more about track in the track method reference. track track the segment ios sdk can automatically track a few important common events, such as application installed, application updated, and application opened. you can enable this option during initialization by adding the following lines: seganalyticsconfiguration configuration seganalyticsconfiguration configurationwithwritekey:yourwritekey; configuration.trackapplicationlifecycleevents yes; seganalytics setupwithconfiguration:configuration; you should also track events that indicate success in your mobile app, like signed up, item purchased, or article bookmarked. segment recommends that you track a few important events as you can always add more later. heres what a track call might look like when a user signs up: track seganalytics sharedanalytics track:signed up properties: plan: enterprise ; this tells us that your user triggered the signed up event, and chose your hypothetical enterprise plan. properties can be anything you want to record, for example: enterprise seganalytics sharedanalytics track:article bookmarked properties: title: snow fall, subtitle: the avalanche at tunnel creek, author: john branch ; once youve added a few track calls, youre all set. you successfully instrumented your app, and can enable destinations from your segment workspace. track step 1: download the library to install the library: 1 clone the repository from github into your desired application directory. if youre a composer user, you can use this. git clone https:github.comsegmentioanalyticsphp myapplicationfolders 2 add the following to your php script to load the segment analytics library in your code: requireoncepathtoanalyticsphplibsegment.php; use segmentsegment; 3 in your initialization script, make the following call in the example, segment first renames this module to analytics for convenience: analytics set up our segment tracking and alias to analytics for convenience classaliassegment, analytics; segment::inityourwritekey; 4 replace yourwritekey with the actual write key, which you can find in segment under your project settings. otherwise, all that data goes straight to devnull. you only need to call init once when your php file is requested. all of your files then have access to the same analytics client. yourwritekey devnull init analytics note: the default php consumer is the libcurl consumer. if this is not working well for you, or if you have a highvolume project, you might try one of segments other consumers like the forkcurl consumer. step 2: identify users the identify method is how you tell segment who the current user is. it includes a unique user id and any optional traits that you might know about them. heres what a basic call to identify might look like: identify identify segment::identifyarray userid f4ca124298, traits array name michael brown, email mbrownexample.com ; this identifies michael by his unique user id in this case, f4ca124298, which is what you know him by in your database and labels him with name and email traits. f4ca124298 name email note: when you actually put that code on your site, you need to replace those hardcoded trait values with the variables that represent the details of the currently loggedin user. the easiest way in php is to keep a user variable in memory. user segment::identifyarray userid userid, traits array name userfullname, email useremail ; with that call on the page, youre now identifying every user that visits your site. if you only want to use a basic crm set up, you can stop here. just enable salesforce, intercom, or any other crm system from your segment workspace, and segment starts sending all of your user data to it. step 3: track actions the track method is how you tell segment about the actions your users are performing on your site. every action triggers what segment calls an event, which can also have associated properties. heres what a call to track might look like when a user signs up: track track segment::trackarray userid f4ca124298, event signed up, properties array plan enterprise ; this tells us that the user triggered the signed up event, and chose your hypothetical enterprise plan. properties can be anything you want to record, for example: enterprise segment::trackarray userid f4ca124298, event article bookmarked, properties array title snow fall, subtitle the avalanche at tunnel creek, author john branch ; if youre just getting started, some of the events you should track are events that indicate the success of your site, like signed up, item purchased or article bookmarked. to get started, segment recommends you to track track a few important events as you can always add more later. step 4: flush the data call the segment flush method. this manually sends all the queued call data to make sure it makes it to the segment servers. this is normally done automatically by the runtime, but some php installations wont do it for you, so its worth calling at the end of your script, just to be safe. flush segment::flush; youve successfully installed php tracking. now youre ready to turn on any destination from the segment app. test that its working once youve set up your segment library, and instrumented at least one call, you can look at the debugger tab for the source to check that it produces data as you expected. the source debugger is a realtime tool that helps you confirm that api calls made from your website, mobile app, or servers arrive at your segment source, so you can quickly see how calls are received by your segment source, so you can troubleshoot quickly without having to wait for data processing. the debugger is separate from your workspaces data pipeline, and is not an exhaustive view of all the events ever sent to your segment workspace. the debugger only shows a sample of the events that the source receives in real time, with a cap of 500 events. the debugger is a great way to test specific parts of your implementation to validate that events are being fired successfully and arriving to your source. tip: to see a more complete view of all your events, you might consider setting up either a warehouse or an s3 destination. the debugger shows a live stream of sampled events arriving at the source, but you can also toggle from live to pause to stop the stream and prevent it from displaying new events. events continue to arrive to your source while you pause the stream, they just are not displayed. you can search on any information you know is available in an event payload to search in the debugger and show only matching payloads. you can also use advanced search options to limit the results to a specific event. two views are available when viewing a payload: set up your first destination once youre satisfied that data is arriving from your new source, its time to set up your first destination! as long as you have page or screen data coming from the source, you can quickly enable google analytics to look at the page view statistics. page screen if you dont have a google analytics account, you can either set up a free account, or look at the destination catalog for a different destination to enable. youll need a tracking id for google analytics either a website or serverside tracking id, or another api key if youre substituting another destination. make a note of this id or key as youll need it to connect your destination. to set up your first destination: google analytics congratulations! data is now flowing from the source you set up, to the first destination. do some test browsing on your site or app, then log in to your downstream tool to see the data in place. you can click around and load pages to see your segment calls in action, watch them arrive in the debugger, and see them arrive in the destination tool. note: when youre done with this test source and destination, you can delete them. this prevents you from getting unplanned demo data in your production environment later. the basics of the segment platform and what you can do with it. think through your goals, plan your calls, and set yourself up for success. this page was last modified: 13 aug 2024 need support? questions? problems? need more info? contact segment support for assistance! help improve these docs! was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! get started with segment on this page was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! product for developers company support 2025 segment.io, inc. scraped from: https:segment.comdocsgettingstarted03planningfullinstall planning a full installation on this page now that youve seen segment in action, step back and think through what a full implementation of segment for your organization would look like. figuring out what events to track in segment can feel overwhelming. you should expect this planning process to have the following steps: be prepared to invest time deciding with stakeholders how to track your data, and planning how youll analyze it. the time you spend here will save you lots of time in the future, as following segments best practices allows you to easily change your tracking later. define business objectives tracking is about learning and taking action. think about what you want to know about your product or customers. think about what assumptions need to be tested and what theories need to be proven true or false. think about the unknowns. here are some helpful questions to get started: while it may seem obvious, we highly recommend documenting your highlevel business objectives. more specifically, ask yourself: what are the measurable business outcomes you want to achieve this year? do you want to acquire new customers? generate more new signups, drive more incremental revenue among your current customer base? the best way to answer this question is to interview stakeholders in your organization who will consume the data. with your business goals documented, the next step is to map user actions to those business goals. for example, if one of your goals is to activate new signups, you want to think about which activities are related to a signup. ask yourself, what actions do people take before signing up? do specific actions predict a user signing up? as an example, you might end up with a list like this: while this list represents a tiny fraction of the user actions you could track, it gives a list focused on your top business objectives. this helps break up the huge project of data collection into smaller chunks. decide what to collect with your business objectives documented and mapped to user actions, its time to build standards that you can use when deciding what to track. with your stakeholders, make a list of the actual events page or screen views, and user actions that you want to track. think about all of the ways your users can interact with your site or app when youre first starting out, we recommend that you limit your tracking plan to a few core events, but add lots of properties to provide context about them. we generally see more success with the less is more philosophy of tracking data, but you might also decide to take a more liberal track more and analyze later approach. like everything, each alternative has pros and cons that are important to consider especially as it relates to your companys needs. shortcut: check if a business spec meets your needs segment maintains several business specs, which are recommendations based on your type of business that give recommendations on what to track, what additional traits or properties to collect, and how to format them. the two most common are the b2b businesstobusiness spec, ecommerce spec, and mobile and video specs. if these specs meet your business needs, youre in luck. these specs are built into segment tracking plan templates, so you dont need to start from a blank slate. if your organization sells a product or services to other businesses, you might have different analytics and marketing needs than most companies. you need to understand your customer behaviors both at the userlevel, and also at the company or teamlevel. you can read more about how segment thinks about b2b tracking, and read more about the b2b spec. if your organization sells products online, the ecommerce spec covers the customers journey as they browse your store, click on promotions, view products, add those products to a cart, and complete a purchase. it also provides recommendations about offpage interactions, including interactions with email promotions, coupons, and other systems. you can read more about why companies need an ecommerce spec, read more about ecommerce tracking plans, and dive directly into our ecommerce spec. the native mobile spec is a common blueprint for the mobile user lifecycle. the spec outlines the most important events for mobile apps to track, and automatically collects many of these events when you use the segment android and ios sdks. read more about the benefits of the native mobile spec, or read through the native mobile spec directly. segments video spec helps you understand how customers engage with your video and ad content, including playback events, types of media displayed, and performance metrics. you can read more about our video spec. create naming conventions regardless of approach, here are some important best practices to keep in mind: pick a casing convention: we recommend title case for event names and snakecase for property names. make sure you pick a casing standard and enforce it across your events and properties. pick an event name structure: as you may have noticed from our specs, were big fans of the object blog post action read framework for event names. pick a convention and stick to it. blog post read dont create event names dynamically: avoid creating events that pull a dynamic value into the event name for example, user signed up 11012019. if and when you send these to a warehouse for analysis, you end up with huge numbers of tables and schema bloat. user signed up 11012019 dont create events to track properties: avoid adding values to event names when they could be a property. instead, add values as a property. for example, rather than having an event called read blog post best tracking plans ever, create a blog post read event and with a property like blogposttitle:best tracking plans ever. blogposttitle:best tracking plans ever dont create property keys dynamically: avoid creating property names like feature1:true,feature2:false as these are ambiguous and very difficult to analyze feature1:true feature2:false got all that? great. youre now ready to develop a tracking plan. develop a tracking plan a tracking plan clarifies what events to track, where those events live in the code base, and why youre tracking those events from a business perspective. a good tracking plan represents the single source of truth about what data you collect, and why. your tracking plan is probably maintained in a spreadsheet unless you use segments trackingplan tool, protocols, and serves as a project management tool to get your organization in agreement about what data to use to make decisions. a tracking plan helps build a shared understanding of the data among marketers, product managers, engineers, analysts, and any other data users. plan your identify and group calls the identify call updates all records of the user with a set of traits, and so is extremely important for building your understanding of your users. but how do you choose which traits to include? the example below shows an identify call using analytics.js for segment: analytics.identify name: jane kim, email: janekimexample.com, login: janekay, type: user, created: 20161107t16:40:52.238z, ; the traits represent dimensions in your data that you can group or pivot on. for example, in the above, you can easily create cohorts of all types that are users or accounts created within a time window of your choosing. users when you plan your deployment, think about what information you can collect as traits that would be useful to you when grouping users together, and plan how you will collect that information. the group call is similar to the identify call, but it adds traits associated with a parent account to the users profile. if your organization is a b2b company, you should also plan the group traits to collect, and how youll use them once theyre applied to a user account. plan your track events we recommend starting with fewer events that are directly tied to one of your business objectives, to help avoid becoming overwhelmed by endless number of possible actions to track. as you get more comfortable, you can add more events to your tracking plan that can answer more specialized questions. at segment, we started out tracking these events: then we added some peripheral events to to better understand how were performing, for the following reasons: for an ecommerce company, however, the main events might be something like: tip: as mentioned above, segment has a set of reserved event names specifically for ecommerce, called the ecommerce spec. check it out to see which events segment covers and how they are used in downstream destinations. an online community, on the other hand, has an entirely different set of actions that indicate engagement, as listed below. for example, a community might want to track actions like: with these actions tracked, the community can develop metrics around engagement, and understand how users move towards their ultimate conversion events. you can read more in this article from the online community growthhackers about the events they track and why. define your track event properties each track call can accept an optional dictionary of properties, which can contain any keyvalue pair. these properties act as dimensions that allow destination tools to group, filter, and analyze the events. they give you additional detail on broader events. events should be generic and highlevel, but properties should be specific and detailed. for example, at segment, business tier workspace created is a horrible event name. instead, we used workspace created with a property of accounttier and value of business : business tier workspace created workspace created property accounttier business analytics.trackworkspace created, accounttier: business similar to the traits in the identify call, the properties provide a column that you can pivot against or filter on in your analytics tools or allow you to create a cohort of users in email tools. dont create dynamically generated property names in the properties dictionary. each key creates a new column in your downstream tools, and dynamically generated keys clutter your tools with fragmented data that makes it difficult and confusing to use later. key here is segments lead captured track call: lead captured analytics.trackuserid, lead captured, email: email, location: header navbar url: https:segment.com ; the highlevel event is lead captured, and all of the details appear in the properties dictionary. because of this, we can easily see in our downstream tools how many leads were captured, and from which parts of the site. if you want to learn more about how properties are used by downstream tools, check out the anatomy of a track call. plan for destination tools once youve completed your tracking plan, theres one more step you might want to do before you move on to actually implementing segment. the segment destination catalog contains hundreds of tools, many of which youll be familiar with already. if your organization has an established set of analytics tools, look for those tools in the catalog and bookmark their documentation pages. these docs pages contain important information about how segment transforms data for the destination tool, and they also contain useful details about troubleshooting, setup, and implementation considerations. once you have an initial list of the destination tools your organization uses, you can also check which segment methods those tools accept. this helps you at implementation time to ensure that the calls you use can be consumed by the tools theyre intended for. additionally, you should check which connection modes each tool supports, so you know ahead of time which destinations may need to be bundled. tip: if you know youre looking for a tool for a specific purpose, but havent chosen one yet, you can also check the connection modes by category page to see which tools might be compatible with the least implementation changes. walk through a disposable, demo implementation. take your plans, and make them real. this page was last modified: 30 mar 2023 need support? questions? problems? need more info? contact segment support for assistance! help improve these docs! was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! get started with segment on this page was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! product for developers company support 2025 segment.io, inc. scraped from: https:segment.comdocsgettingstarted04fullinstall a full segment implementation on this page related content before you start implementing from your tracking plan, lets review the segment methods, what they do, and when you should use each. segment methods in detail segments libraries generate and send messages to our tracking api in json format, and provide a standard structure for the basic api calls. we also provide recommended json structure also known as a schema, or spec that helps keep the most important parts of your data consistent, while allowing great flexibility in what other information you collect and where. there are six calls in the basic tracking api, which answer specific questions: among these calls, you can think of identify, group, and alias as similar types of calls, all to do with updating our understanding of the user who is triggering segment messages. you can think of these calls as adding information to, or updating an object record in a database. objects are described using traits, which you can collect as part of your calls. the other three, track, page, and screen, can be considered as increasingly specific types of events. events can occur multiple times, but generate separate records which append to a list, instead of being updated over time. a track call is the most basic type of call, and can represent any type of event. page and screen are similar and are triggered by a user viewing a page or screen, however page calls can come from both web and mobileweb views, while screen calls only occur on mobile devices. because of the difference in platform, the context information collected is very different between the two types of calls. segment recommends that you always use the page and screen calls when recording a pageview, rather than creating a page viewed track event, because the pagescreen calls automatically collect more contextual information. anatomy of a segment message the most basic segment message requires only a userid or anonymousid; all other fields are optional to allow for maximum flexibility. however, a normal segment message has three main parts: the common fields, the context object, and the properties if its an event or traits if its an object. userid anonymousid the common fields include information specific to how the call was generated, like the timestamp and library name and version. the fields in the context object are usually generated by the library, and include information about the environment in which the call was generated: page path, user agent, os, locale settings, etc. the properties and traits are optional and are where you customize the information you want to collect for your implementation. another common part of a segment message is the integrations object, which you can use to explicitly filter which destinations the call is forwarded to. however this object is optional, and is often omitted in favor of noncode based filtering options. integrations identify calls analytics.identify userid: 12345abcde, traits: email: michael.phillipssegment.com, name: michael phillips, city: new york, state: ny, internal: true the identify call allows segment to know who is triggering an event. when to call identify call identify when the user first provides identifying information about themselves usually during log in, or when they update their profile information. when called as part of the login experience, you should call identify as soon as possible after the user logs in. when possible, follow the identify call with a track event that records what caused the user to be identified. when you make an identify call as part of a profile update, you only need to send the changed information to segment. you can send all profile info on every identify call if that makes implementation easier, but this is optional. learn more best practices for identifying users traits in identify calls these are called traits for identify calls, and properties for all other methods. the most important trait to pass as part of the identify call is userid, which uniquely identifies a user across all applications. you should use a hash value to ensure uniqueness, although other values are acceptable; for example, email address isnt the best thing to use as a userid, but is usually acceptable since it will be unique, and doesnt change often. beyond that, the identify call is your opportunity to provide information about the user that can be used for future reporting, so you should try to send any fields that you might want to report on later. consider using identify and traits when: how to call identify you can call identify from any of segments devicebased or serverbased libraries, including javascript, ios, android, ruby, and python. here are two examples of calling identify from two different libraries: analytics.identify12345abcde, email: michael.phillipssegment.com, name: michael phillips, city: new york, state: ny, internal: true ; analytics.identify userid: 12345abcde, traits: email: michael.phillipssegment.com, name: michael phillips, city: new york, state: ny, internal: true using analytics.reset when a user explicitly signs out of one of your applications, you can call analytics.reset to stop logging further event activity to that user, and create a new anonymousid for subsequent activity until the user logins in again and is subsequently identifyed. this call is most relevant for clientside segment libraries, as it clears cookies in the users browser. analytics.reset anonymousid make a reset call as soon as possible after signout occurs, and only after it succeeds not immediately when the user clicks sign out. for more info on this call, see the javascript source documentation. reset page and screen the page and screen calls tell segment what web page or mobile screen the user is on. this call automatically captures important context traits, so you dont have to manually implement and send this data. page and screen call properties you can always override the autocollected pagescreen properties with your own, and set additional custom page or screen properties. some downstream tools like marketo require that you attach specific properties like email address to every page call. this is considered a destinationspecific implementation nuance, and you should check the documentation for each destination you plan to use and make a list of these nuances before you start implementation. named page screen calls you can specify a page name at the start of the page or screen call, which is especially useful to make list of page names into something more succinct for analytics. for example, on an ecommerce site you might want to call analytics.page product and then provide properties for that product: analytics.page product analytics.pageproduct, category: smartwatches, sku: 13d31 ; seganalytics sharedanalytics screen:product properties: category: smartwatches, sku: 13d31 ; ; when to call page segment automatically calls a page event whenever a web page loads. this might be enough for most of your needs, but if you change the url path without reloading the page, for example in single page web apps, you must call page manually . if the presentation of user interface components dont substantially change the users context for example, if a menu is displayed, search results are sortedfiltered, or an information panel is displayed on the exiting ui measure the event with a track call, not a page call. note: when you trigger a page call manually, make sure the call happens after the ui element is successfully displayed, not when it is called. it shouldnt be called as part of the click event that initiates it. for more info on page calls, review page spec and analytics.js docs. when to call screen segment screen calls are essentially the page method, except for mobile apps. mobile screen calls are treated similarly to standard page tracking, only they contain more context traits about the device. the goal is to have as much consistency between web and mobile as is feasible. track calls the track call allows segment to know what the user is doing. when to call track the track call is used to track user and system events, such as: events and properties your track calls should include both events and properties. events are the actions you want to track, and properties are the data about the event that are sent with each event. properties are powerful. they enable you to capture as much context about the event as youd like, and then crosstabulate or filter your downstream tools. for example, lets say an elearning website is tracking whenever a user bookmarks an educational article on a page. heres what a robust analytics.js track call could look like: analytics.trackarticle bookmarked, title: how to create a tracking plan, course: intro to data strategy, author: dr. anna lytics, publishyear: 2019, publishmonth: 03, length: medium 10002000 words, assets: infographics,interactive charts, topics: data planning,segment,data flow, buttonlocation: subheader 3rd column ; with this track call, we can analyze which authors had the most popular articles, which months and years led to the greatest volume of bookmarking overall, which button locations drive the most bookmark clicks, or which users gravitate towards infographics related to data planning. event naming best practices each event you track must have a name that describes the event, like article bookmarked above. that name is passed in at the beginning of the track call, and should be standardized across all your properties so you can compare the same actions on different properties. segments best practice is to use an object action nounverb naming convention for all track events, for example, article bookmarked. segment maintains a set of business specs which follow this naming convention around different use cases such as ecommerce, b2b saas, and mobile. lets dive deeper into the object action syntax that all segment track events should use. objects are nouns nouns are the entities or objects that the user or the system acts upon. its important to be thoughtful when naming objects so that they are referred to consistently within an application, and so that you refer to the same objects that might exist in multiple applications or sites by the same name. use the following list of objects to see if there is a logical match with your application. if you have objects that arent in this list, name it in a way that makes sense if it were to appear in other applications, andor run it by product analytics. actions are verbs verbs indicate the action taken by either a user on your site. when you name a new track event, consider if you can describe the current interaction using a verb from the list below. if you cant, choose a verb that describes what the user is trying to do in your specific case, but try to be flexible enough so that you could use it in other scenarios. property naming best practices segment recommends that you record property names using snake case for example propertyname, and that you format property values to match how they are captured. for example, a username value would be captured in whatever case it the user typed it in as. propertyname username ultimately, you can decide to use a casing different from our recommendations; however, the single most important aspect is that youre consistent across your entire tracking with one casing method. you can read more about best practices for track calls, . all of the basic segment methods have a common structure and common fields which are automatically collected on every call. you can see these in the common fields documentation. common properties to send with a track call the following properties should be sent with every track call: initiator displayformat totalresultcount totalitemspages itemsperpage currentitempage destinationurl sortcolumn sortdirection how to call track you can make a track call from any of segments clientside or serverside libraries, including javascript, ios, android, ruby, and python. here are two examples of calling track from two different libraries: analytics.trackarticle bookmarked, title: how to create a tracking plan, course: intro to data strategy, author: dr. anna lytics, ; analytics.track userid: 12345abcde, event: article bookmarked, properties: title: how to create a tracking plan, course: intro to data strategy, author: dr. anna lytics think through your goals, plan your calls, and set yourself up for success. unlock the power of segment with destinations. this page was last modified: 07 nov 2023 further reading need support? questions? problems? need more info? contact segment support for assistance! help improve these docs! was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! get started with segment on this page related content was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! product for developers company support 2025 segment.io, inc. scraped from: https:segment.comdocsgettingstarted05datatodestinations sending data to destinations on this page once youve got data flowing into segment, what do you do with it? the segment destination catalog lists all of the places we can send your data. routing data to destinations when you enable a destination in the segment app, you link it to a specific source or sources. by default, segment first processes the data from the selected sources, then translates it and routes it from the segment servers to the api endpoint for that destination. this means that if you previously had loaded code or a snippet for that tool on your website or app, you should remove it once you have segment implemented so you dont send duplicate data. you might also want to enable tools that need to be loaded on the users device either a computer or mobile device in order to function properly. for our analytics.js library, you can make these changes from the segment app, and the segment systems then update the bundle of code served when users request the page to include code required by the destination. you can read more about this in our documentation on connection modes. adding new destinations adding a destination is quick and easy from the segment app. youll need a token or api key for the tool, or some way to confirm your account in the tool. recommended destinations if youre just starting out, we know the catalog can be really overwhelming. how do you choose from all of the available destinations? weve written a lot about how to choose your tools, but as a start, we recommend that you have one tool from each of the following categories: if youre adding more destinations after youve done your segment instrumentation, you might want to check that the destinations you choose can accept the methods youre already using, and that they can use the connection modes youre already using. we also feel that its really important to have a data warehouse, so you can get a clearer view of all of your data for analytics purposes. more on that just below. adding a warehouse warehouses are a special type of destination which receive streaming data from your segment sources, and store it in a table schema based on your segment calls. this allows you to do a lot of interesting analytics work to answer your own questions about what your users are doing and why. all customers can connect a data warehouse to segment. free and team customers can connect one, while business customers can connect as many as needed. you should spend a bit of time considering the benefits and tradeoffs of the warehouse options, and then choose one from our warehouse catalog. when you choose a warehouse, you can then use the steps in the documentation to connect it. this may require that you create a new dedicated user or service user to allow segment to access the database. once your warehouse is configured and running, you can connect to it using a business intelligence bi tool such as looker, mode, tableau, or others to analyze your data indepth. there are also a number of business tier features you can then use with your warehouse, including selective sync and replay. check out our course on warehouses in segment university. must be logged in to access. take your plans, and make them real. test your implementation and see where your data is and isnt arriving. this page was last modified: 09 aug 2022 need support? questions? problems? need more info? contact segment support for assistance! help improve these docs! was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! get started with segment on this page was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! product for developers company support 2025 segment.io, inc. scraped from: https:segment.comdocsgettingstarted06testingdebugging testing and debugging on this page one of the most important questions youll ask early on is how do i know if segment is working? there are several ways to check if your data is flowing. one is the debugger tab in each source in the segment web app, where you can see data coming from a source into segment. another is the event delivery tool which shows which data is arriving at specific destinations. for monitoring purposes, youll also see alerts in the workspace health tool if your sources or destinations produce repeated errors. want more? check out our course on debugging and troubleshooting. must be logged in to access. the source debugger the source debugger is a realtime tool that helps you confirm that api calls made from your website, mobile app, or servers arrive to your segment source, so you can troubleshoot your segment set up even quicker. with the debugger, you can check that youre sending calls in the expected format, without having to wait for any data processing. the debugger is separate from your workspaces data pipeline and is not an exhaustive view of all the events ever sent to your segment workspace. the debugger only shows a sample of the events that the source receives in real time, with a cap of 500 events. the debugger is a great way to test specific parts of your implementation to validate that events are being fired successfully and arriving to your source. to see a more complete view of all your events, we recommend that you set up either a warehouse or an s3 destination. the debugger shows a live stream of sampled events arriving into the source, but you can also pause the stream from displaying new events by toggling live to pause. events continue to arrive to your source while you pause the stream. you can search in the debugger to find a specific payload using any information you know is available in the events raw payload. you can also use advanced search options to limit the results to a specific event. two views are available when viewing a payload: event delivery the event delivery tool helps you see if segment is encountering issues delivering your data from your sources to their connected destinations. segment sends billions of events to destinations every week. if our systems encounter errors when trying to deliver your data, we report them in the event delivery tool. here is an example of what the event delivery tool looks like: event delivery is most useful when: you can access the event delivery tool from the destination settings tab in any supported destination. event delivery is only available for cloudmode destinations, which receive data through the segment servers. devicemode destinations receive data through an api endpoint outside the segment servers, where we cannot monitor or report on it. event delivery is not available for warehouses or amazon s3 destinations. using event delivery the ui shows three parts that report on segments ability to deliver your source data: key metrics, error details, and delivery trends. before you begin, select a time period from the drop down menu at the right. the event delivery display updates to show only information about your selected time period. this panel displays quantitative information about the destinations data flow: delivered: the number of messages segment successfully delivered to the destination in the selected time period. not delivered: the number of messages segment was unable to deliver. if this number is greater than zero, the reasons for these failures appear in the errors table below. p95 latency: the time it takes for segment to deliver the slowest 5 of your data known as p95 latency. the latency reported is endtoend: from the event being received through the segment api, to the event being delivered to partner api. this helps tell you if there is a delay in your data pipeline, and how severe it is. the error details table displays a summary of the errors in a given period, and the most important information about them. you can click any row in the table to expand it to show more information. the error details view gives you as much information as possible to help you resolve the issue. the example below shows an example error details panel. this view includes: you sent the data you sent to segments api. request to destination the request segment made to the partner api. this payload will likely be different from what you sent it because segment is mapping your event to the partners spec to ensure the message is successfully delivered. response from destination the response segment received from the partner api. this will have the raw partner error. if you need to troubleshoot an issue with a partners success team, this is usually something theyll want to see. view segments list of integration error codes for more information about what might cause an error. trends when debugging, its helpful to see when issues start, stop and how they trend over time. the event delivery view shows a graph with the following information: delivered: the number of events that were successfully delivered in the time period you selected. not delivered: the number of events that were not successfully delivered in the time period you selected. the latency view shows the endtoend p95 latency during the time period you selected. unlock the power of segment with destinations learn about what you can do next with segment this page was last modified: 06 jul 2022 need support? questions? problems? need more info? contact segment support for assistance! help improve these docs! was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! get started with segment on this page was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! product for developers company support 2025 segment.io, inc. scraped from: https:segment.comdocsgettingstartedwhatsnext whats next on this page youre just getting started with segment, but theres so much more to explore! privacy tools and filtering segment includes a free suite of privacy tools to help your organization comply with regulations like the gdpr and the ccpa. the privacy portal allows you to easily audit, monitor, and enforce privacy rules against your segment data, to proactively protect your customers. improve data quality with protocols you had a taste of the planning needed to set up clear, consistent, reliable and extensible data schemas in planning a full install. business tier customers can use segments protocols package to help with this process, to keep track of what data is being collected where, and to normalize their data as it flows through segment. clean, consistent data helps you move faster to build marketing campaigns and act on analytics insights. with protocols, you can use tracking plans to build consensus in your organization about which events and property you intend to collect across your web, mobile or serverside data sources. once defined, you can connect the tracking plan to your sources to automatically validate the data is flowing correctly. you can also turn on enforcement to block bad data, and even fix incorrect data with transformations. single view of the customer with engage engage is a powerful personalization platform that enables you to create unified customer profiles in segment, to build and enrich audiences, and to activate audiences across marketing tools. with engage, you can create unified customer profiles, enrich those profiles with new traits, build audiences using those profiles, and sync audiences to marketing tools to power personalized experiences, and better understand and market to your customers. more learning resources segment university is segments free, online classroom for learning the basics of segment. analytics academy is a series of lessons designed to help you understand the value of analytics as a discipline, and to help you think through your analytics needs, and get started creating robust and flexible analytics systems to help you grow. need ideas or prior art? segment recipes are some cool things you can do by hooking your segment workspace up to different destination tools. everything from sending tailored onboarding emails, to joining and cleaning your data with third party tools other resources still hungry for more? check out our list of other segment resources! technical support if youre experiencing problems, have questions about implementing segment, or want to report a bug, you can fill out our support contact form here and our product support engineers will get back to you. you need a segment.com account in order to file a support request. dont worry! you can always sign up for a free workspace if you dont already have one. back to the getting started index this page was last modified: 27 sep 2022 need support? questions? problems? need more info? contact segment support for assistance! help improve these docs! was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! get started with segment on this page was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! product for developers company support 2025 segment.io, inc. scraped from: https:segment.comdocsguides an introduction to segment on this page welcome! this page is a highlevel introduction to the segment platform, including what it does and how. if youre looking for detailed information about architecture, setup, or maintenance, you can skip ahead. what is segment? segment is a customer data platform cdp, which means that it provide a service that simplifies collecting and using data from the users of your digital properties websites, apps, etc. with segment, you can collect, transform, send, and archive your firstparty customer data. segment simplifies the process of collecting data and hooking up new tools, allowing you to spend more time using your data, and less time trying to collect it. you can also enrich the customer data you collect by connecting data from your other tools, and then aggregate it to monitor performance, inform decisionmaking processes, and create uniquely customized user experiences. you can also use unify, segments identity resolution tool, to unify data from individual users to gain a wholistic understanding of their actions. check out how to get started with segment in segment university! must be logged in to access. what does it do? in its very simplest form, segment generates messages about whats happening in your site or app, then translates the content of those messages into different formats for use by other tools called destinations, and transmits messages to those tools. the segment servers also archive a copy of the data, and can send data to your storage systems such as databases, warehouses, or bulkstorage buckets. how does segment work? segments libraries generate and send messages to the tracking api in json format. segment provides a standard structure for the basic api calls, along with a recommended json structure also known as the spec, a type of schema that helps keep the most important parts of your data consistent, while allowing great flexibility in what other information you collect and where. segment messages when you implement segment, you add the segment code to your website, app, or server, which generates messages based on specific triggers you define. at its very simplest, this code can be a snippet that you copy and paste into the html of a website to track page views. it can also be as complex as segment calls embedded in a react mobile app to send messages when the app is opened or closed, when the user performs different actions, or when time based conditions are met for example ticket reservation expired or cart abandoned after 2 hours. segment has sources and destinations. sources send messages into segment and other tools, while destinations receive messages from segment. anatomy of a segment message the most basic segment message requires only a userid or anonymousid; all other fields are optional to allow for maximum flexibility. however, a normal segment message has three main parts: the common fields, the context object, and the properties if its an event or traits if its an object. userid anonymousid the common fields include information specific to how the call was generated, like the timestamp and library name and version. the fields in the context object are usually generated by the library, and include information about the environment in which the call was generated: page path, user agent, os, locale settings, etc. the properties and traits are optional and are where you customize the information you want to collect for your implementation. another common part of a segment message is the integrations object, which you can use to explicitly filter which destinations the call is forwarded to. however this object is optional, and is often omitted in favor of noncode based filtering options. integrations segment sources segment provides several types of sources which you can use to collect your data, and which you can choose among based on the needs of your app or site. for websites, you can embed a library which loads on the page to create the segment messages. if you have a mobile app, you can embed one of segments mobile libraries, and if youd like to create messages directly on a server if you have, for example a dedicated .net server that processes payments, there are several serverbased libraries that you can embed directly into your backend code. you can also use cloudsources to import data about your app or site from other tools like zendesk or salesforce, to enrich the data sent through segment. destinations once segment generates the messages, it can send them directly to the segment servers for translation and forwarding on to the destinations youre using, or it can make calls directly from the app or site to the apis of your destination tools. which of these methods you choose depends on which destinations youre using and other factors. you can read more about these considerations in our connection modes documentation what happens next? messages sent to the segment servers using the tracking api can then be translated and forwarded on to destination tools, inspected to make sure that theyre in the correct format or schema, inspected to make sure they dont contain any personally identifying information pii, aggregated to illustrate overall performance or metrics, and archived for later analysis and reuse. what are the other parts of the segment platform? in addition to connections our core message routing product segment offers additional features to help your organization do more with its data, and keep data clean, consistent, and respectful of enduser privacy. the following products are available: where can i learn more? im a segment developer im a segment data user im a segment workspace administrator whats a workspace? a workspace is a group of sources that can be administered and billed together. workspaces help companies manage access for multiple users and data sources. workspaces let you collaborate with team members, add permissions, and share sources across your whole team using a shared billing account. when you first log in to your segment account, you can create a new workspace, or choose to log into an existing workspace if your account is part of an existing organization. whats a source? in segment, you create a source or more than one! for each website or app you want to track. we highly recommend creating a source for each unique source of data each site, app, or server, though this isnt required. sources belong to a workspace, and the url for a source looks something like this: https:segment.comsources https:segment.comsources you can create new sources using the button in the workspace view. each source you create has a write key, which is used to send data to that source. for example, to load analytics.js, the segment javascript library on your page, the snippet on the quickstart guide includes: analytics.js analytics.loadyourwritekey; whats a destination? destinations are business tools or apps that you can connect to the data flowing through segment. some of segments most popular destinations are google analytics, mixpanel, kissmetrics, customer.io, intercom, and keenio. all of these tools run on the same data: who are your customers and what are they doing? but each tool requires that you send that data in a slightly different format, which means that youd have to write code to track all of this information, again and again, for each tool, on each page of your app or website. enter segment. do it once. segment eliminates this process by introducing an abstraction layer. you send your data to segment, and segment understands how to translate it so we can send it along to any destination. you enable destinations from the catalog in the segment app, and user data immediately starts flowing into those tools. no extra code required! segment supports many categories of destinations, from advertising to marketing, email to customer support, crm to user testing, and even data warehouses. you can view a complete list of available destinations or check out the destination page for a searchable list broken down by category. whats a warehouse? a warehouse is a central repository of data collected from one or more sources. this is what commonly comes to mind when you think about a relational database: structured data that fits neatly into rows and columns. in segment, a warehouse is a special type of destination. instead of streaming data to the destination all the time, we load data to them in bulk at regular intervals. when we load data, we insert and update events and objects, and automatically adjust their schema to fit the data youve sent to segment. this page was last modified: 28 mar 2023 need support? questions? problems? need more info? contact segment support for assistance! help improve these docs! was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! get started with segment on this page was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! product for developers company support 2025 segment.io, inc. scraped from: https:segment.comdocsguidesintroimpl segment for developers on this page this guide explains all you need to know to get started with your segment implementation, and directs you to more resources depending on your specific needs. if you havent already, you should read the detailed explanation of segment on the previous page! see a quick example of segment working on an ecommerce website. must be logged in to access. what does segment do? segment sends messages about activities in your mobile apps, websites or servers, receives those messages, and translates and forwards the message content to destination tools. it also can send the contents of those messages to a bulk storage destination for archiving. in more complicated implementations, segment can serve as a wrapper to trigger messages directly to other apis, and can inspect, correct, classify and block the message contents. types of segment messages segments libraries generate and send messages to our tracking api in json format, and provide a standard structure for the basic api calls. we also provide recommended json structure also known as a schema, or spec that helps keep the most important parts of your data consistent, while allowing great flexibility in what other information you collect and where. there are six calls in the basic tracking api, which answer specific questions: among these calls, you can think of identify, group, and alias as similar types of calls, all to do with updating our understanding of the user who is triggering segment messages. you can think of these calls as adding information to, or updating an object record in a database. objects are described using traits, which you can collect as part of your calls. the other three, track, page, and screen, can be considered as increasingly specific types of events. events can occur multiple times, but generate separate records which append to a list, instead of being updated over time. a track call is the most basic type of call, and can represent any type of event. page and screen are similar and are triggered by a user viewing a page or screen, however page calls can come from both web and mobileweb views, while screen calls only occur on mobile devices. because of the difference in platform, the context information collected is very different between the two types of calls. tip! segment recommends that you always use the page and screen calls when recording a pageview, rather than creating a page viewed event, because the pagescreen calls automatically collect much better context information. anatomy of a segment message the most basic segment message requires only a userid or anonymousid; all other fields are optional to allow for maximum flexibility. however, a normal segment message has three main parts: the common fields, the context object, and the properties if its an event or traits if its an object. userid anonymousid the common fields include information specific to how the call was generated, like the timestamp and library name and version. the fields in the context object are usually generated by the library, and include information about the environment in which the call was generated: page path, user agent, os, locale settings, etc. the properties and traits are optional and are where you customize the information you want to collect for your implementation. another common part of a segment message is the integrations object, which you can use to explicitly filter which destinations the call is forwarded to. however this object is optional, and is often omitted in favor of noncode based filtering options. integrations message schemas, blocks, and specs the segment specs provide recommended message schemas the information we recommend that you collect for each type of call. these are recommendations not requirements, but if you follow these schema guidelines the segment servers can more easily identify parts of your messages, and translate them to downstream tools. in addition to the recommended message schemas, segment also provides blocks: recommendations on what information to collect and how to format it, for different industries and use cases. these are recommendations only, but by collecting all of the information in these blocks, you can ensure that common tools used in that usecase have the information they need to function. a third section of the spec is the industry specs which provide recommendations that include an explicit translation or mapping in the segment servers, to best power the downstream destinations commonly used in these industries. sources and destinations when you start out, you create a workspace, which serves as a container for all of your sources and destinations. segment has sources and destinations. sources send data into segment, while destinations receive data from segment. segment has five types of sources: web analytics.js, mobile, server, and cloud app, plus a fifth type: usercreated source functions. web, mobile, and server sources send firstparty data from your digital properties. cloudapp sources send data about your users from your connected web apps, for example a ticketing system such as zendesk, a payments system such as stripe, or a marketing tool like braze. connection modes segment has several types of sources, and many destinations can accept data from all of them. however, some are only compatible with specific source types for example, web only, or server only. to find out which source types a specific destination can accept data from, check the documentation for that destination for a supported sources and connection modes section. segments web source analytics.js, and native clientside libraries ios, android, reactnative allow you to choose how you send data to segment from your website or app. there are two ways to send data: cloudmode: the sources send data directly to the segment servers, which then translate it for each connected downstream destination, and send it on. translation is done on the segment servers, keeping your page size, method count, and load time small. healthcare and life sciences hls customers can encrypt data flowing into their destinations hls customers with a hipaa eligible workspace can encrypt data in fields marked as yellow in the privacy portal before they flow into an event stream, cloudmode destination. to learn more about data encryption, see the hipaa eligible segment documentation devicemode: you include additional code on your website or mobile app which allows segment to use the data you collect on the device to make calls directly to the destination tools api, without sending it to the segment servers first. you still send your data to the segment servers, but this occurs asynchronously. this is also called wrapping or bundling, and it might be required when the source has to be loaded on the page to work, or loaded directly on the device to function correctly. when you use analytics.js, you can change the devicemode destinations that a specific source sends from within the segment web app, without touching any code. if you use server source libraries, they only send data directly to segment in cloudmode. server library implementations operate in the server backend, and cant load additional destination sdks. to learn more about connection modes and when you should use each, see the details in the destinations docs. planning your segment implementation the journey of a thousand miles begins, ideally, with a plan. regardless of if youre a new company just implementing analytics for the first time, or a multinational corporation modernizing your analytics stack, its a great idea to start with a tracking plan. for new implementations, this can be as simple as a document where you write down these four things for each item you track: if youre a large or longestablished organization and youre replacing existing tools, youll want to spend more time on this to maintain analytic parity and continuity of tooling. we highly recommend reading up on tracking plans and schemas for protocols, our tool for managing and sharing tracking plans and enforcing schemas. regardless of your organizations size or age, youll want to take an inventory of the destination tools youll be using with segment, and make a list of the connection modes each one accepts. this makes it easier to check off when youve implemented each one, so youre not missing anything. how do i test if its working? there are several ways to check if your data is flowing. one is the debugger tab in each source in the segment web app, where you can see data coming from a source into segment. another is the event delivery tool which shows which data is arriving at specific destinations. for monitoring purposes, youll also see alerts in the workspace health tool if your sources or destinations produce repeated errors. how do i filter my data? there are several different ways to ensure that you can collect your data once, but filter it out of specific destinations. see filtering data for a list of the available methods and descriptions. troubleshooting if youre seeing errors thrown by your destinations, you might have an implementation issue. see the integration error codes list or contact our success engineering team for help. have suggestions for things to add to this guide? drop us a line. segment terraform provider segment has a terraform provider, powered by the public api, that you can use to manage segment resources, automate cloud deployments, and change control. take a look at the segment provider documentation on terraform to see whats supported. this page was last modified: 09 apr 2024 need support? questions? problems? need more info? contact segment support for assistance! help improve these docs! was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! get started with segment on this page was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! product for developers company support 2025 segment.io, inc. scraped from: https:segment.comdocsguidesintrouser segment for data users on this page if you arent involved in setting up your segment implementation, or are just starting to set up destinations for your organizations workspace, this guide is for you. what is segment? if you read the detailed explanation of segment on the previous page, you can skip ahead! segment is a system for sending messages from your websites, mobile apps, and servers. these messages contain data about events on, or users of those systems, and these messages can sent on to other tools, and gathered together in a warehouse for later analysis. segment can also bring in information about your users from external systems, such as helpdesks or crm systems, and collate that information to help you analyze your data, build audiences of users, and personalize your users experiences. once you or your organizations developers have your segment sources set up and sending data, you can log in to the segment app and set up destinations, which are how segment sends that data to other tools like google analytics, mixpanel, and many others. environments and labels depending on your organizations configuration and access settings, you might be able to see one or multiple environments for example, production, testing, development, or one or multiple labels, which control access to different parts of your organizations segment system. if you see several environments, contact your segment administrator for more details so you can make sure you make your changes in the right place. data inside segment data enters the segment systems from sources, but once data is in the system, your organization may have different tools configured to control and change it. this could change what data is available to you, or any destinations you set up. for example, protocols makes sure that data coming into segment follows specific formats and patterns, and might block and discard malformed or unwanted data. the privacy tool can be configured to remove personally identifiable information pii from the data. and several different methods are available to filter data so that it doesnt send certain types of events, or reach specific destinations or warehouses. set up a destination depending on the access level you have in your organizations segment workspace, you might be able to create new destinations, or you might only be able to edit existing ones. to add a new destination, youll usually need some information such as a token or api key from the destination tool to start. youll enter that into the segment app so we can connect to and send data to that tool. youll also need to know which source youll be sending data from. to set up a destination: tip: segment usually is able to translate data into a format that the destination expects, however some destinations such as adobe analytics may require manual mapping steps to configure properly. if you see additional fields for mapping configuration, read the documentation for that destination to learn more. troubleshooting if youre setting up a destination to use cloudmode data data thats sent through segment, rather than directly from a users device, you can use the event tester and event delivery tools to check that data is arriving, and being correctly delivered to the destination. have suggestions for things to add to this guide? drop us a line! this page was last modified: 14 jul 2021 need support? questions? problems? need more info? contact segment support for assistance! help improve these docs! was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! get started with segment on this page was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! product for developers company support 2025 segment.io, inc. scraped from: https:segment.comdocsguidesintroadmin segment for workspace administrators on this page if your job is to set up or maintain a segment workspace for your organization, or assist other people using the segment web app, this guide is for you. if youre more interested in segment implementation details, see the developer intro guide. what is segment? if youve already read an introduction to segment, you can skip ahead. segment is a system for sending messages from your websites, mobile apps, and servers. these messages contain event and user data that you can send to other tools or collect in warehouses for further analysis. segment also gathers information about your users from external systems, like help desk software or crms. you can use this collated information to analyze data, build user audiences, and personalize your users experiences. whats a workspace? a workspace is a group of sources that can be administered and billed together. workspaces help companies manage access for multiple users and data sources. workspaces let you collaborate with team members, add permissions, and share sources across your whole team using a shared billing account. when you first log in to your segment account, you can create a new workspace, or choose to log into an existing workspace if your account is part of an existing organization. the workspace administrators role you dont have to be a developer to be a workspace administrator for an organization, and this guide only covers tasks specifically related to managing a workspace in the segment app. however, many workspace admins are also involved in the segment implementation process as there are usually some tasks that must be performed in the workspace to complete an implementation. if you think you might develop a segment implementation or help out other developers, first read segment for developers. note: workspace roles are only available to business tier customers. if youre on a free or team plan, all workspace members are granted workspace administrator access. in addition, workspace administrators set up and maintain the organizations workspace settings, which include: changing a workspace name and slug wont impact configured sources or destinations, which connect using an internal id and writekey. writekey workspace administrators might also maintain: tasks in connections as an administrator, you might be asked to help other members of your organization with tasks related to setting up and troubleshooting your segment implementation. setting up destinations destinations are the endpoints to which segment sends data flowing from your sources. destinations can be thirdparty external tools, like google analytics or mixpanel, or bulkstorage resources, like warehouses. you can set up a destination from within the segment app by navigating to the destination catalog and selecting the tool you want to set up. in most cases, youll need an existing api key or token so that segment can send the data to the correct account. if youre setting up a warehouse or other storage destination, more steps might be required; see the warehouses documentation for more details. troubleshooting use these segment features to keep tabs on your workspace: still stumped? contact support for more help troubleshooting. have suggestions for this guide? reach out with your feedback. this page was last modified: 07 jun 2022 need support? questions? problems? need more info? contact segment support for assistance! help improve these docs! was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! get started with segment on this page was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! product for developers company support 2025 segment.io, inc. scraped from: https:segment.comdocsguidesfilteringdata filtering your segment data on this page there are many ways you can use segment to filter event and object based data to control which destinations it reaches. this document lists the most commonly used ways you can filter data in segment, and explains when youd use each. filtering with the integrations object the integrations object is the only filtering method that cannot be edited using the segment web app. as such, it is both the most reliable, and the most complicated filtering option to change. the integrations object is available to all customers regardless of segment plan. use this option when you absolutely, for sure, 100 know that you always, or never want this data in a specific destination or set of destinations. you can also build logic in your app or site to conditionally enable or disable destinations by rewriting this object, however this is not recommended as it is time consuming to change, especially for mobile apps. the integrations object filters track, page, group, identify, and screen events from both client and cloud based sources, and routes or prevents them from getting to the listed destinations. track page group identify screen you can use the integrations json object as part of your segment payloads to control how segment routes your data to specific destinations. an example payload is below: integrations anonymousid: 507f191e810c19729de860ea, context: locale: enus, page: title: analytics academy, url: https:segment.comacademy , integrations: all: true, mixpanel: false, salesforce: false, my destination function my workspace: true by default, the integrations object is set to all: true. you do not need to include this flag in the object to use this behavior, but if youll be using the integrations object frequently to control destination filtering, you might want to do this to make it explicit for later readers. change this to all: false to prevent any downstream destinations from receiving data, not including data warehouses. if you set segment.io: false in the integrations object, analytics.js 2.0 drops the event before it reaches your source debugger. you can also add destinations to the object by key, and provide a true or false value to allow or disallow data to flow to them on an individual basis. the destination info box at the top of each destination page lets you know how to refer to each destination in the integrations object. integrations all: true all: false segment.io: false true false if you are using multiple instances of a destination, any settings you set in the integrations object are applied to all instances of the destination. you cannot specify an instance of a destination to apply integrations object settings to. note that destination flags are case sensitive and match the destinations name in the docs for example, adlearn open platform, awe.sm, or mailchimp. the syntax to filter data to a data warehouse is different. refer to the warehouse faqs for more details. destination filters destination filters allow you to control the data flowing into each specific destination, by examining event payloads, and conditionally preventing data from being sent to destinations. you can filter out entire events, or just specific fields in the properties, in the traits, or in the context of your events. destination filters support cloudbased serverside, actionsbased, and mobile and web devicemode destinations. destination filters arent available for, and dont prevent data from reaching your warehouses or s3 destinations. destination filters are only available in workspaces that are on a business tier plan. keep these limitations in mind when using destination filters. to set up destination filters from the segment web app for the destination from which you want to exclude data: you can set up destination filters using the options presented in the segment web app, or using segments filter query logic fql. if you use fql, your query syntax is limited to 5kb per query. persource schema integrations filters integration filters allow you to quickly change which destinations receive specific track, identify, or group events. access this tool in any source that is receiving data by navigating to the schema tab. schema integration filters are available to workspaces that are on a business tier plan only. you can apply integrations filters to specific events regardless of whether the source is connected to a tracking plan. to update which destination an event can be sent to, click the integrations dropdown menu to see a list of the destinations each call is sent to. you can turn those destinations on or off from within the dropdown menu. the events filtered out of individual destinations using this method still arrive in your data warehouses. warehouses do not appear in the integration filters dropdown, and you cannot prevent data from flowing to warehouses using this feature to do that use warehouse selective sync. integration filters are allornothing for each event. if you require more detailed control over which events are sent to specific destinations, you can use destination filters to inspect the event payload, and conditionally drop the data or forward it to the destination. integration filters wont override an existing value in the integrations object. if the integration object already has a value for the integration, the per source schema integration filters will not override this. for example, if youre sending events to appsflyer with the appsflyerid passed into the integration object: appsflyerid integrations: appsflyer: appsflyerid: xxxxxx for the same event you have appsflyer turned off using the per source schema integrations filter, this filter wont override the above object with a false value, and events still send downstream. in this scenario, you can use destination filters to drop the event before it sends downstream. schema event filters you can use schema event filters to discard and permanently remove page, screen and track events from eventbased sources, preventing them from reaching any destinations or warehouses, as well as omit identify traits and group properties. use this if you know that youll never want to access this data again. this functionality is similar to filtering with the integrations object, however it can be changed from within the segment app without touching any code. when you enable these filters, segment stops forwarding the data to all of your cloud and devicemode destinations, including warehouses, and your data is no longer stored in segments warehouses for later replay. use this when you need to disable an event immediately, but may need more time to remove it from your code, or when you want to temporarily disable an event for testing. in addition to blocking track calls, you can block all page and screen calls, as well as omit identify traits and group properties. if the source is not connected to a tracking plan, youll find event filter toggles next to the integration filters in the sources schema tab. when an event is set to block, the entire event is blocked. this means no destinations receive it, including data warehouses. when you block an event using schema filters, it wont be considered in the mtu count unless blocked event forwarding is enabled. when an event is blocked, the name of the event or property appears on your schema page with a counter which shows how many times it has been blocked. by default, data from blocked events and properties is not recoverable. you can always reenable the event to continue sending it to downstream destinations. in most cases, blocking an event immediately stops that event from sending to destinations. in rare cases, it can take up to 6 hours for an event to completely stop arriving in all destinations. this feature is only available if the source is not connected to a tracking plan, and is only available in workspaces that are on a business tier plan. protocols tracking plan blocking and property omission if youre using protocols, and youre confident that your tracking plan includes exactly the events and properties you want to record, you can tell segment to block unplanned events or malformed json. when you do this, segment discards any data coming from the source that doesnt conform to the tracking plan. by default, the blocked events are permanently discarded: they do not flow to destinations, and cannot be replayed similar to schema controls. however, you can opt to send data in violation of the tracking plan to a new segment source so you can monitor it. this source can affect your mtu count. if you have protocols in your workspace, and have a tracking plan associated with the source, youll see additional options in the schema configuration section of the sources settings page. from this page you can choose how to handle data violations across different types of calls and properties, whether that be blocking events entirely or omitting violating properties. destination insert function a customizable way to filter or alter data going from a source to a cloudmode destination is to use insert functions. this feature gives you the ability to receive data from your segment source, write custom code to alter or block it, and then pass that altered payload to a downstream cloudmode destination. warehouse selective sync warehouse selective sync allows you to stop sending specific data to specific warehouses. you can use this to stop syncing specific events or properties that arent relevant, and could be slowing down your warehouse syncs. see the warehouse selective sync documentation to learn more. this feature is only available to business tier customers, and you must be a workspace owner to change selective sync settings. privacy portal filtering the privacy portal is available to all segment customers, because segment believes that data privacy is a right, and that anyone collecting data should have tools to help ensure their users privacy. more enhancements are available to bt customers who may need tools for managing complex implementations. the privacy portal tools allow you to inspect your incoming calls and their payloads, detect potential personally identifiable information pii in properties using matchers, classify the information by different categories of risk, and use those categories to determine which destinations may or may not receive the data. learn more about these features in the privacy portal documentation. this page was last modified: 02 feb 2024 need support? questions? problems? need more info? contact segment support for assistance! help improve these docs! was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! get started with segment on this page was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! product for developers company support 2025 segment.io, inc. scraped from: https:segment.comdocsguidesduplicatedata handling duplicate data on this page segment guarantees that 99 of your data wont have duplicates within an approximately 24 hour lookback window. warehouses and data lakes also have their own secondary deduplication process to ensure you store clean data. 99 deduplication segment has a special deduplication service that sits behind the api.segment.com endpoint and attempts to drop 99 of duplicate data. segment stores at least 24 hours worth of event messageids, which allows segment to deduplicate any data that appears with the same messageid within the stored values. api.segment.com messageid messageid segment deduplicates on the events messageid, not on the contents of the event payload. segment doesnt have a builtin way to deduplicate data for events that dont generate messageids. the message deduplication is not scoped to a specific source or a workspace, and applies to all events being received by segment. messageid messageid keep in mind that segments libraries all generate messageids for each event payload, with the exception of the segment http api, which assigns each event a unique messageid when the message is ingested. you can override these default generated ids and manually assign a messageid if necessary. the messageid field is limited to 100 characters. messageid messageid messageid messageid warehouse deduplication duplicate events that are more than 24 hours apart from one another deduplicate in the warehouse. segment deduplicates messages going into a warehouse including profiles sync data based on the messageid, which is the id column in a segment warehouse. messageid id data lake deduplication to ensure clean data in your data lake, segment removes duplicate events at the time your data lake ingests data. the data lake deduplication process dedupes the data the data lake syncs within the last 7 days with segment deduping the data based on the messageid. messageid this page was last modified: 02 aug 2024 need support? questions? problems? need more info? contact segment support for assistance! help improve these docs! was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! get started with segment on this page was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! product for developers company support 2025 segment.io, inc. scraped from: https:segment.comdocsguidesignorebots internet bots on this page whats a bot? if you stumbled onto this page by accident and dont know what a bot is or are just curious to learn more, the following wikipedia article provides an awesome summary: https:en.wikipedia.orgwikiinternetbot. surprisingly, more than half of all web traffic is made up of bots. while a fraction of them are good bots with a regulated pattern, and therefore beneficial to all online businesses, the majority of them have malicious intents and are mostly unregulated. is it possible to ignore bad bots? segment doesnt offer an outofthebox solution to filter or ignore bot traffic. as such, you generally have two options: handle the filtering at a destinationlevel: some of segments destination partners, like mixpanel, filter bots automatically. whereas others such as hubspot allow you to set up bot filtering manually. the advantage of filtering bots at a destination level is that it allows you to implement a robust, easytomaintain solution. however, as it pertains to segment, the downside is that bot traffic will still make it to segment, affecting your mtu count. write custom logic that suppresses bot activity from being sent to segment: if you want to prevent bot traffic from making it to segment in the first place, another option is to write your own custom code. the logic, in pseudocode, would look something like this if you know a particular characteristic of the bot traffic to filter out, such as the useragent: var robots useragent1, useragent2 if ! window.navigator.useragent in robots send analytics calls analytics.track the benefit here is that you would be able to limit the impact that bots have on your mtu count. on the flip side, its much harder to implement and maintain a custom filter. if i see a massive mtu spike because of bots, can i apply for a refund? as a matter of policy, segment doesnt provide refunds for botrelated mtu spikes, as bot traffic is out of segments control. however for extenuating circumstances, you can petition for a refund, assuming youre able to provide proof of the bots effect. im seeing a lot of browser traffic from boardman; is that from segment or a bot? segment uses amazons hosting services, which are based in boardman, oregon. however many bots also originate from aws in boardman as well. one way you can confirm whether or not traffic is coming from segment vs. a bot is to check the useragent of the inbound call. segments is: mozilla5.0 devicemodel.slice0, 3 ; cpu osname osversion.replace.g, like mac os x applewebkit600.1.4 khtml, like gecko version osversion.charat0 .0 mobile10b329 safari8536.25 this page was last modified: 28 oct 2022 need support? questions? problems? need more info? contact segment support for assistance! help improve these docs! was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! get started with segment on this page was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! product for developers company support 2025 segment.io, inc. scraped from: https:segment.comdocsguidessegmentvstagmanagers segment vs. tag managers tag managers, also known as tag management systems tms, were a popular solution before the mainstream adoption of mobile apps. they primarily helped digital analytics and online marketers manage web tags or beacons on a website. built on an older technology, tag managers inject either a piece of javascript or an ad pixel into a website. they carry out rules that marketers create for each tag, like firing an ad channel pixel when that network refers a website visitor. every tag requires users to create rules. no data is stored, and no code is eliminated. in addition to ad networks, todays datadriven businesses use a variety of tools to optimize their product and marketing spends. in order to ab test copy, nurture sales leads, email customers, and provide fast support, businesses integrate variety of analytics and marketing tools. segment makes it easy to install, try, and use them all. tag managers primarily focus on ad networks, and cant support modern tools without extensive customization. rather than firing and forgetting, segment takes a datacentric, deliberate approach to destinations. you dont need to set up special parameters for each tool segment does that for you. segment structures your data so we can understand what it is, and can translate it correctly for each destination we send it to. segment works because all of these tools operate on the same customer data: who is on your app and what are they doing. segment collects this data once, then translates and sends it to every tool you use. because segment also archives the data, segment can replay your historical data into new tools, and send your raw data to a data storage solution for later analysis. every organizations data stack and business requirements are unique. segment also works well in tandem with a tag manager. for example, segment sends data directly to the google tag manager gtm destination. while you can use segments analytics.js library through a tag manager, segment doesnt recommended this for a few reasons: a hybrid approach makes it difficult to determine the root cause of technical problems, and complicates troubleshooting. segment cannot guarantee destination compatibility in a hybrid segmenttagmanager installation, and cannot guarantee support on these installations. all qa and regression testing assumes a native installation of analytics.js on the page. one of segments main charters is to not lose data. our system and cloud infrastructure is designed to ensure that data loss does not happen. if you implement the entry point of data capture segments libraries using a tag manager, you introduce risk of data loss and make it difficult or impossible to troubleshoot. this implementation behind a tag manager can introduce major delays and performance issues, which can cause delays with events that need to occur early in your funnel. the biggest challenge is around triggering cascading events. browsers are notorious for dropping calls. when you use a tms to initiate segment events you are introducing a second point of failure for those events. this page was last modified: 16 feb 2023 need support? questions? problems? need more info? contact segment support for assistance! help improve these docs! was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! get started with segment was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! product for developers company support 2025 segment.io, inc. scraped from: https:segment.comdocsguideswhatisreplay replay replay is available to all business plans. see the available plans, or contact support. on this page replay takes an archived copy of your segment data, and resends it to new or existing tools providing huge benefits to mature data systems. by archiving and replaying data, you can avoid vendor lockin, and protect your system against data loss. replays for tooling changes with replays, you can send your existing data to new tools. this means you can send a limited sample of your data to a new tool to test it out, and run similar tools in parallel to verify the data format or accuracy of the output. finally, when youre ready to switch to a new tool, you can replay a full set of your data to the new tool to backfill it with data that extends before you set up the tool no warmup time or operational gap to disrupt your work. note any destinations which accept cloudmode data meaning data from segment, and not directly from users devices can use replay, however they must also process timestamps on the data for replay to be useful. replays for resilience with replays, youre protected from outages and errors. if a destination which you rely on experiences an outage, or is temporarily unable to accept incoming data, you can use replays to resend data to that tool once the service recovers. you can also use replays to recover from errors caused by misconfigurations in your segment systems. for example, if you send data in the wrong format, or want to apply destination filters. in this case, you can change your mapping using a destination filter, clear out the bad data, and replay it to that destination. you can also use this to update the schema in your data warehouse when it changes. for more information, contact us and our success engineers will walk you through the process. replays considerations replays are currently only available for business tier customers, and due to their complex nature are not selfserve. contact us to learn more, or to request a replay for your workspace. when requesting a replay, include the workspace, the source to replay from, the destination tool or tools, and the time period. replays can process unlimited data, but theyre rate limited to respect limitations in downstream partner tools. if youre also sending data to the destination being replayed to in real time, then, when determining your replays limit, youll want to take into account the rate limit being used by realtime events. you should also account for a small margin of your rate limit to allow events to be retried. replay time depends both on the tool segment replays to and the amount of data included in the replay. replays do not affect your mtu count, unless you are using a repeater destination. notify your team before initiating a replay if youre using a repeater destination. once a replay starts, you will not see replayed events in the event delivery tab. you can initiate replays for some or all events, but you cant apply conditional filters that exclude certain rows of data from being replayed. you can set up destination filters to conditionally filter replayed events. the destination is not required to be enabled in order for a replay to be successful, including destination functions. replayeligible destinations replays are available for any destinations which support cloudmode data meaning data routed through segment and which also process timestamps. destinations that are only available in devicemode meaning where data is sent directly from the users devices to the destination tool cannot receive replays. not all destinations support data deduplication, so you may need to delete, archive, or remove any older versions of the data before initiating a replay. contact segment support if you have questions or want help. replays destination filters replays are subject to the destination filters youve configured on that destination. for example, if you request that identify calls be included in the replay, but your destination has a destination filter that blocks identify events, the filter then blocks all identify events from making it to the destination. in this case, segment recommends that you avoid including identify events in the replay if you know theyll be blocked by the destination filter. when you request a replay, segment asks you to provide a list of the events type andor name that you want included in the replay. if you specify a list of events, then segment only includes those specified events in the replay. if you need to exclude events in your replay, contact segment support. the segment team can help you handle filtering youre unable to do in the replay. replays engage there are two types of replays with engage. replay a profile sources data into engage space, sending a standard sources data into an engage space, which can be configured to send over a specified timeframe as well as the ability to specify all or only a specific subset of events by type or name. replay from an engage space to its connected destination, sending data from an engage output source to its connected destination, which includes all the computational data audiences, computed traits, journeys that destination is currently configured to receive, which can be configured to send over a specified timeframe as well as the ability to specify all or only a specific subset of events by type or name. 1. replay a profile sources data into engage space 2. replay from an engage space to its connected destination this page was last modified: 05 jun 2024 need support? questions? problems? need more info? contact segment support for assistance! help improve these docs! was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! get started with segment on this page was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! product for developers company support 2025 segment.io, inc. scraped from: https:segment.comdocsguidesregionalsegment regional segment regional segment is available to customers on the business tier plan. see the available plans, or contact support. on this page on july 10, 2023, the european commission adopted the adequacy decision for the euus data privacy framework dpf. this concludes that eu personal data transferred to the united states under the dpf is adequately protected when compared to the protection in the eu. with this adequacy decision in place, personal data can safely flow from the eu to us companies participating in the dpf without additional safeguards in place. twilio is certified under the dpf and relies on the dpf as its primary personal data transfer mechanism for euus personal data transfer. twilio will rely on the dpf for any swissus personal data transfers as soon as a corresponding swiss adequacy decision is made. twilio understands that interpretations of data residency are multifaceted and some customers might still want their data to reside in the eu. twilio segment therefore offers a data residency solution outside of the dpf. segment offers customers the option to lead on data residency by providing regional infrastructure in both europe and the united states. the default region for all users is in oregon, united states. you can configure workspaces to use the eu west data processing region to ingest for supported sources, process, filter, deduplicate, and archive data through segmentmanaged archives hosted in aws s3 buckets located in dublin, ireland. the regional infrastructure has the same rate limits and sla as the default region. regional data ingestion regional data ingestion enables you to send data to segment from both devicemode and cloudmode sources through regionally hosted api ingest points. the regional infrastructure can failover across locations within a region, but never across regions. cloudevent sources the following cloud sources are supported in eu workspaces: clientside sources you can configure segments clientside sdks for javascript, ios, android, and react native sources to send data to a regional host after youve updated the data ingestion region in that sources settings. segments eu instance only supports data ingestion from dublin, ireland with the events.eu1.segmentapis.com endpoint. if you are using the segment eu endpoint with an analyticsc source, you must manually append v1 to the url. for instance, events.eu1.segmentapis.comv1. events.eu1.segmentapis.com v1 events.eu1.segmentapis.comv1 for workspaces that use the eu west data processing region, the dublin ingestion region is preselected for all sources. to set your data ingestion region: events.eu1.segmentapis.com all regions are configured on a persource basis. youll need to configure the region for each source separately if you dont want to use the default region. all segment clientside sdks read this setting and update themselves automatically to send data to new endpoints when the app reloads. you dont need to change code when you switch regions. serverside and project sources when you send data from a serverside or project source, you can use the host configuration parameter to send data to the desired region: host https:events.segmentapis.comv1 https:events.eu1.segmentapis.com if you are using the segment eu endpoint with an analyticsc source, you must manually append v1 to the url. for instance, events.eu1.segmentapis.comv1. v1 events.eu1.segmentapis.comv1 here is an example of how to set the host: analytics.initialize, new config.sethosthttps:events.eu1.segmentapis.com https:events.eu1.segmentapis.com; create a new workspace with a different region use this form if you need to transition from your existing usbased workspace to an eu workspace. to create a workspace with a different data processing region, reach out your segment account executive, and they will assist you with enabling the feature. once the feature has been enabled, youll be able to selfserve and create a new workspace in a different data processing region by following these steps: once you create a workspace with a specified data processing region, you cant change the region. you must create a new workspace to change the region. eu storage updates segment data lakes aws regional segment in the eu changes the way you configure the segment data lakes aws environment warehouse public ip range use segments custom cidr 3.251.148.9629 while authorizing segment to write in to your redshift or postgres port. bigquery doesnt require you to allow a custom ip address. 3.251.148.9629 known limitations regional segment is currently limited to the eu. future expansion of regional segment beyond the eu is under evaluation by segment product and rd. edge proxies are deprecated. customers using regional endpoints may see usbased ip addresses in event payloads, segment recommends using the usbased endpoint api.segment.io to preserve client ip addresses. for eu customers, segment recommends using a regionalized eu workspace. api.segment.io destination support and regional endpoint availability dont see a regional endpoint for a tool youre using? as more of the partner tools you use sources, destinations, and warehouses start to support a regional endpoint, segment will update this list. your contact for that tool should have a timeline for when theyre hoping to support regional data ingestion. you can also visit segments support page for any segmentrelated questions. the following integrations marked with a checkmark support eu regional endpoints. integrations available in eu workspaces do not guarantee data residency before you configure an integration, you should check directly with the integration partner to determine if they offer eu endpoints. source regional support dont see regional support for a source youre using? as more of the partner sources start to support posting data to our regional endpoint, segment will update this list. your contact for that tool should have a timeline for when theyre hoping to support regional data ingestion. you can also visit segments support page for any segmentrelated questions. the following sources marked with a checkmark are supported in eu workspaces. this page was last modified: 28 jan 2025 need support? questions? problems? need more info? contact segment support for assistance! help improve these docs! was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! get started with segment on this page was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! product for developers company support 2025 segment.io, inc. scraped from: https:segment.comdocsguidesaudiencesandjourneys audiences, journeys, and broadcasts engage foundations requires a business tier account and includes unify. see the available plans, or contact support. on this page audiences, journeys, and broadcasts are fundamental to twilio engage and let you segment your users, send them personalized content, and show them ads from platforms like facebook or google. in this guide, youll learn how to choose between an audience, a journey, and a broadcast for a number of marketing use cases across the customer lifecycle. back to basics first, consider the following definitions for an audience, journey, and broadcast. audience in engage, an audience is a group of users that share certain characteristics. when you create an audience, you group users who meet certain conditions, like having performed an event or having a computed trait. once youve created an audience, you can sync it to marketing automation tools, ads platforms, analytics tools, or data warehouses. depending on the audiences conditions and connected destinations, segment syncs the audiences users in batches or in real time, as they meet the audiences conditions. journey a journey is a logicdriven workflow that progresses users through steps based on conditions and time delays. you add users to a journey with an entry condition, then users progress through the journeys steps based on conditions you define during journey setup. as with audiences, segment can sync users to destinations at designated points in the journey. unlike an audience, a journey can send users to twilio engages native email and sms channels. broadcast a broadcast is a onetime sms or email campaign sent to a group of users. whereas segment continously updates audience membership, segment only calculates the users who will receive your broadcast once. marketers commonly use broadcasts for newsletters, promotional campaign, and events. engage and the customer lifecycle the customer lifecycle provides a helpful framework for thinking about audiences, journeys, and broadcasts. audiences and broadcasts tend to be most effective at the top of the customer lifecycle funnel, where brand awareness and discovery occurs. a journey becomes a better option as customers progress down the funnel, where a more complex strategy involving messaging, social ads, and newsletters helps move customers closer to conversion. choosing between audiences, journeys, and broadcasts with the customer lifecycle in mind, use the following table as a starting point for selecting an audience or journey for common marketing use cases: while these suggestions will work for most use cases, you may need to consider other factors before you implement your own campaign. asking the following questions will help you identify the right approach. over the course of a campaign, how many touchpoints do i want to create? audiences and broadcasts work best for single, oneoff messages or touchpoints. if you need a campaign with time delays and branching logic, opt for a journey. for example, an audience works well if you want to show a single ad when a user abandons a cart. if, however, you want to show an ad, wait several days, then send the user an email if theyve not completed their purchase, go with a journey. do i want to use engage premier channels like sms and email? you can message users with engage premier channels. if youd like to send an sms or email campaign to a customer, use a journey. do i need branching logic? create a journey if you want to incorporate branching logic into your campaign. do i want to conduct an ab test or create a holdout group? a number of journeys step types, like randomized splits, let you run experiments and test your campaigns. if you want to experiment with different groups, use a journey. do i want my customers to receive the same campaign more than once? with journeys, you can allow customers to reenter a journey theyve exited or restrict them to a onetime journey. audiences, on the other hand, admit users whenever they meet the audiences criteria. for example, you may want to retarget a user with an ad whenever they view a page on your website. in this case, an audience works well since the user can reenter the audience regardless of how many times theyve already done so. putting it together with this guidance in mind, take your next steps with engage by learning how to build a journey, work with engage audiences, and send a broadcast. this page was last modified: 12 jun 2023 need support? questions? problems? need more info? contact segment support for assistance! help improve these docs! was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! get started with segment on this page was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! product for developers company support 2025 segment.io, inc. scraped from: https:segment.comdocsguideshowtoguides how to guides index on this page segments howto guides provide an indepth walk through and examples of the many things you can do to implement, automate, engage with, and begin analyzing your data. weve also got a series of quickstart guides for each of our source libraries. implementation engagement and automation analytics quickstart guides this page was last modified: 25 apr 2022 need support? questions? problems? need more info? contact segment support for assistance! help improve these docs! was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! get started with segment on this page was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! product for developers company support 2025 segment.io, inc. scraped from: https:segment.comdocsguideshowtoguidesautomatedmultichannelreengagement automating multichannel reengagement campaigns on this page compelling and engaging brands delight their customers at every interaction. as customers move seamlessly across channelssuch as email, push notifications, display adsbrands must similarly meet them with tailored and consistent messages. with segment, you can craft a tailored message while using a combination of adroll, customer.io, and other tools to dynamically switch between channels. talk to a product specialist today about using data to tailor your brand experience. tools used retargeting with adroll: adroll is a retargeting and prospecting tool that allows you to show display ads to a behaviorallydefined cohort push notifications with braze: braze is a multichannel marketing campaign focused on the mobile experience emails with customer.io: customer.io is a flexible email provider that allows you to create cohorts based on customer actions. you can build complex onboarding emails, nurture email campaigns, as well as marketing automation workflows. there are other email tools on segments platform, such as bronto, sendgrid, and mailchimp. check out the full list of email tools. its important to register for these tools and enable them on your segment source project. when segment collects tracking data, itll also route it to all of your enabled tools. then your tools, especially ones like customer.io, braze, and adroll, where you can define cohorts of your users, will be working off a dynamic, yet consistent data set. this is paramount in getting the dynamic messaging to update accordingly. set it up when you send tracking data from your app or website to segment, segment will send the same data to all of your tools. segment also collects key messaging events like push notification opened and email opened from braze and customer.io, respectively, and sends that to other tools. by defining cohorts based on these events, you can create dynamic campaign audiences, to which customers can add and remove themselves. in each of your destinationsbraze, facebook, customer.io, adrollyou can create custom campaigns to show display ads or send emails to a specific segment of users who have performed or not performed a given action, or event. in this crosschannel reengagement example, well start with push notifications. 1st line of defense: the push notification in braze, create a segment of customers who added a product to their cart, but did not check out. the segment definition, in this case, should be people who have performed product added, but have not performed order completed . send a push notification to these customers with a message that the cart was abandoned and that they can complete the transaction with, for example, a 10 coupon. product added order completed 2nd line of defense: the email reminder because segment automatically collects secondparty data from braze, you now also have push notification event data, like push notification opened and push notification received in segment. you can use the properties on each of these events to define a property called campaignname so you can tie these activities to a given campaign. push notification opened push notification received properties campaignname this is helpful because now, you can define segments in customer.io for customers who have triggered push notification received, but not push notification opened . youve now automated the process of targeting customers who dont open your push notifications. in customer.io, you can create a campaign that sends an email to those people asking them to check their push notifications and offering them a coupon to complete their order. push notification received push notification opened 3rd line of defense: paid advertising since segment collects email event data, like email opened, from customer.io, you can similarly create segments in facebook ads and adroll for when customers dont open your email. create a segment where users have an email delivered event, but no email opened event. when users meet these criteria, theyll get automatically added to your retargeting campaigns. you can then serve them custom creatives about them neglecting to open your emails and, again, perhaps offer them a coupon to complete the transaction. email opened email delivered email opened when users do not open an activation email, add them to a specific retargeting campaign that contains messaging to remind them to activate. with segment, automate not just switching across channels, but also the messaging in each channel so that the entire experience is cohesive. the added benefit is that we can create specifically targeted retargeting campaigns for people who no longer open our emails or push notifications. automating these processes with segment makes channelswitching more seamless for your customers. create an engaging and consistent brand experience this is just a simple cart abandonment example that dynamically follows customers as they switch between channels. because segment collects and routes the second party data of emails and push notifications being opened, you can create specific campaigns with messaging that targets your customers as they interact with your brand. with over 200 different tools on segments platform, you can take this idea and create other tailored shopping experiences to reengage your customers. talk to a product specialist today about using data to tailor your brand experience. this page was last modified: 06 oct 2023 need support? questions? problems? need more info? contact segment support for assistance! help improve these docs! was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! get started with segment on this page was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! product for developers company support 2025 segment.io, inc. scraped from: https:segment.comdocsguideshowtoguidescollectonclientorserver collecting data on the client or server one of the most common questions segment receives is: should i use one of your clientside libraries or one of your serverside libraries? this is such an important topic that youll find an indepth article in segments analytics academy: when to track on the client vs server. its worth a read. below, you can also read some quick logic around why you may want to choose either option. clientside good things to send from the clientside are things that you wouldnt usually store in your database. things like page views, button clicks, page scroll length, mouse movements, social shares, and likes. things like utm tags, operating system, device type, or cookied data like returning visitors are all easiest to track clientside. of course, some things like mouse movements are only available on the clientside so you should definitely track that there. some destinations can only accept data when the event is sent from the browser. they require events on the client since they rely on cookies and most of those tools do not have an api that segment can send serverside data to. more on this in segments analytics.js docs. serverside charging customers often happens when they arent online, and accuracy for payments is so important. serverside tracking tends to be more accurate than user devices since its a more controlled environment. in general clientside data is fine for watching general trending, but its never going to be perfect. especially if your customers are likely to use things like adblock or oldnonstandard browsers. for example, if youre sending triggered emails based on events, its probably a good idea to make sure your user profiles are sent through segments servers so no one gets left out or misemailed. another good type of data to send serverside are things that need to be calculated from a database query. this might be something like friend count if your site or app is a social network. sensitive information is also best kept out of browsers. any data you dont want exposed to users should be sent serverside. selecting destinations each segment library allows an integrations object either as a top level object or nested in options object. integrations this flag may be especially useful in legacy source types, where an event might be triggered on both the client and server for various reasons. the following will cause the payload to be sent to all enabled tools except facebook pixel: analytics.identifyuser123, email: jane.kimexample.com, name: jane kim , integrations: facebook pixel: false ; this page was last modified: 07 mar 2024 need support? questions? problems? need more info? contact segment support for assistance! help improve these docs! was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! get started with segment was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! product for developers company support 2025 segment.io, inc. scraped from: https:segment.comdocsguideshowtoguidescollectpageviewsserverside collecting pageviews on the server side segment believes that clientside collection is appropriate for collection of basic pageviews. if youd like to track page calls from your server to segment, segment recommends doing it in addition to any client side tracking youre doing with analytics.js, and doing it in a separate source so that you can configure where to send the probably redundant, albeit higherfidelity data. page with this approach, you might use a request middleware to log a pageview with every page load from your server. pageview there are a few things to be mindful of if you want to make sure you can attribute these anonymous page views to the appropriate user in your clientside source eg, for effectively joining these tables together to do downfunnel behavioral attribution. youll want to ensure they share an anonymousid by respecting one if its already there, and setting it yourself if not. to do that, you can read and modify the ajsanonymousid cookie value in the request. anonymousid ajsanonymousid be sure to pass through as many fields as you can in segments page and common spec, so that you get full functionality in any downstream tools you choose to enable. segment recommends specifically ensuring you pass the url, path, host, title, search, and referrer in the message properties and ip and useragent in the message context . properties context heres an example of an express middleware function that covers all those edge cases: if you have any questions or would like help generally adopting this method for other languages and frameworks, be sure to get in touch. import express from express import analytics from analyticsnode import stringify from qs const app express const analytics new analyticswritekey app.usereq, res, next const search, cookies, url, path, ip, host req populate campaign object with any utm params const campaign if search.utmcontent campaign.content search.utmcontent if search.utmcampaign campaign.name search.utmcampaign if search.utmmedium campaign.medium search.utmmedium if search.utmsource campaign.source search.utmsource if search.utmterm campaign.keyword search.utmterm grab userid if present let userid null if cookies.ajsuserid userid cookies.ajsuserid if no anonymousid, send a randomly generated one otherwise grab existing to include in call to segment let anonymousid if cookies.ajsanonymousid anonymousid cookies.ajsanonymousid else anonymousid uuid.v4 res.cookieajsanonymousid, anonymousid const referrer req.getreferrer const useragent req.getuseragent const properties search: stringifyquery referrer, path, host, url any custom props eg. title const context campaign, useragent, ip send a call to segment analytics.page anonymousid, either random matching cookie or from client userid, might be null properties, context proceed! next this page was last modified: 10 oct 2023 need support? questions? problems? need more info? contact segment support for assistance! help improve these docs! was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! get started with segment was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! product for developers company support 2025 segment.io, inc. scraped from: https:segment.comdocsguideshowtoguidescreatepushnotification creating a push notification like emails, push notifications are an extremely powerful way to reengage customers on mobile apps. push notifications are personal, so targeting them precisely using customer behavioral data from segment is crucial. for example, wanelo accepts direct product feeds from retailers. for any of these retailers, when a product goes on sale, they can send a push notification to the people who have saved that product in their profile. push messaging focuses around three key features: content: diversify your messaging just as you would with an investment portfolio. you want to target your consumers with right content and avoid opt out for push. for example, netflix uses push notifications to let users know when their favorite shows are available. rather than sending every user a notification every time any new show or season is released. frequency: consider your app store category. newssports apps send push notifications daily or multiple times a day if its game day. so do social networkingmessaging apps. however, apps that are utilitarian, for example, food and drink, health and fitness, or productivity only message when necessary. timing: always send push notifications to users in their local timezone. in general, mobile usage peaks between 6pm 10pm. choose a destination self evaluate when trying to choose a destination that suits your needs. you will find many alternatives, but choosing the right one for your app is important! key metrics for a successful push ask users to opt in to push notifications upon app install or after the first time they use an app, so its easier to be transparent about how users can opt out later. let your customers decide what notifications they want to receive. it may help to break up your notifications into categories so you can empower your customers with this decision. creating lists of your app users based on characteristics or events that align to specific campaigns will help you better target your mobile marketing efforts. make sure to use deep linking to guide users to the specific screen relevant to that offer. pay attention to user time zones and customize messages based on time of year holidays to make brand personable. the ideal frequency depends on the type of app you have. test different action words, phrases, message lengths, and more. to autoenroll new users into existing campaigns. dont silo the success of your campaign to just app opens. this page was last modified: 10 oct 2023 need support? questions? problems? need more info? contact segment support for assistance! help improve these docs! was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! get started with segment was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! product for developers company support 2025 segment.io, inc. scraped from: https:segment.comdocsguideshowtoguidescrosschanneltracking tracking customers across channels and devices on this page the paths consumers take to your app or website are more complex than ever, often involving a variety of online communities and multiple devices. your next repeat customer might stumble across your display ad on a newsletter youve never heard about, or receive a recommendation from a coworker in a slack channel. but these offdomain and crossdevice brand interactions are equally, if not more, important to track and understand. with this data, you can identify more sources of qualified traffic and determine the best shopping experiences for conversion. in this guide, youll learn where and how to track these critical events so that you can understand your customers journey before they even get to your storefront, as well as their preferred shopping experiences. if youre interested in learning about what to track, check out segments guide on creating an ecommerce tracking plan. talk to a product specialist today about building a clean, highquality data spec so you can focus on brand engagement and sales growth. where are they coming from? offdomain tracking digital marketing consists of owned marketing, earned marketing, and paid marketing. owned marketing encompasses all activities you have full control over. it can be further split into first and secondparty data. firstparty data is customer data generated on your site or in your app. secondparty data is customer data generated when your customers interact with your email or push notifications for example, email opened or push notification received. earned marketing is when publications, newsletters, or blogs organically create some content that refers to, or promotes you. paid acquisition, like display ads or embedded advertorials, dont exist on your domain. to track the inbound traffic from both earned and paid acquisition sources, segment uses utm parameters and deep links if youre directing a customer to a specific screen in your mobile app that has the product to purchase. track engagement on your email channels while these are still under owned marketing, they happen off your domain. an example is sending an engagement email to your customer base with a calltoaction to visit your store. if youre using segment and an email or push notification tool on segments platform, you can easily collect secondparty data such as email sent and push notification opened. learn more about which email and push notification tools segment supports. here are some of the most commonly used and popular events tracked through email and push notifications on segment: email delivered email opened push notification received push notification opened deep link clicked if your email tool is not supported on segment, you can still track email opens with segments tracking pixel. this pixel functions like an advertising pixel in that it embeds an image onto pages where javascript and post requests are disabled. view a list of tools segment supports. in your email template html, include an image tag where the src is a url that is carefully constructed to hit segments appropriate endpoint with a json payload that is base64 encoded. src an example of the payload that will be sent to segment upon an email open is: writekey: yourwritekey, userid: 025waflo3d65, event: email opened, properties: subject: try our new 10 toast, email: andysegment.com then, you would base64 encode that and append it to the segment endpoint: https:api.segment.iov1pixeltrack?data add the complete url as the src in the image tag. src learn more about segments pixel api. track earned traffic with utm parameters utm parameters are types of query strings added to the end of a url. when clicked, they let the domain owners track where incoming traffic is coming from and understand what aspects of their marketing campaigns are driving traffic. utm parameters are only used when linking to your site from outside of your domain. when a visitor arrives to your site using a link containing utm parameters, segments clientside analytics.js library will automatically parse the urls query strings, and store them within the context object as outlined in the spec: common docs. these parameters do not persist to subsequent calls unless you pass them explicitly. context utm parameters contain three essential components: utmcampaign: this is the name of your campaign. all marketing activities that support this campaign, needs to have the same utmcampaign so that downstream analysis to measure performance for this specific campaign can be done off this primary key. example: nationaltoastday utmmedium: how the traffic is coming to your site. is it through email, a display ad, or an online forum? this ensures segments downstream analysis can easily see which channel performs the best. examples: email, paiddisplay, paidsocial, organicsocial utmsource: where the traffic is specifically coming from. you can be specific here. this ensures segments downstream analysis can measure which specific source brings the most conversions. examples: twitter, customer.io email tool, facebook, adroll with these being optional: utmcontent: for multiple calls to action on a single page, utmcontent indicates which one. for example, on a website, there may be three different display ads. while the link on each display ad will have the same utmcampaign, utmmedium, and utmsource, the utmcontent will be different. examples: banner, leftside, bottomside utmterm: this is the parameter suggested for paid search to identify keywords for your ad. if youre using google adwords and have enabled autotagging, then you dont need to worry about this. otherwise, you can manually pass the keywords from your search terms through this parameter so that you can see which keywords convert the most. note that this parameter is reserved explicitly for search. examples: toast, butter, jam if youd like utm parameters to persist in subsequent calls, youll need to manually add those fields in the context.campaign object of your event call. for example: context.campaign analytics.page97980cfea0067, , campaign: name: tps innovation newsletter, source: newsletter, medium: email, term: tps reports, content: image link , ; you can also store the values in cookies andor localstorage and use analytics.js middleware to enrich the payload for subsequent calls. learn more about the semantics with each utm parameter. the key isnt to stick with the definitions that closely, but to be consistent within your own analytics system. proper utms use a marketing campaign is a single marketing message across several platforms, media, and channels, with a consistent and clear calltoaction. since the marketing campaign is from offdomain to your storefront on your property or domain, then its critical to use the proper and consistent utm params across all of your channels: emails paid acquisition guest blog post in partners newsletter article in the news offline events in real life meat space your utm parameters would match a pattern such as: having the same utmcampaign across all channels different utmsource and utmmedium depending on the channel if you were on paid acquisition, the placement of the display ad would determine what goes in utmcontent if you were using paid search, the term would be utmterm an example would be a national toast day campaign. this campaign would include emails, paid acquisition with adroll and facebook ads, organic social twitter, and promotional content on partners blogs. having the consistent utm parameters naming convention simplifies the downstream analysis and the ease of querying across dimensions, such as within the campaign, which medium or source was the best. or which placement of the display ad led to the most conversions. learn more about measuring roi of marketing campaigns with sql and utm parameters. what device are they using? crossdevice tracking its common for customers to discover you on their desktop before making the purchase much later on their phone. how do you tie all of these events back to the same customer so you can understand which marketing activities on what screens are responsible for conversions? track serverside when possible tracking with javascript in the browser has its benefits, such as using browser technologies to automatically track things like utm parameters, referring domain, ip address, and user agent. but here are a few reasons why it might make sense for your store to track on the server side. are your customers technically savvy and use ad blockers? ad blockers restrict requests from a list of blocklisted domains to your browser, which means that none of your event tracking will work properly. if you sell to a technical audience, it is possible that you may be underreporting your analytics by a material amount. do you have multiple devices? if you have multiple devices with the same customer check out flow, moving those events to the serverside will reduce your surface area of your code base. this means less maintenance and faster changes. learn more about client vs server tracking. if you do move key checkout events to the server side, you will have to manually send the data automatically collected by segments clientside javascript library to your server. these pieces of tracking data are still important for the following reasons: utm parameters: collecting the utm params will allow you to tie conversion events to your marketing campaign or activities. this is valuable in that you can immediately measure performance and calculate roi on your campaigns. ip address: the ip address can provide location intelligence for your customers. this means you can personalize your shopping experience or engagement emails with inventory that might be more relevant depending on your customers locations. user agent: the user agent will inform you of your customers preferred device and shopping experience. are they converting on a mobile web browser? native app? or on their laptop? learn how to usecontext to manually send this information on the server side. context track the same user across devices if your store allows user registration and users are logged in when they shop on your site or app, then you can track them across devices. this works by using a userid instead of an anonymousid to track key events and where they occur. this userid serves as the primary key in your downstream tools and data warehouse, allowing you to join all of a profiles anonymous activities with logged in activities. you also can get a complete picture of a profiles location, and what device they are on while using your app or website. userid anonymousid userid learn more about pulling the entire user journey for a single user given a userid. unfortunately, tracking the same user across devices only works if they log in to each device. anonymous browsing in each distinct experience for example, mobile safari, native iphone, browser on laptop generates its own unique anonymousid . each anonymousid is limited to the scope of that browser or app, only measuring activities in those sessions. its not until the user logs in when the userid is generated if registering for a new account or the userid is retrieved from your database, and then mapped to the anonymousid of that session. segment keeps a table of anonymousids mapped to a single userid so you can analyze a users activity across multiple devices. anonymousid anonymousid userid userid anonymousid anonymousid userid if a user logs in on multiple devices, then you would be able to analyze even the anonymous activity across those devices. consequently, its important to encourage your users to log in so that you have this capability. attribute offline conversions to online impressions one of the biggest challenges for brickandmortar stores is to measure the impact of their online advertising campaigns on their instore purchases. attributing offline conversions has traditionally been difficult to achieve, due to the lack of offline data and robust infrastructure to route that data. for facebook advertisers, facebook offline conversions allow you to tie offline conversions to your campaigns. its important to note that the offline data is labeled to an event set that has been assigned to a facebook campaign. here are the two ways to attribute offline conversions to facebook advertisements: uploading offline event data about actions that arent captured with facebook pixel or app events to facebook for them to match actions to your facebook ads enable and configure segments facebook offline conversions destination, which automates attributing offline events to your facebook ads in realtime learn more about the benefits of segments facebook offline conversions destination. most other advertising networks provide some functionality of manually uploading offline data to match with their online advertising data. here is a short list of other services: attributing instore purchases to an impression from a display ad online is critical to help marketers and advertisers understand which campaigns or creatives are driving sales. the more realtime the data and insights, the more nimble your business can be in altering course so that additional resources can be put towards the right marketing actions. learn about the funnel before your website or app the internet has made it easy for customers to come from nearly anywhere to your digital storefront. but there are ways to track and collect data to better understand these complicated paths so you can be intentional with your marketing efforts to tap into these communities. by tracking in these locations with the above mentioned techniques, your downstream analysis will also be simpler. with utm params, youll be able to quickly measure the performance of a campaign or a particular channel. by properly tracking on multiple devices, you can understand which shopping experiences are most preferred. these tracking techniques are invaluable to understanding the source of your highest quality customers. talk to a product specialist today about building a clean, highquality data spec so you can focus on brand engagement and sales growth. this page was last modified: 12 aug 2024 need support? questions? problems? need more info? contact segment support for assistance! help improve these docs! was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! get started with segment on this page was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! product for developers company support 2025 segment.io, inc. scraped from: https:segment.comdocsguideshowtoguidesdynamiccouponprogram setting up a dynamic coupon program to reward loyal customers on this page one component of building a successful and engaging ecommerce brand is rewarding your most loyal customers. with segment warehouses and sql, you can retrieve a table of your most valuable customers, then reward them. this guide will walk you through setting up a dynamic and automated coupon program based on conditions that define your most valuable customers, as well as how to measure the programs performance. talk to a product specialist today about using data to tailor your brand experience. tools used emails with customer.io: customer.io is a flexible email provider that allows you to create cohorts based on customer actions. you can build complex onboarding emails, nurture email campaigns, as well as marketing automation workflows. retention analytics with amplitude: amplitude is an analytics tool that focuses on understanding retention and funnel analysis. its important to register for these tools and enable them on your segment source project. when segment collects tracking data, it routes it to all of your enabled tools, meaning that they get a single consistent data set. most importantly, the data generated by users interacting with emails is sent through segment so you can analyze email performance, and how it impacts conversion with amplitude. not using customer.io or amplitude? check out the other segment supported email marketing and analytics tools. the loyalty program say, as the marketing manager of our fictitious, ondemand artisanal toast company, toastmates, you want to experiment with a coupon program to retain your best customers. through a combination of sql and statistical analysis on a set of historical data, youve identified the conditions for our most valuable customers as: learn how to define these conditions in how to forecast ltv for ecommerce with excel and sql. will rewarding a 5 coupon to this cohort after they make the second purchase a month lead to higher engagement and ltv? set up this program using customer.io as the email provider and measure its performance on engagement and ltv with amplitude. conduct a split test half of the cohort will represent the control group and will not receive any emails; the other half will receive an email with the 5 coupon for one month. after which, use amplitude to see if there were any correlations between the coupon email and conversions. set it up first, register for an account with customer.io and amplitude. then, enable customer.io and enable amplitude on your segment project. finally, go into your customer.io account and enable sending data to segment: you can find those destination settings in customer.io here. when everything is enabled, customer event data such as order completed and product added, as well as their properties, will all be sent to your configured destinations, including customer.io and amplitude. then you can define cohorts based on these events in customer.io to add to email campaigns or conduct funnel analytics in amplitude. order completed product added talk to a product specialist to learn what else you can accomplish with these tools. define the cohort in customer.io now define the specific cohort in customer.io as per our conditions listed earlier: someone who spends over 20 per order and shops over twice a month. in customer.io, go to segments and create segment: after this cohort is created, then when a customer makes the third purchase in a month and its over 20, they will be added to this segment. next, create a segment trigger campaign, where customer.io will send a message the first time someone enters a segment. the segment in this case will be the one you just created: coupon loyalty experiment. save the changes and enable the campaign. then, make sure that your ecommerce backend is set up properly to handle the coupons. if its available in your system, create a coupon that only works for a specific set of customers. measure performance after a month has passed for the split test, you can measure the performance of the email coupon program to see whether its making a material impact on conversions. in amplitude, create a funnel that compares the two cohortsone who received this coupon email vs. the control group who did notand see its impact on conversions and revenue generated. first, define a behavioral cohort with the conditions of being loyal customers so you can use it when analyzing the conversion funnel: youll also have to create a second identical cohort, except with the only difference that these customers did not receive the coupon email. you need this cohort to create the conversion funnel with the control group. after youve created these two cohorts, create two funnel charts. the first funnel will look at the control group. the second funnel will look at the group that received the coupon email. resulting in: the control group that did not receive the email for the coupon resulted in 233 people visiting the store, with 66 conversions. the funnel for the group who did receive the emails can be created with these parameters: resulting in: the email itself drove 168 customers to the store, which also saw higher conversions to product added and ultimately order completed. product added order completed note that this funnel is only looking customers who went through these events in this specific order. this analysis doesnt consider customers who are part of the emailed cohort, yet didnt open the email, but still visited the site andor made a purchase. at first glance, it appears that the group that was emailed did receive an absolute number of more conversions. however, these funnels are still inconclusive, given that you havent explored the impact on the top line revenue, as well as overall engagement with the brand. fortunately, you can continue to use amplitude to analyze impact on revenue itself. find new ways and channels to retain your most valuable customers retaining and rewarding your customers is paramount to a strong and engaging brand. this example is just one of millions that you can employ to find new ways to delight and excite your customer base. other ideas can be to send messages to your customers with a referral code to invite their friends. or set up a coupon for customers who are just shy of entering your most valuable customers cohort. or, if youre hosting a pop up shop event, sending a special and personalized invite to your strongest users first, as a way to thank them for their business. the possibilities are endless when you use your customer data to drive sales. talk to a product specialist today about using data to tailor your brand experience. this page was last modified: 25 oct 2023 need support? questions? problems? need more info? contact segment support for assistance! help improve these docs! was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! get started with segment on this page was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! product for developers company support 2025 segment.io, inc. scraped from: https:segment.comdocsguideshowtoguidesforecastwithsql forecasting ltv with sql and excel for ecommerce on this page customer lifetime value ltv is the amount of money that an individual customer will spend with a given business in the future. its often used to value cohorts in your customer base, determine how much to spend in acquiring or retaining new users in a given cohort, rank customers, and measure the success of marketing activities from a baseline ltv forecast. the ltv calculation is not straightforward for ecommerce businesses, since future payments are not contractual: at any moment, a customer may never make a single purchase again. additionally, forecasting future purchases requires statistical modeling that many current ltv formulas lack. this guide shows how to calculate forwardlooking ltv for noncontractual businesses using sql and excel. this analytical approach allows you to accurately rank your highest value customers, as well as predict their future purchase sizes to help focus your marketing efforts. this guide assumes youre using the tracking schema described in how to implement an ecommerce tracking plan and are storing data in a segment warehouse. talk to a product specialist to learn how companies like warby parker and crate barrel use a data warehouse to increase engagement and sales. calculating ltv: buy til you die in a noncontractual setting, you cant use a simple retention rate to determine when customers terminate their relationship. this is because the retention rate is a linear model that doesnt accurately predict whether a customer has ended her relationship with the company or is merely in the midst of a long hiatus between transactions. the most accurate noncontractual ltv model, named buy til you die btyd, focuses on calculating the discounted estimation of future purchases based on recency of last purchase, frequency of purchases, and average purchase value. this model uses nonlinear modeling to predict whether or not a user is alive or dead given historic transactions to forecast future probability and size of purchases. since ltv is a critical metric for ecommerce companies, its important that this model, instead of simpler linear formula that is based on retention rates, is used for its calculation. use sql to build the necessary table, which will be exported as a csv and opened in google sheets. then, use solver to estimate the predictive model parameters, which ultimately calculates the future purchases of each customer. finally, the ltv calculation is simply the net present value of each customers future purchases. rank them by ltv, then find behavioral patterns across the top 10 or 50 customers to figure out how best to target or retain this cohort. recency, frequency, and average size as a growth analyst at the fictitious ondemand artisanal toast company, toastmates, its important to know which customers are worth more to the business than others. most important, you should understand what similarities these customers all have to help guide the marketing team in their efforts. the first step in creating the btyd model is to get historic purchasing data of at least a month. in your analysis, you can use data from the past six months. the data must include the columns userid email is fine too, number of purchases within the specified time window, days since last purchase, and days since first purchase. userid then, use this google sheet, which provides all of the complex calculations for estimating the model parameters, as well as forecasting the future sales of each customer. this sheet is view only, so be sure to copy it entirely so you can use it. to retrieve a table with the right columns for analysis, use the follow sql query: with firsttransaction as select u.email, datediffday, minoc.receivedat::date, currentdate as first from toastmates.ordercompleted oc left join toastmates.users u on oc.userid u.email where oc.receivedat dateaddmonth, 6, currentdate group by 1 , frequency as select u.email, countdistinct oc.checkoutid as frequency from toastmates.ordercompleted oc left join toastmates.users u on oc.userid u.email where oc.receivedat dateaddmonth, 6, currentdate group by 1 , lasttransaction as select u.email, datediffday, maxoc.receivedat::date, currentdate as last from toastmates.ordercompleted oc left join toastmates.users u on oc.userid u.email where oc.receivedat dateaddmonth, 6, currentdate group by 1 , averagetransactionsize as select u.email, avgoc.total as avg from toastmates.ordercompleted oc left join toastmates.users u on oc.userid u.email where oc.receivedat dateaddmonth, 6, currentdate group by 1 order by 2 desc select distinct u.email, nvlf.frequency, 0 as frequency, nvlz.last, 0 as dayssincelasttransaction, nvla.first, 0 as dayssincefirsttransaction, t.avg as averagetransactionsize from toastmates.users u left join firsttransaction a on u.email a.email left join frequency f on u.email f.email left join lasttransaction z on u.email z.email left join averagetransactionsize t on u.email t.email order by 2 desc this returns a table where each row is a unique user and the columns are email, number of purchases within the time window, number of discrete time units since last purchase, and average purchase order. here is a screenshot of the first twelve rows returned from the query in mode analytics. export this data to a csv, then copy and paste it in the first sheet of the google sheet where the blue type is in the below screenshot: also be sure to add the total time in days in cell b6. this is important as the second sheet uses this time duration for calculating net present value of future payments. how to use the google spreadsheet after you paste in the csv from the table into the first tab of the sheet, the next step is to estimate the model parameters the variables on the top left of the sheet. in order to do this, we need to use a feature of microsoft excel called solver. you can export your google sheet as an excel document. then, use excel solver to minimize the loglikelihood number in cell b5, while keeping the parameters from b1:b4 greater than 0.0001. after solver runs, cells b1:b4 will be updated to represent the models estimates. now, you can hard code those back into the sheet on google sheets. the next sheet relies on these model estimates to calculate the expected purchases per customer. model and predict future customer purchases the model requires four pieces of information about each customers past purchasing history: her recency how many time units her last transaction occurred, frequency how many transactions she made over the specified time period, the length of time over which we have observed her purchasing behavior, and the average transaction size. in the example, you have the purchasing behavior data over the course of six months with each unit of time being a single day. you can apply a both a betageometric and a negative binomial distribution bgnbd to these inputs and then use excel to estimate the model parameters an alternative would be the paretonbd model. these probability distributions are used because they accurately reflect the underlying assumptions of the aggregation of realistic individual buying behavior. learn more about these models. after estimating the model parameters, you can predict a particular customers conditional expected transactions by applying the same historic purchasing data to bayes theorem, which describes the probability of an event based on prior knowledge of conditions related to the event. estimating the model parameters the top left part of the first sheet represent the parameters of the bgnbd model that must be fitted to the historic data you paste in. these four parameters r, alpha, a, and b will have starting values of 1.0, since youll use excel solver to determine their actual values. the values in columns f to j represent variables in the bgnbd model. column f, in particular, defines a single customers contribution to a the overarching function, on which well use solver to determine the parameters. in statistics, this function is called the likelihood function, which is a function of the parameters of a statistical model. in this particular case, this function is the loglikelihood function, which is b5, as calculated as the sum of all cells in column f. logarithmic functions are easier to work with, since they achieve its maximum value at the same points as the function itself. with solver, find the maximum value of b5 given the parameters in b1:b4. with the new parameter estimates, you can now predict a customers future purchases. predicting a customers future purchases in the next sheet, you can apply bayes theorem to the historic purchasing information to forecast the quantity of transactions in the next period. multiply the expected quantity with the average transaction size to calculate the expected revenue for that period, which you can extrapolate as an annuity, of which you can find the present discounted value assuming discount rate is 10. central to the bayes theorem formula is the gaussian hypergeometric function, which is defined by 2f1 in column m. evaluate the hypergeometric function as if it were a truncated series: by adding terms to the series until each term is small enough that it becomes trivial. in the spreadsheet, we sum the series to its 50th term. the rest of the variables in bayes theorem is in columns i through l, which use the inputs from the customers historic purchasing information, as well as the model parameter estimates as determined from solver cells b1:b4. the expected quantity of purchases in the next time period is calculated in column h. finally, multiply that with the average transaction size and you can get the expected revenue for the next time period. rank your customers this exercise allows you to rank your customers from most valuable to least by ordering column f in descending order. you can take the userid s of the top several customers and look across their shopping experiences to identify any patterns that they share, to understand what behaviors are leading indicators to becoming high value customers. userid below is a simple query to get a table of a users actions in rows. just replace the useridwith the user in question. userid with anonymousids as select anonymousid from toastmates.tracks where userid 46x8vf96g6 group by 1 , pageviews as select from toastmates.pages p where p.userid 46x8vf96g6 or anonymousid in select anonymousid from anonymousids order by p.receivedat desc , trackevents as select from toastmates.tracks t where t.userid 46x8vf96g6 or anonymousid in select anonymousid from anonymousids order by t.receivedat desc select url, receivedat from pageviews union select eventtext, receivedat from trackevents order by receivedat desc this above query for user whose userid is 46x8vf96g6 returns the below table: userid 46x8vf96g6 at toastmates, most of the highest forwardlooking expected ltv customers share one thing in common: averaging two orders per month with an average purchase size of 20. with that in mind, you can define a behavioral cohort in our email tool, customer.io, as well as create a trigger workflow so we can send an email offer to these customers. learn how to use email tools to target this cohort of high value customers. reward your best customers this exercise is useful not only as a forward looking forecasting model for customer ltv, but also as a quality ranking system to see which customers are worth more to your business. coupled with the ability to glance across the entire shopping experience of a given customer, you can identify broad patterns or specific actions that may be an early signal for a high value shopper. recognizing these high value shoppers means being proactive in nurturing, rewarding, and retaining them. and this is just the beginning. having a rich set of raw customer data allows you to create accurate projection models for ltv so you know not only how much you can spend to acquire them, but also how to rank your customers by value. ultimately, these insights lead to the right actions that can build an engaging shopping experience and drive sales. talk to a product specialist to learn how companies like warby parker and crate barrel use a data warehouse to increase engagement and sales. this page was last modified: 15 mar 2024 need support? questions? problems? need more info? contact segment support for assistance! help improve these docs! was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! get started with segment on this page was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! product for developers company support 2025 segment.io, inc. scraped from: https:segment.comdocsguideshowtoguidesimporthistoricaldata importing historical data on this page when transitioning over to segment, customers commonly want to import historical data into tools they are migrating to or evaluating. note: historical imports can only be done into destinations that can accept historical timestamped data. most analytics tools like mixpanel, amplitude, or kissmetrics can handle that type of data just fine. one common destination that doesnt accept historical data is google analytics, since their api cannot accept historical data. method 1: using a custom solution general instructions use any serverside library, which sends requests in batches to improve performance. once you have data to import, follow the steps below: export or collect the data to be imported. include timestamp data in your export if the data needs to appear in end tools in a historical reference. for instance, if youre importing emails and its relevant to know when someone joined your email list, you may need to export the timestamp. if no timestamp is specified when importing, the data will show a timestamp from the time the data was received. decide which destinations need to receive the data. by default, data coming into segment will be forwarded to all destinations connected to a given source. to limit data to specific destinations, the integrations object must be modified. with historical data, you often only want to send the data to a specific destination or into your data warehouse. for example, in node.js set the integrations object as follows. integrations integrations analytics.track event: upgraded membership, userid: 97234974, integrations: all: false, vero: true, google analytics: false once youve done that, youll need to write an application or worker to send the data to segment. you will need to cycle through each set of data and map it to a segment serverside library method or build an array matching the http import api format. tip: segment recommends using a segment library for this process, as they set contextual message fields like messageid used for deduping and sentat used for correctly client clock skew that segments api uses to correct behavior upon ingestion. messageid sentat tip: the serverside libraries will automatically batch requests to optimize for performance and prevent linear request volume. this batching behavior is modifiable, and some of the underlying libraries implement a configurable max queue size that may discard messages if you enqueue requests much faster than the client can flush them. we recommend overriding the max queue size parameter for the library to a high value youre comfortable you can remain under in your batch job. demo projects the following projects are opensource and do not have official segment support. if you encounter issues, the best way to get help is by opening an issue on the projects github page. feel free to clone the repository and adjust the code to suit your unique needs. one of segments success engineers wrote an alpha prototype node.js app for importing data utilizing the http api, which weve included below: example node.js import application additionally, one of segments software engineers developed a react app with more out of the box functionality for importing events. the features include a modern ui, transformations, and event format checking prior to import: desktop react csv uploader marketlytics has documented their experience using the alpha prototype importer and offer some helpful visuals and tips. alternative solution if a serverside library doesnt meet your needs, you can use the segment bulk import http api directly. note: when you use the http api to export historical data to upload to segment, remove all the original sentat, messageid, and projectid fields from the archived message before forwarding them back to segment. sentat messageid projectid method 2: using reverse etl please refer to the reverse etl guide for more details. this page was last modified: 14 may 2024 need support? questions? problems? need more info? contact segment support for assistance! help improve these docs! was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! get started with segment on this page was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! product for developers company support 2025 segment.io, inc. scraped from: https:segment.comdocsguideshowtoguidesjoinuserprofiles joining user profiles on this page one of the first questions we get when our customers start querying all of their data is, how do i join all this data together? for example, lets say youd like to know if support interactions in zendesk increase revenue in stripe, or which percentage of users opened your email campaign and visited your website or mobile app? the key to answering these advanced questions is tying your data together across these sources. to do that, you need a common user identifier. what is the user id problem? each saas tool you use has its own way of identifying users with a unique primary key. and, you will find each of these different ids across different collections of tables in your database. so, when you want to start matching joe smith who entered a ticket in zendesk and also clicked through a campaign in mailchimp, it starts to get tricky. for example, stripe keeps track of users with a customerid, segment requires auserid, and marketo uses email to uniquely identify each person. customerid userid email to effectively join across these sources, you need to understand how each id maps to each other. the best way to do this is to create a common identifier across tools. use a common identifier when possible when you install a new tool or use segment to install all of them at once, you need to choose what you will put in the id field. there are lots of different options for this: emails, twitter handles, usernames, and more. however, we suggest using the same id you generate from your production database when you create a new user. database ids never change, so they are more reliable than emails and usernames that users can switch at their leisure. if you use this same database id across as many tools as possible, it will be easier to join identities down the road. in mongodb, it would look something like this 507f191e810c19729de860ea. 507f191e810c19729de860ea analytics.identify1e810c197e, thats the user id from the database name: jane kim, email: jane.kimexample.com also includes email ; though we wish you could use a database id for everything, some tools force you to identify users with an email. therefore, you should make sure to send email along to all of your other tools, so you can join on that trait as a fallback. for segment destination users integrating as many tools as possible through segment will make your joins down the road a little easier. when you use segment to identify users, well send the same id and traits out to all the destinations you turn on in our interface. more about segment destinations. identify a few of our destination partners accept an external id, where they will insert the same segment user id. then you can join tables in one swoop. for example, zendesk saves the segment user id as externalid, making a segmentzendesk join look like this: externalid select zendesk.externalid, users.userid from zendesk.tickets zendesk join segment.usersusers on zendesk.tickets.externalid segment.userid heres a look at the segment destinations that store the segment user id: how to merge identities whether youre using segment or not, we suggest creating a master user identities table that maps ids for each of your sources. this table will cut down on the number of joins you have to do because some ids may only exist in one out of many tables related to a source. heres sample query to create a master user identities table: create table useridentities as select segment.id as segmentid, segment.email as email, zendesk.id as zendeskid, stripe.id as stripeid, salesforce.id as salesforceid, intercom.id as intercomid from segment.users segment zendesk leftjoin zendesk.users zendesk on zendesk.externalid segment.id if enabled through segment or zendesk.email segment.email fallback if not enabled through segment stripe left join stripe.customers stripe on stripe.email segment.email salesforce left join salesforce.leads salesforce on salesforce.email segment.email intercom left join intercom.users intercom on intercom.userid segment.id if enabled through segment or intercom.email segment.email fallback if not enabled through segment group by 1,2,3,4,5,6 youll spit out a user table that looks something like this: while creating this table in sql is a good strategy, wed be remiss not to point out a few drawbacks to this approach. first, you need to run this nightly or at some regular interval. and, if you have a large user base, it might take a while to run. that said, its probably still worth it. how to run a query with your joined data so what can you do once you have all of your ids mapped? answer some pretty nifty questions that is. here are just a few sql examples addressing questions that incorporate more than one source of customer data. segment zendesk which referral source is sending us the most tickets? selectsegment.referralsource, countzendesk.ticketid as countoftickets from zendesk.tickets zendesk left join segment.userssegment onusers.segmentid segment.userid group by 1 order by 2 desc stripe zendesk how many tickets do we receive across each pricing tier? select stripe.planname as planname, countzendesk.ticketid as countoftickets start with zendesk from zendesk.tickets zendesk merge users left join useridentities users on zendesk.id users.zendeskid add stripe left join stripe.charges stripe on users.stripeid stripe.customerid group by plan name, from most tickets to least groupby1 orderby2desc advanced tips an alternative to the lookup user table in sql would be writing a script to grab user ids across your thirdparty tools and dump them into your database. youd have to ping the apis of each tool with something like an email, and ask them to return the key or id for the corresponding user in their tool. a sample script, to run on a nightly cron job, would look something like this: var request requiresuperagent; https:www.npmjs.compackagesuperagent var username ; var password ; var host https:segment.zendesk.comapiv2; gets the user object in zendesk by email address. param string email param function fn functiongetuseridsemail, fn request .gethost userssearch.json?query email .authusername, password .endfn; get the first zendesk user that matches kanyekimye.com getuseridskanyekimye.com, functionerr, res if err return err; res.body.users will be an array res.body.users0.id will return the id of the first user ; here is the documentation for zendesks api for more information. this page was last modified: 21 apr 2023 need support? questions? problems? need more info? contact segment support for assistance! help improve these docs! was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! get started with segment on this page was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! product for developers company support 2025 segment.io, inc. scraped from: https:segment.comdocsguideshowtoguidesmeasureadvertisingfunnel measuring your advertising funnel on this page its surprisingly hard to answer questions about the roi of your ad campaigns. what does a click actually result in? how much should i pay for it? we built our sources for facebook ads and google adwords to help you understand the true performance and cost of your campaigns. in this article, we dig into the nuances of data collection and potential gotchas around measuring clicks, pageviews, and ultimately, conversions. measuring campaign performance today, most marketing teams think about their paid acquisition funnel as three major steps this makes sense when looking at overall campaign performance, but hides several crucial funnel steps that can make the difference between increasing a campaigns spend and shutting it off due to poor results. because page optimization and ad blockers can impact measurement of your funnel, its important to look at the four additional steps happening between the ad click and conversions. lets go through each true funnel step in a little more detail. impressions clicks: when a user views an ad, the ad platform increments the count of impressions for that ad. when an ad is clicked, the ad platform logs a click. this is all handled by the ad platforms servers. facebook and google work hard to filter invalid and fraudulent traffic, whether thats a mistaken click, a bot, or a competitor looking to drain your advertising budget. any bad traffic is removed from both your reporting and your monthly bill. page request initiated: after an ad is clicked, a users browser attempts to load your landing page. this request is the first contact your application has with the user, and the server responds with the content to render the landing page. first javascript loaded: the users browser starts to download the landing page content, which includes the html, javascript, and css. the browser parses and renders this content, loading the javascript sequentially as it parses the page. by default, analytics.js uses the async tag, which means that the browser wont block the page and will load analytics.js once everything else is ready. analytics.js wants to get out of the way where possible so you can create the best experience for your customers. async page fully rendered: the page is fully rendered once all the html, css and scripts have been loaded on the page. this time can vary a lot based on the speed of the internet connection how fast all the assets download and the device itself how fast the local computer can run all of the scripts. thirdparty scripts loaded: finally, thirdparty scripts are asynchronously loaded onto the page. the speed at which these scripts are loaded depends on a variety of factors, like the page size, network speed, and the size and number of the thirdparty scripts. once these scripts are loaded, analytics.js triggers a page call to our api. page conversion event: from there, a user might fill out a form, signup, or buy your product! how does this impact my ad reporting? there are three lessobvious contributors to falloff across the paid acquisition funnel: slow loads, ad blockers, and bounces. for the sake of illustration, this means that if you have 100 ad clicks, you will be able to count most but not all corresponding page views because some visitors may bounce exit or hit the back button before analytics.js is executed. similarly, you may miss some attributable conversions due to slow load times your page calls cant fire in time and ad blockers which often block analytics not just ads. heres how it works. slow loads slow loads can impact your attribution modeling, making campaigns appear to have worse performance than reality. in the general case, when a user hits your landing page, your tracking code loads and triggers a pageview event that you can use to attribute that user to a campaign. but if thirdparty scripts take on the order of seconds to load for example, on 1x or 3g networks, users may click off the page before your tracking code executes. in this case, the pageview never gets recorded and your ability to attribute that click to a conversion is lost. this is generally not an issue for most companies because they are focused more on people who spend a good deal of time on their pages. however, it is a potential source of opaqueness, particularly for users with slow or bad network connection. bounces bounces can occur at any stage of the funnel between an ad being clicked and thirdparty scripts loading on the page. some bounces are not tracked because the user doesnt even last the few seconds to request your html, render it, and execute tracking. if they quickly hit back or close the browser window, your ad platform will report clicks that dont show up in your analytics tracking. ad blockers it is likely the case that some percentage of your users are using ad blockers. its estimated that 22 of mobile smartphones worldwide and 16 of us web traffic use ad blockers. segment customers have reported ad blockers for as little as a few percentage points of their visitors, to upwards of 70 of traffic for companies with very techforward audiences. but just because a user is using ad blockers doesnt mean that they arent seeing and clicking on ads. facebook recently announced that they would be suppressing ad blockers, and adblock plus, the most popular ad blocking and antitracking software, categorizes google search ads as acceptable ads. that said, many ad blockers do block analytics tools like google analytics, mixpanel and segment. this means that there exists some percentage of your conversions that actually came through your paid acquisition channels, but are unattributable due to ad blockers. what if i need more precise tracking? segment offers two ways of joining your user clickstream data to your paid acquisition channels: standard clientside tracking or advanced serverside page calls. both options come with their own tradeoffs that are important to consider for your use case. clientside tracking standard analytics.js is loaded with the async tag by default, which means that the library and all its destinations are loaded near the end of the page rendering. the benefit is that analytics.js doesnt slow down page loads, but it does mean that tracking is not executed immediately on page load. when you use standard clientside tracking, youll lose pageview data for visitors who bounce or click off the page before analytics.js executes, and for visitors with ad blockers enabled. serverside page calls advanced if you want to capture adblock, bounce, and slow load traffic, we recommend adding an additional page call to the serverside. this allows you avoid the browser altogether and see the total number of requests emanating from your paid acquisition channels. youll get visibility on an extra step in that funnel. page the general approach is to use an arbitrary anonymousid e.g. a uuid in the serverside page call and then also set the anonymousid as the ajsanonymousid cookie in the browser. you can read more about how to implement that here. this approach is tricky to implement, so we recommend that this is undertaken only for use cases in which bounce andor adblock data is critical. anonymousid page anonymousid ajsanonymousid estimating the impact of moving serverside if you want to get a quick estimate for the number of additional clicks youd track using serverside tracking, you can use redirect tracking with a url shortener to estimate the number of clicks coming from google adwords or facebook ads. this will give you an estimate for the number of times an ad is clicked minus some bounce in the few hundred milliseconds of the redirect, which will closely match serverside page tracking should you choose to implement it. page heres how it works use a url shortener like bit.ly to link to a landing page, with a custom parameter like ?ttg2 . ?ttg2 add the shortened link to your ad. measure total clicks from the bit.ly stats page. in your warehouse, count the number of pages with that unique url parameter from step 1 make sure youre looking at the same timeframe. select receivedat, url from .pages where url like warehouses and search like ttg2 order by receivedat we hope this overview helps explain the technical nuances of measuring what happens when a customer finds you using an ad! if you have any other questions, feel free to share them in the segment community for discussion. this page was last modified: 21 apr 2023 need support? questions? problems? need more info? contact segment support for assistance! help improve these docs! was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! get started with segment on this page was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! product for developers company support 2025 segment.io, inc. scraped from: https:segment.comdocsguideshowtoguidesmeasuremarketingroi measuring the roi of your marketing campaigns on this page the purpose of marketing campaigns is to drive traffic and sales. but how do you know which campaigns yield the most conversions or what channel across the campaigns was most effective? this guide provides you with the tools to answer these questions with sql so that your marketing team can reproduce the hit campaigns and consistently generate loyal customers. talk to a product specialist to learn how companies like warby parker and crate barrel use a data warehouse to increase engagement and sales. analyze campaign performance the goal of marketing campaigns is to drive engagement and conversions. most commonly performed by attracting traffic to the site, these campaigns use utm parameters for attribution. in our analysis, well be heavily relying on utm parameters to analyze not only campaign, but also channel performance. learn how to effectively use utm parameters in your marketing campaign strategies. for our analysis walkthrough, well use fictitious ecommerce and marketing data from ondemand artisanal toast company, toastmates. toastmates is currently running these two campaigns: each of these campaigns used a combination of channels. here is a table with the channels and corresponding utm parameters so when we build the sql query, we can make sure all of the traffic sources are accounted for. well use sql below to measure the performance of each campaign and what that means for future marketing activities. build the funnel the following query creates a table where each row is a customer and the columns are the date time when a key funnel event happens that have the contextcampaignname to match that of the utmcampaign . the key funnel events in this analysis are store visitedbased on a page view to the store url, product viewed , and order completed . given that each channel may have some key top of the funnel action that is unique to itself, lets save that analysis for when were analyzing across channels. contextcampaignname utmcampaign store visited product viewed order completed feel free to copy and paste the below query for your analysis so long as you replace nationaltoastday with your own utm campaign parameter. nationaltoastday with users as select from toastmates.users , pageviewed as select p.receivedat as pageviewedat, p.contextcampaignname, p.userid from toastmates.pages p left join users u on u.id p.userid where p.contextcampaignname is not null and p.url ilike toastmates.comstore , productviewed as select v.receivedat as productviewedat, v.contextcampaignname, v.userid from toastmates.productviewed v left join users u on u.id v.userid , ordercompleted as select c.receivedat as ordercompletedat, c.contextcampaignname, c.userid from toastmates.ordercompleted c left join users u on u.id c.userid select p.userid as userid, pageviewedat, productviewedat, ordercompletedat, p.contextcampaignname from pageviewed p left join productviewed v on p.userid v.userid left join ordercompleted c on p.userid l.userid order by 5 desc here are the first four rows of the resulting table: then, we can use tweak the query above into the one below to perform some simple count and sum on the previous table to get conversion metrics as well as total revenue derived from the campaign. count sum with users as select from toastmates.users , pageviewed as select p.receivedat as pageviewedat, p.contextcampaignname, p.userid from toastmates.pages p left join users u on u.id p.userid where p.contextcampaignname is not null and p.url ilike toastmates.comstore , productviewed as select v.receivedat as productviewedat, v.contextcampaignname, v.userid from toastmates.productviewed v left join users u on u.id v.userid , ordercompleted as select c.receivedat as ordercompletedat, c.contextcampaignname, c.total, c.userid from toastmates.ordercompleted c left join users u on u.id c.userid select p.contextcampaignname, countpageviewedat as storevisits, countproductviewedat as productviews, countordercompletedat as orderscompleted, sumtotal as totalrevenue from pageviewed p left join productviewed v on p.userid v.userid left join ordercompleted c on p.userid l.userid group by 5 order by 5 desc here is the resulting table: this analysis not only gives us a great snapshot of the conversion points along each campaigns funnel, but also shows that weve generated 3,100.37 from the national toast day campaign and 3,824.68 from the toast your friend campaign. also we can see that the quality of the traffic from the national toast day is higher, but weve had more total traffic from toast your friend, which makes sense since its an ongoing campaign. but this is not yet roi, since we havent incorporated the spendthe labor of your marketing team and the paid acquisition channels to source part of this trafficthat went into these channels. add campaign costs the main costs that are incorporated in an roi calculation are salaries prorated by personhour and media spend. while we could conceivably create a custom, static table in sql that contains the spend information over time, the faster and more practical way would be a back of the envelope calculation. the costs associated with a given campaign consist of two major pieces: the personhour cost and any associated media spend. calculating the prorated personhour is an estimate of the number of hours and people used to set up and manage the campaign, then multiplied by the hourly rates based off their annual salaries. the media spend is the advertising cost for distributing creatives to generate traffic to your store want to easily export advertising data from google adwords or facebook ads? check out segment sources. when we have the aggregate cost numbers, the formula for roi is: campaign roi profit attributed to campaign campaign cost campaign cost here is a spreadsheet to illustrate the roi calculation for both campaigns: though roi numbers are one success metric, its an important benchmark for comparing performance when launching new campaigns or comparing against past campaigns. but how can we go one step further and see what worked and what didnt? one approach is to see which channels convert better, so you know how to adjust your marketing spend or media buys in your current campaigns or future ones. analyze channel performance a single campaign can include a wide variety of channels: email, display ads, push notifications, forums, etc. all of which yields different engagement and conversion rates. effective marketers will keep a pulse on each channel throughout the duration of the campaign to understand whether a target audience is being saturated, a creative refresh is needed for advertising, or how to efficiently allocate future spend towards a source that converts. the analysis is similar to measuring the performance across a single campaign, with the only change being finding events where we focus on contextcampaignmedium or contextcampaignsource instead of contextcampaignname . the sql below measures the conversion rates at key funnel events for nationaltoastday , but broken down by utmmedium . contextcampaignmedium contextcampaignsource contextcampaignname nationaltoastday utmmedium you can copy the below into your favorite editor, as long as you change out the contextcampaignname and contextcampaignmedium parameters to ones that applies to your business. contextcampaignname contextcampaignmedium with users as select from toastmates.users , pageviewed as select p.receivedat as pageviewedat, p.contextcampaignname, p.userid from site.pages p left join users u on u.id p.userid where p.contextcampaignname nationaltoastday and p.contextcampaignmedium is not null and p.url ilike toastmates.comstore , productviewed as select v.receivedat as productviewedat, v.contextcampaignmedium, v.userid from toastmates.productviewed v left join users u on u.id v.userid , ordercompleted as select c.receivedat as ordercompletedat, c.contextcampaignmedium, c.userid, c.total from toastmates.ordercompleted c left join users u on u.id c.userid select p.contextcampaignmedium as utmmedium, countpageviewedat as storevisits, countproductviewedat as productviews, countordercompletedat as orderscompleted, sumc.total as totalrevenue from pageviewed p left join productviewedat v on p.userid c.userid left join ordercompleted c on p.userid c.userid group by 1 order by 1 desc the resulting table: since the national toast day campaign is relatively new, the majority of the traffic is from the email and an article news. but we can see that the social channels have a lower conversion from store visits to product views. email has the best overall conversion to revenue, which may be attributed to the recipients already familiar with the toastmates brand or having previously had a stellar endtoend shopping experience. we can further breakdown this analysis by seeing which email, display ads, and social channels performed the best, by adding utmsource and utmcontent ,assuming that youve properly added them in your earned and paid media links. also note that this preliminary analysis in sql doesnt account for doublecounted users, who had impressions with our brand on multiple channels e.g. someone seeing a display ad, yet converted on the email outreach. fortunately, there are multitouch attribution models that can be applied to better understand the weights of each activity towards conversion. utmsource utmcontent learn more about multitouch attribution models. build repeatable hit marketing campaigns measuring the roi and performance of marketing campaigns and marketing channels tells a compelling story about what types of campaigns resonate with your audience. how does your audience like to be engaged? text, push notifications, email? what campaign messaging hooks work the best in getting them back at your store? you can apply this analytical approach and performance measurement techniques to a wide variety of marketing activities, such as offline marketing, billboards, or sponsoring events. these insights can empower your team to focus on what works and eliminate what doesnt. talk to a product specialist to learn how companies like warby parker and crate barrel use a data warehouse to increase engagement and sales. this page was last modified: 21 apr 2023 need support? questions? problems? need more info? contact segment support for assistance! help improve these docs! was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! get started with segment on this page was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! product for developers company support 2025 segment.io, inc. scraped from: https:segment.comdocsguideshowtoguidesmigratefromothertools migrating code from other analytics tools on this page switching from your current clientside javascript event tracking to segment is easy. below you can find migration guides for the following tools: if youd like us to add more tools or mobileserverside examples to this guide let us know! google analytics custom events google analytics custom events are simple to record in segment. youll record them with our track method and use the same properties you would when sending to google analytics directly. the only mapping exception is the event action. that will automatically be populated by the event name you include in the track call. heres an example: gasend, hittype: event, eventcategory: account, eventaction: signed up, eventlabel: premium, eventvalue: 4 ; becomes: analytics.tracksigned up, category: account, label: premium, value: 4 ; since event category is required well populate it with all if you dont specify one. you can read more about this in our google analytics docs. all ecommerce segment has full support for the google analytics ecommere api and the enhanced ecommerce api as well. make sure you follow our ecommerce tracking plan to make sure youll be able to use all ecommerce features in the tools we support. for an ecommerce transaction to appear in google analytics youll need to enable ecommerce for your google analytics view and send an order completed event to segment. this simplifies things a lot compared to the direct google analytics code. heres an example: garequire, ecommerce; gaecommerce:addtransaction, id: 93745, revenue: 30, shipping: 3, tax: 2, currency: usd ; gaecommerce:additem, id: 23423, name: monopoly: 3rd edition, sku: j9032, category: games, price: 19.00, quantity: 1 ; gaecommerce:additem, id: 22744, name: uno card game, sku: q9332, category: cards, price: 3.00, quantity: 2 ; gaecommerce:send; becomes: analytics.trackorder completed, orderid: 93745, total: 46, shipping: 3, tax: 2, currency: usd, products: id: 23423, name: monopoly: 3rd edition, sku: j9032, category: games, price: 19, quantity: 1 , id: 22744, name: uno card game, sku: q9332, category: cards, price: 3, quantity: 2 at the very minimum you must include an orderid for each order and for each product inside that order you must include an id and name. all other properties are optional. custom dimensions through segment you can record userscope custom dimensions using our identify, page, or track methods. a full explanation can be found in our google analytics docs page, but heres a quick example: gaset, dimension5, male; gasend, pageview; becomes: analytics.identify gender: male ; analytics.page; this example assumes you have already mapped gender to the correct dimension in your segment source settings for google analytics. everything else to see a full list of google analytics features and how they work through segment read our google analytics docs page. mixpanel event tracking event tracking is mixpanels bread and butter. below are all the relevant mixpanel functions and how you can map them to segment functions. switching your event tracking from mixpanel to segment couldnt be easier. our trackmethod maps directly to mixpanels. the event name is the first argument and the event properties are the second argument. mixpanel.trackregistered, type: referral ; becomes: analytics.trackregistered, type: referral ; the identify method in mixpanel is used to merge together events from multiple environments so your unique events number is accurate and your funnels dont break. since mixpanel.identify only takes a single argument a userid it maps directly to our identify method: mixpanel.identify mixpanel.identify123; becomes: analytics.identify123; mixpanel has the idea of super properties, which are user traits that get attached to every event that the user does. in segment you can set mixpanel super properties using our identify method. super properties are only supported in clientside libraries analytics.js, ios, android. heres an example: mixpanel.register gender: male, haircolor: brown ; becomes: analytics.identify gender: male, haircolor: brown ; this also works when you include a userid argument in your identify call. alias alias is necessary in mixpanel to tie together an anonymous visitor with an identified one. the mixpanel and segment alias methods both work the same. in clientside javascript passing a single argument will alias the current anonymous or identified visitor distinctid to the userid you pass into it: mixpanel.alias1234; becomes: analytics.alias1234; track links if you are tracking links with mixpanels tracklinks helper you can switch that code to the segment tracklink helper function in analytics.js. and heres an example: track click for link id nav mixpanel.tracklinksfreetriallink, clicked freetrial link, plan: enterprise becomes: var link document.getelementbyidfreetriallink; analytics.tracklinklink, clicked freetrial link, plan: enterprise ; track forms if you are tracking forms with mixpanels trackforms helper you can switch that code tothe segment trackform helper function in analytics.js. and heres an example: track submission for form id register mixpanel.trackformsregister, created account, plan: premium ; becomes: var form document.getelementbyidregister; analytics.trackformform, created account, plan: premium ; people tracking mixpanel people tracking is a separate database from the event tracking outlined above. for that reason there are separate api methods to record data to mixpanel people. this method sets people properties in mixpanel people. in segment you will use ouridentify method to accomplish this. heres an example: mixpanel.people.set email: jake.petersonexample.com, name: jake peterson ; becomes: analytics.identify email: jake.petersonexample.com, name: jake peterson ; this also works when you include a userid argument in your identify call. as you can see segment also recognizes special traits like email and name and translates them to the keys that mixpanel expects we automatically add the dollar sign. for more information check out our mixpanel docs. increment to use mixpanel increment through segment you wont event need anything in your code! all you have to do is list the events youd like to increment automatically in your mixpanel destination settings. read more in our mixpanel increment docs. revenue mixpanels revenue report requires the use of a special function called trackcharge. in segment that special function becomes a simple track call. by using the event name order completed well also use that event for any tools you use that recognize our ecommerce spec. trackcharge order completed mixpanel.people.trackcharge30.50, orderid: f9274 ; becomes: analytics.trackorder completed, revenue: 30.50, orderid: f9274 ; this page was last modified: 07 nov 2023 need support? questions? problems? need more info? contact segment support for assistance! help improve these docs! was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! get started with segment on this page was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! product for developers company support 2025 segment.io, inc. scraped from: https:segment.comdocsguideshowtoguidessegmentandattribution segments role in attribution on this page at a higher level, attribution tools allow you to connect a specific campaign to user acquisition, giving you more visibility into campaign performance. see the destination catalog for a list of attribution tools that segment supports. there are three stages of mobile attribution as it relates to segment. customer installs your app the install is attributed by an attribution provider adjust, appsflyer, etc attribution information is sent back to segment here is a bit more information on what is happening at each of those stages. customer installs your app when lifecycle events are enabled, the application installed and application opened events are triggered on the first app open after the app is installed. note, if the app is deleted and then later reinstalled on the device, these events will be triggered again on first app open. situations where install counts look lower in segment than in other tools. some tools, like itunes or google play, count install on download rather than on app open like segment. itunes and google play is able to easily collect data on download but not as easily able to collect firstparty data on app open. whereas other tools, such as segment, need their sdk to be loaded in app and initialized on app open before they are able to collect the install information. for example, if a user downloads your app but does not open it, the install will be counted in itunesgoogle play but not counted in segment or other tools. situations where install counts look higher in segment than in other tools many tools deduplicate install data. some tools only allow one install event per lifetime of deviceid. others deduplicate by deviceid accepting only one install per utc day. each and every tool is different. segment, on the other hand, does not deduplicate. we dont believe our role in your data pipeline should be deduping particular events. in fact, there may be situations where you may want to account for multiple application installed events such as: user sells their phone, user uninstalls and later decides to reinstall, etc. it is better to think about the application installed data in your segment warehouse as the raw source of data, giving you flexibility to query for more information on how installs are counted in different tools, here are a few resources from our partners: adjust discrepancies and why data does not always match up the install is attributed by an attribution provider devicemode connection when you enable an attribution destination in devicemode, our integration code will also load that tools sdk. upon app launch, the destinations sdk will send install information which is then use to attribute that install to a campaign on their backend. segment loads the destinations sdk, but attribution happens outside of segment. cloudmode connection destination receives the application installed event and attributes the installation on their backend. attribution information is sent back to segment devicemode connection for tools that support this, if you have enabled track attribution data in your segment dashboard, our integration listens to the attribution tools sdk for a change in attribution state. note: not all devicemode attribution tools offer track attribution data functionality. see the settings section for a particular tool in your segment dashboard for confirmation. when there is a change in attribution state, the integration code triggers an install attributed call to be sent back to your segment source and on to all other enabled destinations in device and cloudmode. here is an example of how that call is triggered in the appsflyer integration code. this is the similar for other attribution providers such as adjust. cloudmode connection for tools that support serverside postback, after install is attributed, an install attributed event is triggered and sent serverside to your segment source and forwarded on to all enabled cloudmode destinations. example install attributed event: analytics.trackinstall attributed, provider: tuneadjustappsflyer, campaign: source: networkfbadwordsmopubsource, name: campaign name, content: organic content title, adcreative: red hello world ad, adgroup: red ones ; for more detailed information on a particular attribution destination and functionality, see our destinations docs. this page was last modified: 07 feb 2023 need support? questions? problems? need more info? contact segment support for assistance! help improve these docs! was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! get started with segment on this page was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! product for developers company support 2025 segment.io, inc. scraped from: https:segment.comdocsguideshowtoguidessetupnotificationsalerts how do we set up eventtriggered notifications or alerts? on this page below youll find a bunch of ways to set up notifications for yourself based on the data youre sending through segment. connections alerting connections alerting allows segment users to receive inapp, email, and slack notifications related to the performance and throughput of an eventstreaming connection. connections alerting allows you to create two different alerts: for more information about connections alerting, see the connections alerting documentation. google analytics custom alerts you can use google analytics custom alerts to send yourself emails whenever a specific traffic segment drops below or above a threshold you set. learn how to set up email alerts in googles documentation. analytics email summaries with tools like amplitude, kissmetrics, and mixpanel, you can set up email reports delivered to you on a daily basis. they are completely customizable, so you can keep an eye on as many events or other metrics youd like. realtime traffic monitoring chartbeat and gosquared both offer awesome realtime dashboards to see whats happening right now on your site. they both include the option to get notified when your traffic hits a certain threshold. for example, if your onsite visitors is less than 100 people, or more than 1,000. gosquared also offers indepth historical and user analysis. chartbeat sticks to realtime anonymous traffic, but offers some sweet features for publishers. webhookbased alerts the last option segment recommends is to use a monitoring tool like pagerduty or datadog and point segments webhooks destination at them. that way you can set up custom alerts in their system. eventtriggered emails the last option for alerting based off of segment events is to use one of the email tools available on the segment platform that offers eventtriggered emails. your options there are customer.io, vero, autopilot, outbound, klaviyo, or threads. this page was last modified: 30 may 2024 need support? questions? problems? need more info? contact segment support for assistance! help improve these docs! was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! get started with segment on this page was this page helpful? thanks for your feedback! can we improve this doc? send us feedback! product for developers company support 2025 segment.io, inc. extracted text from pdf1.pdf page 1 table of contents 1. activate audience on destinations. ........cceccesseesseeseeeceseeeeeeeeeeecseesaesacseeeaeaesesaesacseesaeeaseesaeeaseeseaseeeeeseateee page 2 1. activate audience on destinations activation is the process of linking the cohort of customers to various advertising platforms, such as facebook, adform, twitter and more. this allows you to implement focused marketing strategies that align with the criteria established for the target cohort. after defining the audience, you can activate it on a destination either immediately or you can choose to activate it later. you can revisit this audience later for further modifications. e to activate the destination immediately, after defining the criteria for your audience, on the last screen of the audience creation, click activate now. eo your audience is saved how would you tke to proceed with this audience? activate later enable membership e this takes you to a new screen, where you can either choose to enable ab testing on the audience or activate the created audience by linking it to a specific destination as shown in the image below. o set ab test this functionality is exclusively available for users who have opted for it. selecting this option takes you to a screen where you can split the audience into two or more variations. for more information about ab testing and its application, refer here. link to destination this functionality helps you to activate the created audience by linking to a destination. for stepbystep instructions on how to page 3 activate an audience, refer here. your audience is saved how weal you keto aetiatethi set ab test create audience previous page whats next next page need support? questions? problems? need more info? contact us, and we can help! raise a ticket extracted text from pdf2.pdf page 1 table of contents 1. best practices for using zeotap cdp.........ceccssccssccsscesscssscssecssecssecesecseeesseceecseecsessseecseceseceseceseceseeesesueeesessees page 2 1. best practices for using zeotap cdp this section offers helps you with the best practices and recommendations to accurately set up the zeotap cdp account across different implementation stages. this section covers the following topics: e validating data before onboarding e setting up organisation and user e setting up sources e setting up destinations e setting up predictive audience e setting up journeys e setting up consent orchestration e setting up profile api whats next previous page validating data before onboarding next page need support? questions? problems? need more info? contact us, and we can help! raise a ticket extracted text from pdf3.pdf page 1 table of contents 1. components of the source listing page..........cccscesscssscssceecseecssecsseeseeesecsecesesesecssesesesesecesecesecessceeeceeeseeesges page 2 1. components of the source listing page the source listing page provides information about a source created and some key information about the source. integrate sources active archived ous mapped 200 ul 17,2024, 1308, a7, 2024, 12:46 mapped 490 aug 12,2024, 1408 sa.09, 2024, 14:8 s4.09, 2024, 13837 mapped 16 1ul02, 2024, 2008 41 02,2024, 18:30 the source listing page displays the columns as mentioned below. colu mn details id the id is associated with the source and is autogenerated by the system. sour the name of the source as provided by you. along with the source name, ce the following two details are also visible: name category an icon indicating the source category appears beside every source. e data source the data source appears under every source name. for example, flat file, website events and so on. e data entity the data entity tag appears besides the source name. depending on the data type, it can be either ce or nce. note: an alert icon appears beside the source if any erroneous record is ingested in the last 90 days. this alert remains until the errors are fixed. in case of flat files, the erroneous files are deleted and replaced with the page 3 colu statu total reco rds inges ted details correct files. the status of a source. the following are the available statuses: created indicates that the source has been created in sources, but it is yet to receive any data. integrated indicates that the data has started flowing into the source, but the taxonomy standardisation is yet to be applied. at this point, you can preview the incoming data through the preview data tab of the particular source. mapped indicates that the data collections are created on top of an integrated source based on the common taxonomy. this means that at least one data collection exists for this source. archived indicates that the source is archived for future ingestion. to know more about archived sources, refer to archived source. paused indicates that the source is paused temporarily for both data collection and ingestion. paused collection indicates that the source is paused temporarily for data collection. paused ingestion indicates that the source is paused temporarily for data ingestion. the total record count of the data ingested and are available for activation from this source. on hovering over the number, a pie chart appears with the following details: ingested the total number of records ingested and are available for activation. failed the records that failed to ingest during ingestion. ingestion efficiency the percentage of the total records ingested versus the page 4 colu last inges ted crea ted date crea ted details total records received for the ingested records e last refreshed date the last date and time when this information in the popup was updated. total records ingested last ingested 160 k 8th sep 2021, 21:30 hr a e ingested 160k failed 30k ingestion efficiency 80 note: the data displayed here is for the lifetime of the source. this means that the data displayed here is from the time the source was created in zeotap. the timestamp of the last fully processed ingested recordfile for this source. the date when this source was created. the user id of the users who created the source. page 5 colu details by actio click the three dots to choose an action from the dropdown menu. based on ns the status of a source, you can pause, rename, archive or edit the mapping for the source. you can also pause data collection, pause data ingestion, resume data collection or resume data ingestion, depending on the source type and status, as explained in the table below. source source action impact type status time to effect all temporarily streaming sources pause pauses the integrated : where data collection of zeota or up to 1 hour collection data from p mapped fetches the source or collects data up to 15 minutes note: e for batch temporarily sources, pauses the if any file is pause : : ingestion of already naa ; source data allsources mapped in the ngestion into the process of system ingestion, the records will continue to ingest until the page 6 colu resumes the resume . collection data . of data from collection the source resumes the resume ingestion data of source . data into the ingestion system note: details all streaming sources where zeotap fetches or collects data all sources paused or paused collection paused or paused collection takes effect. e for streaming sources, ingestion should pause within 15 minutes. up to 1 hour up to 15 minutes pausing data collection invalidates the write key on the source page, stopping data collection. to avoid errors on the source page, remove the tags before performing this action. pausing data ingestion and data collection for a source moves the source to the paused state. page 7 colu details e saving the mapping for a paused source moves the source to the mapped state. click the view source report button to view the complete data journey and record status information for each source in one place through the followings charts: note that the dashboard refreshes every 8 hours. chart 1 the source ingestion trend chart visualises cumulative event data over time. the yaxis represents the total number of events cumulative, and the xaxis shows the start of the week. the chart data is based on ingested records. the chart supports filtering for the last 7 days, 30 days and 90 days to meet specific needs. you can also download the chart information in csv format based on the selected filters. source ingestion trend select sources 60m 40m 20m cumulative records 10m 8m. wk 26 jun 24 wk27jul01 wk28jul08 wk 29 jul 15 wk 30jul 22 wk 31 jul29 wk 32aug0s wk 33aug 12 wk 34aug19 wk35 aug 26 hh ingested records chart 2 the batch vs streaming ingestion trend chart displays the count of ingested events over time. the yaxis represents the number of ingested events, and the xaxis represents the date. the chart distinguishes between two types of data sources: page 8 colu details e batch sources e streaming sources batch vs streaming ingestion trend 100m ingested records count wk 26 jun 24 wk27jul01 wk 28 jul08 wk 29jul 15 wk 30jul22 wk 31jul29 wk 32aug05 wk33aug 12 wk34aug19 mi bach ml stream chart 3 the top sources ingestion trend chart displays the number of ingested events over time for your top data sources. the yaxis shows the count of ingested events, and the xaxis shows the start of the week date. the chart reflects the top sources with the highest event counts. you can filter the chart to view the top 5, top 10 or top 15 sources based on cumulative ingestion records, giving youa top sources ingestion trend view. page 9 colu details top sources ingestion trend top 5sources 10m ingested records count wk 26 jun 24 wk27julol wk28 jul08 wk 29jul15 wk 30jul22 wk 31 jul29 wk 32augos wk 33 aug 12 wk34aug 19 wk35 aug 26 wi customersprod ml eventimde ml ordersprod mj unsubsprod. chart 4 the batch sources report provides detailed information about batch sources and their event data. the report includes the following columns: e date: the date the batch was received. e source name: the name of the source. e source type: the type of source for example, zeotap google cloud, sftp, js, web pixel and so on. e file name: the name of the file ingested. e status: the current status of the batch. e received records: the number of records received from the source. e ingested records: the number of records successfully ingested. page 10 colu mn batch sources report file drop date sep 22, 2024 sep 21, 2024 sep 20, 2024 sep 19, 2024 sep 18, 2024 sep 17, 2024 sep 16, 2024 sep 15, 2024 source name customersprod ordersprod unsubsprod customersprod ordersprod unsubsprod customersprod ordersprod unsubsprod customersprod ordersprod unsubsprod customersprod ordersprod unsubsprod customersprod ordersprod unsubsprod customersprod ordersprod unsubsprod customersprod ordersprod unsiths rnd details source type file name sftppull sftppull sftppull sftppull sftppull sftppull sftppull sftppull sftppull sftppull sftppull sftppull sftppull sftppull sftppull sftppull sftppull sftppull sftppull sftppull sftppull sftppull sftppull sftpipull table column selection integrate sources active archived 45 search source out test cache a website events el ied ig upc tt website events ge test cache fiat fites jin test stream ingestion website events stavus created created created created total records ingest. lastingested 4 created date aug 09, 2024, 23:50 aug 09, 2024, 23:26 aug 09, 2024, 23:25 aug 09, 2024, 22:52 select sources 06051820240922zeotapcrmcus 06051820240922zeotapcrmink 06051820240922zeotapcrmun: 06051420240921zeotapcrmcus 06051420240921zeotapcrmink 06201420240921zeotapcrmun: 06044820240920zeotapcrmcus 06044820240920zeotapcrmink 06044820240920zeotapcrmun: 06042520240919zeotapcrmcus 06042520240919zeotapcrmink 06042520240919zeotapcrmun: 06190620240918zeotapcrmcus 06190720240918zeotapcrmink 06040720240918zeotapcrmun: 06034920240917zeotapcrmcus 06034920240917zeotapcrmink 06034920240917zeotapcrmun: 06033220240916zeotapcrmcus 06183220240916zeotapcrmink 06183220240916zeotapcrmun: 06031420240915zeotapcrmcus 06031420240915zeotapcrmink 06 92 14 20240915 zentan crm un: total sources: 215 columns to show 10 source name status total records ingested last in created date created by you can choose to display specific columns from the dropdown menu as shown in the image above. however, note that id, source names, status and total records ingested are mandatory and you cannot remove the selection. 10 page 11 sources previous page components of the preview data tab next page need support? questions? problems? need more info? contact us, and we can help! raise a ticket 11 extracted text from pdf4.pdf page 1 table of contents 1. create calculated attributes...... cee eecsceeseeseeseeeesseeecesseesecsesseesecsecsecsecsesseesessesseesessesseesessesaeeeseseseeesaees page 2 1. create calculated attributes this step allows you to derive userlevel insights by aggregating your users isolated actions. you can then use this data to create more powerful customer cohorts. as a marketer, you can use calculated attributes to create new attributes for a user by aggregating their event data over a specific time period. for example, 9dayrevenue of a user, 1weekpageviews to check the engagement of a user, unitspurchased by a user for a specific category like tshirts. these calculated attributes are used as segmenting criteria and can then be forwarded to different integrations. for example, in a workflow, you can define high spenders as users with 9dayrevenue 500 or low engagement users by putting 1weekpageviews 5 criteria. for more information about calculated attributes, refer here. description otal cart checkout value in the last 3 months runcompuraion over entire data map the catalogue previous page create audience next page need support? page 3 questions? problems? need more info? contact us, and we can help! raise a ticket extracted text from pdf5.pdf page 1 table of contents 1. create a soule... eee cecescenscseseseessesesessaeesseeeeceeeeaeesseesseeeseeeseeeseeesecesecseesecesesesesessueeeseseseseseaeesseeeseeeseeeaeees page 2 1. create a source to get started with zeotap cdp, begin by creating a new source to bring data to zeotap cdp. you must also select a source category that align with your specific needs and create the source accordingly. the following are the various source categories supported in zeotap cdp: e website events: to send data from your websites to zeotap cdp, the following are the two commonly used methodsfiles to implement website event tracking: o web js, this is a clientside library that can be implemented on websites to track events, page visit information, user logins, user details and any other information relating to the product or services offered on the website. for more information about webjs source, refer here. o pixel files, also known as tracking pixels or web beacons, are small, invisible elements embedded within web pages to collect information about user behaviour and interactions. for more information about pixel files, refer here. e app events: to collect customer data in mobile applications, we integrate our native android and ios sdks software development kits. these sdks track user interactions and capture events within the app. the following are the two native sdks available in the system: o android sdk, this is designed to support all android devices and tablets, including amazon fire tv. the sdk simplifies the process of sending data to any tool without having to implement a new api every time. for more information about android sdk, refer here. o ios sdk, this supports all ios devices and tablets. the sdk simplifies the process of sending the data from your ios app to zeotap. for more information about ios sdk, refer here. e flat files: to store data collected from external sources, often in formats like csv or json, which can be directly uploaded from the sources user interface or through page 3 other methods like the gcloud console, gs utils, or through thirdparty tools like cyberduck. the following are the two ways for transferring batch data using flat files: o zeotap google cloud storage, zeotap supports importing of data collected from other sources or stored outside collect onto the platform. this data can be in the form of csv or json files. for more information about zeotap google cloud storage, refer here. o sftp secure file transfer protocol, sources supports importing of data collected from other sources or stored outside sources onto the platform. as an organisation, you can use the sftp source to onboard your data to zeotaps sftp server using one of the different modes of connection that we recommend. for more information about zeotap sftp sources, refer here. server to server: sources can be registered for servertoserver data transfer under the http api source. the source details contain the api endpoint and the writekey to be used for sending the data. tag managers: provides a userfriendly interface that allows marketers and website administrators to add, update, and manage tracking codes without the need for direct involvement from developers or frequent code changes on the website. the following are the two popular tag managers that zeotap offers: o adobe experience platform tag extension, zeotap provides the zeotap collect tag extension within the adobe experience platform marketplace to capture events and user identities and enable cookie syncing on the web. for more information about adobe experience platform tag extension, refer here. co google tag manager, the zeotap collect tag is available as a custom template on google tag manager for easy integration. this is a javascript tag that captures events and user information as the customers navigate your websites. for more information about google tag manager, refer here. page 4 crm data: refers to the comprehensive set of information about customers and their interactions with a business that is stored and managed within the crm platform. the following are the crm data source integrations that zeotap offers: o salesforce crm,the sources module supports importing data from the salesforce crm. salesforce crm stores data as standard objects that are like tables. for more information about salesforce crm, refer here. data warehouse: a data warehouse is a centralised and integrated repository that stores large volumes of structured and unstructured data from various sources. the following are the data warehouses integration that zeotap offers: o snowflake, zeotap brings simplicity to your data onboarding process by letting you connect directly to your source data residing in snowflake through sources. for more information about snowflake, refer here. co bigquery, zeotap simplifies your data onboarding process by letting you connect directly to your source data residing in bigquery through sources. for more information about bigquery, refer here. o amazon 3, zeotap brings simplicity to your data onboarding process by letting you connect directly to your source data residing in amazon aws s3 through the sources module of zeotap cdp. note that you can configure autosync of data between your s3 account and the source created in zeotap cdp by using the sync frequency feature. for more information about bigquery, refer here. customer engagement channels: these are powerful tools that enable businesses to interact with customers across multiple touchpoints, deliver personalised experiences, and collect valuable data on customer preferences, behaviours, and responses. the following are the customer engagement channels source integrations that zeotap offers: o braze currents page 5 o batch o dotdigital o hubspot source o airship batch o airship realtime data streaming o cleverpush batch source o cleverpush realtime source message queue: a message queue is like a buffer that receives messages in a specific order and forwards them to the concerned subsystem or application in the same order. message queues decouple the sender and recipient, allowing them to operate independently and at their own pace. consumers retrieve messages from the queue when they are ready to process them. they can retrieve and process messages independently and at their own pace, allowing for asynchronous processing .the following are the message queue integration that zeotap offers: o pubsub stream, pubsub publishsubscribe stream is a messaging pattern that allows different applications and services to communicate with each other in real time. in a pubsub system, messages are published to a central exchange or topic and subscribed to by various recipients. the recipients receive notifications or data as soon as new messages are published, which makes it an efficient and scalable way to exchange information in realtime . for more information about pubsub stream, refer here. o pubsub batch, pubsub publishsubscribe batch upload is a way to efficiently and reliably send multiple messages to pubsub topics in a single request. this feature page 6 allows you to save network requests and improve the performance and scalability of your applications. in pubsub batch upload, you create a batch file containing all the messages you want to send. the batch file can be read as often as necessary, but once it is exhausted, only new messages are sent. pubsub batch upload is useful for processing large numbers of messages at once in realtime applications, as well as in backend processes that process data over time . for more information about pubsub batch, refer here. how zeotap cdp works previous page implement the source next page need support? questions? problems? need more info? contact us, and we can help! raise a ticket extracted text from pdf6.pdf page 1 table of contents 1. create augicnce...... ee eeeceeeeccesceseeceeececseesecsecsecsecsecsecseeseesecsacsessaesecsesseesecsassassesseeseseesesseeseesassasaesesaseesieeaes page 2 1. create audience the following are the steps involved in creation of audiences in zeotap cdp: e create audience: upon successfully creating a source and ingesting your data into the zeotap system, the next step involves unifying this data by mapping it to the corresponding fields on the catalogue. subsequently, you can proceed to create a cohort of customers, commonly referred to as audience as per your use case. create new audience e define criteria for your audience: further, define the criteria for the created audience to qualify customers unified profiles. you can leverage the available attribute types to define your audience and create the optimal marketing strategy for them. the attribute types include attributes such as events, profile attributes, calculated attributes, consent, marketing preferences and more. to page 3 know more about attributes and how to apply them to your audience, refer here. lets define who your users are specify the conditions that a user mut atey to become part of his audance, users that dont mest the coniton below wal be ftred ou rom the audience, userwho have performed an event x where, event name equal x itcstorecheckoutpage x and event properties are a016 sttibute or ace nested block mm eli oc ewer tome equant x eheckor co sn ee od sq aaa seeioute or ada nested block add condition create calculated attributes previous page activate audience on destinations next page need support? questions? problems? need more info? contact us, and we can help! raise a ticket cotes 1 remove event properties add event property extracted text from pdf7.pdf page 1 table of contents 1. source integrations batchrealtimme.........cccccssccssccsecessceseceseceseceeseseesseecseececcseceseseaeecseseseseseseseseeeseseenseeeeees page 2 1. source integrations batchrealtime here you can find the sources integrations that are supported by the zeotap cdp. data delta categor attributes fetch source name y bupporte supported supporte d web javascript click here . customer web website data ce no pixelweb iframeap only p pixel android sdk click here . customer . ios sdk app events data ce click here no only react native . package click here hubspot customer yes data ce microsoft dynamics non any identifiers or crm crm data customer attributes entity nce salesforce crm data no batch braze currents e contacts customer customer e campaigns engagemen data ce no t channels only e preference dotdigital s e campaign opens e campaign page 3 source name categor airship batch airship realtime cleverpush batch cleverpush real time zeotap google cloud storage sftp push flat files sftp pull adobe experience platform tag extension tag manager google tag manager serverto servertoserver s28 server s28 data supporte d customer data ce non customer entity nce data customer data ce data customer data ce non customer delta attributes fetch supported supporte d clicks e campaign page views e named user e airship channel id no airship channel id channel id click here no click here no click here yes click here no no no page 4 source name categor snowflake bigquery data amazon s3 warehouse databricks pubsub stream message queue pubsub batch related topics data supporte d entity nce data customer data ce non customer entity nce data customer data ce non customer entity nce data attributes supported click here any identifiers or attributes any identifiers or attributes any identifiers or attributes delta fetch supporte d no yes yes yes no no for information about all the sources and destinations supported by zeotap cdp, refer here. onboard raw pil data into zeotap previous page website events ce next page need support? questions? problems? need more info? contact us, and we can help! raise a ticket 4 page 5 extracted text from pdf8.pdf page 1 table of contents 1. data or catalogue definition... cc cccccsccssecsssessecssccsecsseceseccsecesecesecesesesscsseseeesseecsecseecseseasecseseaeseseseseseneseges page 2 1. data or catalogue definition in this stage, you need to define the incoming fields and map it to the respective zeotap catalogue field, specify the sensitivity of the data, define consent and more. develop a schema document with transformation requirements define the data model define the sensitivity of the data establish timetolive strategy define the granular consent fields provide sample files for testing finalise the enrichers to use develop a schema document with transformation requirements ensure that you develop a schema document for each source, along with the following details: a schema document is a structured document that outlines the format, structure, and specifications of data within a dataset. in the context of your requirement, it serves as a comprehensive guide for handling data transformation needs. confirm the presence of at least one id, consent and country field. explicitly map incoming fields to the corresponding zeotap catalogue field in the schema document. specify consent details, considering the customers desired consent type, applicable channels and the creation of custom consent fields if the source lacks an explicit consent field. when no specific consent field exists, consider the entire dataset as consented for all purposes. define the data model page 3 define the consolidated data model obtained during the discovery stage, covering event, profile and other custom attributes. define the sensitivity of the data specify the sensitivity of data, including personally identifiable information pii and other sensitive data. pil and sensitive data are masked within the product and special personally identifiable information spil data must be classified as pii data. establish timetolive ttl strategy establish the timetolive ttl strategy for both persistent and nonpersistent ids. timetolive ttl is a strategy that defines the lifespan or expiration period of data within a system. it specifies the duration for which data, particularly identifiers ids, remains valid and accessible. the ttl strategy is crucial for managing data freshness, security, and compliance with privacy regulations. persistent ids typically have a longer ttl, often set to forever. these identifiers, like customerspecific ids, are meant to persist indefinitely. nonpersistent ids, on the other hand, have a defined ttl after which they expire. this duration is determined by the ttl strategy and ensures that outdated or irrelevant data is not retained unnecessarily. define the granular consent fields define the granular consent fields for each source along with the following details: e identify the attributes denoting consent. e determine the customers desired consent type. e define the applicable channels for consent. in cases where the source lacks an explicit consent field, establish a custom consent field and generate a hardcoded or derived enricher. if no specific consent field exists, the platform considers the complete data as consented for all purposes. provide sample files page 4 provide sample files to zeotap for testing purposes, adhering to best practices and recommendations for various source types. finalise the enrichers to use gather a list of required enrichers, such as datetime and currency transformations. for an exhaustive list of available enricher types within the system, refer here. discovery previous page set up id strategy next page need support? questions? problems? need more info? contact us, and we can help! raise a ticket extracted text from pdf9.pdf page 1 table of contents 1. discovely..........eccccseseceseeecesneecesceeseaeeceaceceeaceceeeecesaeceeaueessaeecseaeeceaeecesaecssaeeceeaeeceeaeecseaeecesaeeseseeeeeaeeseeeeeeatees page 2 1. discovery in the discovery phase, you can start by defining your use cases and understanding your customers requirements. prioritise use cases and map out data sources, destinations, usage timelines and more. you need to get clarity from your customers on the following aspects of using a cdp for their use case. by adhering to the instructions below, ensures a fast, hasslefree and successful integration. e get prioritised list of use cases for using zeotap cdp e get prioritised list of sources to integrate with zeotap e get prioritised list of destinations to target your customers e gather use cases for profile api e gather use cases for calculated attributes e gather use cases for journeys e other general requirements get prioritised list of use cases for using zeotap cdp obtain a prioritised list of use cases that your customers intend to address using zeotap cdp. this helps you to align the integration with their specific needs and objectives. for each use case, ensure to map out the source, destination and establish clear timelines for when they plan to employ them. this level of detail ensures that the integration is precisely tailored to their requirements. for information about some of the realtime use cases that we have solved through zeotap cdp, refer here. get prioritised list of sources to integrate with zeotap cdp obtain the prioritised list of sources, which you wish to integrate with zeotap cdp for transferring your customer data. in addition, ensure that you have the following details readily available regarding the source integration: page 3 e data model ensure that you have clearly defined the fields that you wish to send to zeotap through source integration. e onboarding format specify the preferred method for onboarding, such as flat file, api, data warehouse, sdk, or other applicable formats. e managing deltas verify that customers adhere to the practice of sending only delta updates, especially when utilizing flat files or establishing configurations within db tables. this ensures the efficient and incremental transfer of data, minimising redundancy. get prioritised list of destinations to target your customers obtain the prioritised list of destinations to concentrate your marketing efforts on targeting specific customer cohorts. in addition, ensure that you have the following details readily available regarding the destination integration: e credentials of the platforms secure the necessary credentials for the identified platforms. to understand the difference between platforms and destinations, refer here. e use cases to be activated be clear about the use cases to be activated on the destinations. for example, suppression, creating lookalikes in the platforms and more. e preferred output data fields define the preferred output identifiers for each platform. for example, for facebook emails, maids, or both; for braze first name, last name, email, braze id andsoon. gather use cases for profile api you can use our profile api to read, write and delete the user profiles from the zeotap system. ensure that you have the following details readily available for for effective use of our profile api: e identify data for profile api specify the data intended for utilisation through the profile api, such as segment membership or other profile attributes. note that event data is not supported through profile api. e delete api caller clearly define the caller of the delete api and the responsible system or individual for the delete api operation. page 4 for more information about profile api, use cases for profile api, workflow of the api, endpoint, best practices and more, refer here. gather use cases for calculated attributes for calculate attributes, identify the use case and possible conditions that you want to achieve. e for example, create a lead score for routing the leads to specific sales representatives. e for example, to show a welcome back banner to people who did not login in the last 7 days, precreate a calculated attribute counting last7dayslogin. e to target a customer when they are viewing a product they have been most engaged with, precreate a calculated attribute tracking mostviewedproduct in the last 3 days. 6 calculated attributes are computed daily and exclusively supported for events data. gather use cases for journeys for you to be able to use journeys and create workflows, ensure the following: e you have a realtime streaming source created in sources. e you have mapped the streaming source in your catalogue. a workflow runs on the event name and other attributes that are mapped in the e catalogue. hence, ensure that the above points are taken care of. other general requirements below are the general requirements that can be considered during discovery: e collect your precise countryregion requirements. e inform that default support for the eu is available; approval is required in the contract for other regions in terms of data residency. page 5 before you begin previous page data or catalogue definition next page need support? questions? problems? need more info? contact us, and we can help! raise a ticket extracted text from pdf10.pdf page 1 table of contents 1. get started with zeotap cdp... ceccscssccssscssccssccssessecsseecseceseceseceseccseseseceseceseseecseesseceeecseeeesseseseeeseseneees page 2 1. get started with zeotap cdp welcome to zeotap cdp, your gateway to a userfriendly customer data platform that streamlines data integration, enriches customer profiles, enables precise segmentation and facilitates personalised marketing across multiple platforms. this document is designed to provide you the crucial initial steps to get started with zeotap cdp and gives you a comprehensive overview of zeotaps capabilities. it also helps you to seamlessly integrate and utilise your data with zeotap cdp. whether you are new to data driven platforms or an experienced user, this document will walk you through the prerequisites steps that are essential before setting up your zeotap cdp account, the overall workflow of zeotap cdp and the best practices to be followed while using zeotap cdp. this section covers the following topics: e introduction e before your begin prerequisites e how zeotap cdp works e best practices for using zeotap cdp previous page introduction next page need support? questions? problems? need more info? contact us, and we can help! raise a ticket extracted text from pdf11.pdf page 1 table of contents 1. how zeotap cdp wofks........cccccscccsssssecsseeseccsseeseceecesscsseessecesecesesesesesesesecsuecesesseessesseeceeeseeeseseseseseeeaeseseeeneees page 2 1. how zeotap cdp works after gathering all necessary customer information as outlined in the prerequisites section, you can get started with zeotap cdp in the following order: 1. create a source 2. implement the source 3. preview data 4. map the catalogue 5. create calculated attributes 6. create audience 7. activate audience on destinations set up id strategy previous page create a source next page need support? questions? problems? need more info? contact us, and we can help! raise a ticket extracted text from pdf12.pdf page 1 table of contents 1. set up id strategy... ecescccesseeesseecesseeceseecesaeecseaeesesaeceeaaececaeeceeaeeceaaeseaeeceaeeeeesueeseaaeeseaeeeseaeeseeaaesseeeeeeanees page 2 1. set up id strategy ensure that you prepare the sourcespecific data dictionary listing attributes from each source and a data model illustrating the relationships between each source and their respective identifiers before proceeding with the configuration of the id strategy. we recommend you to develop an id catalogue document to define relationships between identifiers for the id strategy configuration, including all relevant identifiers within your accounts catalogue. once the id strategy configuration is finalised, document the corresponding data scenarios and replicate them in the zeotap cdp interface. following that, you can set up your id configuration. for more information about how to configure the id strategy for your account, refer here. by default, we enable the identify and link using all ids option, which takes into account all the id attributes across sources for resolving or creating user profiles. note that this is an irreversible process. any changes done after source creation oe are only applicable to the new data that is ingested. once the data mapping is complete, the customer profiles are created and unified as per the id strategy active in the account. on res ou m7 exp frew net oo sd ont oe sd ro et ei oon wie vo ei ones rote ror eid oes eo0kie goo ro sd see name gm ose os vet ei von vom econ cot ror eid ome data or catalogue definition previous page how zeotap cdp works next page need support? questions? problems? need more info? contact us, and we can help! raise a ticket extracted text from pdf13.pdf page 1 table of contents 1. implement the soulce.......cccceccesecsscssecssecesecssccsseceeceeecseeeseecseceseecsecesecesscesecesecesecaeseseceeeeeseceecesecaeseseessesegs page 2 1. implement the source integrate sources v xome back to sources, implementation details europe eu daay contacts once the source is created, proceed with its implementation. refer to the stepbystep instructions provided in the implementation guide tailored to the chosen source type. you can download this document from the implementation details tab of the source that you created. e toimplement a web js source, refer here. e to implement a pixel source, refer here. e to implement pixels on a site, ad or campaign, refer here. e toimplement android sdk source, refer here. e to implement ios sdk source, refer here. e to implement a react native package, refer here. e toimplement zeotap google cloud storage source, refer here. e to implement sftppush source, refer here. e to implement sftppull source, refer here. e to implement server to server source, refer here. e to implement adobe experience platform tag extension source, refer here. e to implement google tag manager source, refer here. e to implement bigquery source, refer here. e to implement snowflake source, refer here. e to implement amazon s3 source, refer here. page 3 e to implement batch source, refer here. e to implement braze currents source, refer here. e to implement dotdigital source, refer here. e to implement hubspot source, refer here. e to implement airship batch source, refer here. e to implement airship realtime source, refer here. e to implement cleverpush batch source, refer here. e to implement cleverpush realtime source, refer here. create a source previous page preview data next page need support? questions? problems? need more info? contact us, and we can help! raise a ticket extracted text from pdf14.pdf page 1 table of contents 1. inte grate... cece ecenneceeeeeeeeeeeeeeeeeeessaeeeeeeeseaaaaeeeeeeeeaaeeeeeeeensaaeeeceeseaaeaeeseeeenaeeeeeseeesaaeeeeeseeaaaaeeeeeeeeeeeeees page 2 1. integrate the integrate module within zeotap cdp streamlines the convergence of diverse data origins onto one cohesive platform, delivering a harmonised perspective of customers. comprising sources and destinations as integral submodules, integrate facilitates the collection, integration, and secure transmission of enriched customer data. sources centralises information from multichannel touchpoints and systems, fueling personalised marketing endeavours. whereas, destinations empowers efficient data conveyance to external platforms, such as dsps and dmps, for targeted campaigns. get started with integrate to get started with the integrate module, follow the stepbystep process outlined: ? note ensure to obtain initial customer information as detailed in the before you begin section of the get started guide before getting started with integrate. stepbystep process step 1 source creation: to get started with zeotap cdp, begin by creating a new source in the sources module. you must also select a source category and source type that align with your specific needs and create the source accordingly. for more information about how to create sources for different categories, refer here. step 2 source implementation: once the source is created, proceed with its implementation. refer to the stepbystep instructions provided in the implementation guide tailored to the chosen source type. you can download this document from the implementation details tab of the source that you created. page 3 integrate sources v zon back to sources implementation details previewoata catalogue mapping source report pe 25th sep 2023 customer data rope eu daly contacts step 3 previewing data: after implementation, you can examine the data that has been received into the system under the preview data tab. note that once the data starts flowing into the system, the status of the source changes to ntegrated. integrate sources v xom back to sources implementation details previewoata catalogue mapping source report looks good? get started by clicking onthe map to catalogue button on your right refresh data sechived created properties dated falco 20220506709:03:46.4052 ereatodate20220506t09:03:46.4052,e 20230918706:35:48.6072 false 20220517710:52:26.5332 ereatedate20220517710:52:26 5332, e 20230918706:3804,3062 false 20220517710:53:43.9652 ereatedate20220517710:53:43.9682,e20230918706:3957.8602 false 20220517710:53:43.9652 ereatedate20220517710:53:43 9682, e 20230918706:34022862 false 20220523713:38:53.7452 ereatedate20220523713:38553,7482,e 20221128711:04:469002 false 20220523713:38:53.7462 ereatedate20220523713:36553.7462,e20221128711:06499732 falco 2022052371338:53.7452 ereatedate20220523713:3853.7482,e 20221128711:0654.4612 falco 202205237133853.7452 crestedate20220523713:3853.7482,e20221126711:0453.6582 false 202205237133853.7452 crestedate20220523713:38553.7482,e 20221128711:06:47.7582 ? note if no data is ingested, then a message stating we havent received any data yet. please check again later and ensure that the source is implemented correctly. in case of any issues, contact supportzeotap.com message appears on the screen. page 4 step 4 catalogue mapping: this is the stage in which you can standardise the incoming data to a single organisationallevel catalogue by mapping and applying the required data transformations. ensure that your ingested data such as identifiers, traits, consent, events and more are appropriately mapped against the fields available in the zeotap catalogue. this ensures the structuring the data flow efficiently. you can map the ingested fields to the catalogue fields by clicking map to catalogue under catalogue mapping. for more information about the types of data that you can map and for detailed steps of mapping, refer here. integrate sources v xo back to sources implementat ion details previewoata catalogue mapping source report eee torunthe rfm and clv models, ensure that the you have mapped event timestamp and event ecommerce price, order value or cart value in 2 event cevent.categoryname event category event. productname product nome eventeventname event name event. productid product id eventproduct brand product brand ceventid cardia ? note you can only map a source that has moved to the ntegrated state. once the mapping is successfully done, then the source transitions to mapped state. post this state, the filedata dropped against this sources will be resolved and ingested into your zeotap system. step 5 create calculated attributes: this step allows you to derive userlevel insights by aggregating your users isolated actions. you can then use this data to create more powerful customer cohorts. as a marketer, you can use calculated attributes to create new attributes for a user by aggregating their event data over a specific time period. for example, 9dayrevenue of a user, 1weekpageviews to check the engagement of a user, unitspurchased by a user for a specific category like tshirts. these calculated attributes are used as segmenting criteria and can then be forwarded to different integrations. for example, in a workflow, you can define high spenders as users with 9dayrevenue 500 or low engagement users by putting 1weekpageviews 5 criteria. for more information about calculated 4 page 5 attributes, refer here. calculated attributes a seb 3 months purchase value deserptin total cart checkout value in last 3 months for each userla my database, calculate anew abut as: su or shoppingcare value x ad when they promed ad condition un computation over entre data. ancet save step 6 create your audience: upon successfully creating a source and ingesting your data into the zeotap system, the next step involves unifying this data by mapping it to the corresponding fields on the catalogue. subsequently, you can proceed to create a cohort of customers, commonly referred to as audience as per your use case. this audience can be further refined by applying specific criteria. step 7 activation: once your audience is welldefined, you can then activate it ona designated destination. to know more about how to activate the audience on the page 6 destination, refer here. segment audience zeotap checkout audience i google marketing porsonalistion platform webxite or amazon og google marketing google marketing 3 platform prattorm goo, prana markey tion keting google marketing email marketing platform camosian in this area, google marketing google marketing platform platform google marketing google marketing platform lov esasaas related topics for information about all the sources and destinations supported by zeotap cdp, refer here. previous page sources next page need support? questions? problems? need more info? contact us, and we can help! raise a ticket extracted text from pdf15.pdf page 1 table of contents l. introguction 0. ee eeeeeeeeeeseesceeeteceecseeeesecseesaeseesecsaesacseesaesacsaesaesaesassaesaesassaesasassaeaeaesaeeaseaeeaeeaseaeeaseaseaeeaeeaseass page 2 1. introduction in a nutshell, you first create a source within zeotap cdp to gather your customer data like events from your site or app processed either in batches or realtime. channel this plethora of data into the zeotap system in a specific format by mapping your incoming fields to zeotap catalogue fields. this forms unified profiles of your customers in the zeotap system based on the configured id strategy. you can then make use of another tool of zeotap cdp called segment, to create cohorts of your customers, known as audiencessegments. finally, link these audiencessegments to outbound platforms such as facebook, snapchat, airship, batch and so on, to achieve your use case using another zeotap tool named destinations. page 3 joe integrate start here discovery offline activities create a source destination review or define custom . preview save destination identifers required for id g unify data resolution in your accounts catalogue map the consent marketing preference to your accounts catalogue configure id strategy , 1 segment i orchestrate data processing in batches y a data processing in realtime a create audience create workflows link destination link destination integrate unify a segment i orchestrate protect page 4 use cases the following are some of the use cases that can be solved using zeotap cdp: unified customer profiles: cdp aggregates customer data from various sources to create unified profiles, ensuring a comprehensive understanding of individual customer behaviours and preferences. personalised marketing: cdp enables businesses to deliver personalised marketing messages by analysing customer data, enhancing the effectiveness of campaigns and increasing customer engagement. realtime data access: cdp provides realtime access to customer data, empowering businesses to respond promptly to customer interactions and deliver timely and relevant communication. crosschannel coordination: cdp ensures consistent messaging across different channels, maintaining a cohesive brand image and improving the overall customer experience. optimised campaigns: cdpdriven insights refine marketing campaigns, improving targeting accuracy and maximizing return on investment roi by tailoring strategies based on customer behaviour. customer retention strategies: cdp identifies potential churn indicators, allowing businesses to implement proactive customer retention strategies and personalised engagement to retain valuable customers. compliance with data protection regulations: cdp centralises customer data management, facilitating compliance with data protection regulations by ensuring secure and organised handling of customer information. effective suppression of existing customers: cdp suppresses existing customers from marketing campaigns to prevent repeated targeting, reducing marketing fatigue, and avoiding unnecessary outreach to those already engaged. optimising loyalty programs: cdp supports loyalty programs by tailoring promotions based on individual customer profiles, increasing customer engagement, and fostering loyalty through targeted incentives. preventing customer fatigue: cdp analyses customer interaction patterns to detect signs of fatigue, enabling businesses to adjust marketing strategies and content to maintain customer interest and satisfaction. page 5 get started with zeotap cdp previous page before you begin next page need support? questions? problems? need more info? contact us, and we can help! raise a ticket extracted text from pdf16.pdf page 1 table of contents 1. components of the preview data tabu........ccccccsccssscssecsscsscessecseccssessecsseceseceseessesesesseceseseeeseeseseceeseaeseeeeaees page 2 1. components of the preview data tab after successfully implementing the source, you can examine the data that has been received into the system under the preview data tab. the displayed data is a sample from the last five files that were ingested. once the data starts flowing into the system, the status of the source changes to ntegrated. ? note f no preview is available, then an error message s displayed stating the reason. you can try to fix the issue and try again after some time. in addition, ensure that the source is implemented correctly. in case you need further assistance, reach out to our support team at supportzeotap.com. zeotap integrate sources v back to sources: is tcflivestream 5. plementation details preview ata catalogue mapping source report giannis leme! n catal apping si 0 internal fields in grey suit an te as ssani04a2e3. f et 4sari04a2ea1 : bet the following table provides information about the different options available on the preview data screen, along with their functions: note that all internal fields will be in grey to distinguish from the external fields. no, option description view this option enables you to review a list of errors in the data of the 1 error dropped file log before mapping in the catalogue. examples include missing column names, page 3 sl 2 option refresh column filters description issues with file format and more. for a comprehensive list of error scenarios that you may encounter while working with the preview data tab, click here. this option enables you to refresh the sample data displayed under the preview tab. upon refreshing, it presents a different set of the latest sample data from the last five ingested files. it also provides the last refreshed date and time for your reference. this option allows you to conveniently filter and view specific columns in the given preview. use appropriate options to select all fields, select all the internal fields, clear the selections and more as per your requirement. page 4 sl option description columns to show q search column select all select all internal fields pageurl pagepagecategorylevel operatingsystem useremailsha1lowercase useremailsha256uppercase pubpurposesallowed userzs useremailmd5uppercase browserversion userzi browsername ismobile components of the source listing page previous page onboard raw pil data into zeotap next page need support? questions? problems? need more info? contact us, and we can help! raise a ticket extracted text from pdf17.pdf page 1 table of contents 1. map the catalogue.......cccccccssccsscssscssecssecssecsseesscesecseccsecssecsseseseceseceseceseceeecesesseesesessesesseaseeseeeaeseaeseseseseseneees page 2 1. map the catalogue this is the stage in which you can standardise the incoming data to a single organisationallevel catalogue by mapping and applying the required data transformations. ensure that your ingested data such as identifiers, traits, consent, events and more are appropriately mapped against the fields available in the zeotap catalogue. this ensures the structuring the data flow efficiently. map the ingested fields to the catalogue fields by clicking map to catalogue under either the catalogue mapping or preview data tab. a cae integrate sources zone back to sources implementation details preview data catalogue mapping source report rote the following are the important steps that you need to know while performing the catalogue mapping: 1. mapping customer and noncustomer entity data: you can map both customer data and noncustomer entity data in the catalogue. for more information about the how to map the catalogue for customer data and noncustomer entity data, refer here. 2. knowing zeotap standard fields zeotap provides a set of standard fields in your catalogue. if you do not find these standard fields in your organisations catalogue, you can create custom fields. the process of creating the custom fields happens in an interactive interface wherein you can define your data points, bring them into zeotap and manage them independently. using this interface, you can easily edit the existing catalogue field or create a new field. for more information about the standard fields, refer to the relevant below links. page 3 o zeotap standard fields co reserved catalogue fields . add a catalogue field while adding a new catalogue field, search for the desired field you wish to add. if the field is already present in the system, you are prompted to use the existing field and can access the details page. however, if the field does not exist in the system, you have to create a new field as explained in the add a catalogue field topic. . configure enrichers after adding the fields, you have to configure the required enrichers. enrichers are quick functions available for you to perform data transformations. zeotap enrichers can be broadly classified as data transformation enrichers and custom enrichers. . map the consent purposes once you have mapped all the identifiers, on the same screen, click add mapping. if a source has consent data, then select the incoming consent field and map it to the relevant zeotap consent field. for more information about mapping the consent purposes, refer here. . map the marketing preferences capture marketing preferences along with consent to add clarity and assurance to the marketing team while designing the campaign for the right audience. for more information about mapping the marketing preferences, refer here. . review mapping in the review mapping screen that appears, you can find warnings along with error logs that provide a description of the issue for incorrectly mapped fields. once you have corrected these fields, the system automatically refreshes, allowing the warning to disappear. for more information about reviewing the mapping, refer here. . save mapping when you have reviewed all the fields, click confirm and save. e once the mapping is successfully done, then the source transitions to mapped state. preview data previous page create calculated attributes next page need support? 3 page 4 questions? problems? need more info? contact us, and we can help! raise a ticket extracted text from pdf18.pdf page 1 table of contents 1. onboard raw pii data into zotap....... cc cecescssessscssecesecesecesecesecesecsseceseceeecssesseceecsesesessseseseeeseseseseseeeasensees page 2 1. onboard raw pii data into zeotap zeotap accepts both hashed as well as raw pil personally identifiable information into your zeotap account. however, we understand the sensitivity of storing and using pii data across the various channels of activation. hence, we provide a very secure environment to store and use your pii data within your zeotap access. this topic provides you with a view into the onboarding of raw pii data into zeotap and how zeotap treats and stores this information securely within your account. key benefits the following are some of the key benefits: e it opens up the activation of your customer data across channels, which only works with raw identifiers. e zeotap makes the raw pil data accessibility happen without compromising data governance and security. e raw pil identifiers enable better id resolution over oneway hashed values. how to onboard your raw pii data in zeotap? data ingestion and catalogue mapping raw pii can come from any source like flat files, javascript, pixels and mobile sdks. the common steps to be followed for any source type is to define your raw pii in your orgs catalogue. ? note flagging the raw pii fields in your orgs catalogue is a critical step and is the responsibility of the org. page 3 for streaming sources, some additional implementation level steps are required to pass the raw data through our native sources like web js, gtm, pixel tags. follow the steps as mentioned in the respective links below. e web js implementation guide for raw pll data e gtm implementation guide for raw pil data how raw pii data is stored and processed within zeotap encryption any data that is marked as raw pil in your organisations catalogue, is encrypted before ingesting and storing it in our graph or big query table. the pil information is masked across the ui for all roles once it is classified by you in your organisations catalogue. the encrypted data is only available to the downstream systems for activation purposes. ? note zeotap uses the rsa encryption algorithm as the encryption algorithm. if any channel source accepts or requires only raw pil ids like email addresses or phone numbers to execute the campaign set up by you, then the system decrypts and pushes such information as per the setup and as required by the selected channel source. the channel qualified for the raw data push is flagged as accepts raw pil in the channel setup as well as the channel selection screen. hence, look out for this flag to identify such channels at the time of activation. destination when data is pushed to any destination that requires raw pii data, the data is encrypted at the field level in the database and decrypted during the upload process. since the destination does not receive encrypted pil, no key is required because the destination prefers the data in raw format. access control page 4 your data in the bigquery or gcs buckets can only be accessed by users who have the role of a client admin and editor in collect. however, for the purpose of debugging or product support, the zeotap representative can raise a request to the admin of the account. only after the approval of this request, the admin gets timed access to your bigquery or gcs bucket. components of the preview data tab previous page source integrations batchrealtime next page need support? questions? problems? need more info? contact us, and we can help! raise a ticket extracted text from pdf19.pdf page 1 table of contents 1. before you begin... cccccscssccsccsscessccsscssscssecssesssecssecesecesscssecseesseessseeseeesecessceaecseceaecaseseseceesessesseceeseaeeeseenaees page 2 1. before you begin before you start using zeotap cdp, ensure that you address the following prerequisites and obtain the necessary information outlined in each requirement: 1. discovery: begin by defining your use cases and understanding your customers requirements. prioritise use cases and map out data sources, destinations, usage timelines and more. 2. data or catalogue definition: subsequently, gain an understanding of your customers data models, fields, data onboarding formats, delta management and more. 3. id strategy: during this phase, it is crucial to formulate your strategy for identification management. determine how you can effectively handle data identification and integration to achieve optimal outcomes with zeotap cdp. before starting with discovery, ensure that you get the following information as per the presales cycle: e handover checklist e usage expectation this information is crucial as it sets the base for the overall onboarding process. introduction previous page discovery next page need support? questions? problems? need more info? contact us, and we can help! raise a ticket extracted text from pdf20.pdf page 1 table of contents 1. preview data... eee eeccscescessesssssesssesssessaeessecseecseeeseecseesseeeseessesesesesesesecseeseeessseseseseeeeseseseseseaeesseesseeeseseaeees page 2 1. preview data after successfully implementing the source, you can examine the data that has been received into the system under the preview data tab. note that once the data starts flowing into the system, the status of the source changes to ntegrated. if no data is ingested, then a message stating we havent received any data yet. e please check again later and ensure that the source is implemented correctly. in case of any issues, contact supportzeotap.com message appears on the screen. integrate sources v xome back to sources: implementation details preview data catalogue mapping source report beter looks good? get started by licking on the map to catalogue button on your right. c erresh data false 20220506t09.03:46.4052 ereatodate:20220506t09:03:46.4052,e: 20230918t06:36:48.6072 false 20220517710:52:26 5332 ereatedate:20220517710:52:26 5332,e: 20230918t06:38:04.3062 false 2022051771053:43,9652 ereatedate:2022051771053:43,9652,e: 20230918706:39:57.8602 false 2022051771053:43.9652, ereatedate:20220517710:53:43,9652,e 20230918t06:34:02.2882 false 20220523713:38:53.7452 yereatedate:20220523713:38:53,7452,ei 20221128t11:04:46,9002, false 20220523713:38:53,7462 ereatedate:20220523713:38:53,7462,e: 20221128t11:04:49,9732, false 20220523713:38:53,7452 createdate:20220523713:38:53,7452,e 20221128t11:04:54.4612 false 20220523713:38:53.7452, tereatedate:20220523713:38:53,7452,e1 20221128t11:04:53,6582 false 20220523713:38:53.7452 tereatedate:20220523t13:38:53,7452,ei 20221128t11:04:47.7582 implement the source previous page map the catalogue next page need support? questions? problems? need more info? contact us, and we can help! raise a ticket extracted text from pdf21.pdf page 1 table of contents 1. setting up organisation and usep........cccscccssccsscssscsseccsscsseceeceseceescssesseecesesssesescsecesseesesesecsseseseseseseseesateeeeenes page 2 1. setting up organisation and user the following are the best practices for setting up organisations and users within zeotap cdp: data residency: all data will reside in the european union eu region unless the customer specifically asks for a different region, which needs to be notified before hand. if all the data must be unified, then ensure to onboard them onto the same organisation. in case of child organisations, data will not be unified across them. this is ideal when: o data is kept separate across countries or brands. o consent is specific to a particular brand or source. user access management: make sure to complete the tree mapping of admin and other roles in the client side. validating data before onboarding previous page setting up sources next page need support? questions? problems? need more info? contact us, and we can help! raise a ticket extracted text from pdf22.pdf page 1 table of contents 1. setting up soulces.........eeeeesccceseeessseeceseeceneecesaeeseseeceeaeeeseacecsneecsaeeeesuecseaeeeseaeeseeaeeseaeesseeseeaueceeaeeeseeeseeeees page 2 1. setting up sources the following are the best practices and recommendations that must be considered when using sources: for information on best practices to be followed while setting up different sources, refer here. verify the sample data provided by the customer for nested or complex structures. presently, this data type is not supported. if ingestion is necessary, the customer must flatten the data on their end. for server to server aka http api sources, the data formats may need to be transformed. consider the events vs profiles vs other attribute types carefully when defining custom fields based on the customers usecase and specified data type. for information on catalogue fields, refer here. for deltas, you must only share the updated data to ensure optimal use of processing allowance. setting up organisation and user previous page setting up predictive audience next page need support? questions? problems? need more info? contact us, and we can help! raise a ticket extracted text from pdf23.pdf page 1 table of contents 1. soul ces... eeeeeecseeeseeeneeeseceseeeseeseecseecseesseeeseceseecseecsesesssesaeesaseseseaeceaecesseeeeceeceeesssecseeesaceseseseecstessseeaeeesaseaeenaee page 2 1. sources overview sources in zeotap cdp is a crucial tool that helps you gather data from various places like websites, mobile apps, crm systems, email marketing, social media and more. you can create a source for each website or app you want to keep an eye on. we recommend that you create a source for each unique source of data. it allows you to understand who your customers are and how theyre using your product. note that each source you create has a write key, which is used to send data to that source. understand the source categories here, you can find information about the various source categories available in the system and the various data sources available for each category. the following are the source categories available in the system: websiteevents: website events capture specific actions or interactions on a website, providing valuable insights into user behaviour. note that if you have multiple websites, create one source per website. to learn more about website events and associated data sources, refer here. app events: app events track interactions within mobile applications, offering a deeper understanding of user engagement and behaviour. to learn more about app events and associated data sources, refer here. flat files: flat files store data collected from external sources, often in formats like csv or json. you can transfer batch data using sftp or through zeotap google cloud storage. to learn more about flat files and associated data sources, refer here. servertoserver: configure sources for servertoserver data transfer using the http api source. to learn more about the serverto server data transfer type and how to set it up, refer here. tag managers: tag managers simplify the management of tracking tags and scripts on websites and mobile apps. to learn more about tag managers and associated data sources, refer here. page 3 e crm data: crm data comprises comprehensive customer information collected through tools like salesforce crm. to learn more about crm data and associated data sources, refer here. e data warehouse: data warehouses centralise structured and unstructured data, supporting analytics. to learn more about data warehouse and associated data sources, refer here. e customer engagement channels: customer engagement channels encompass various communication touch points. explore these channels and the available data sources here. e message queue: a message refers to a piece of information that is processed by another system or it can be an actual payload, like files or metadata that trigger some processing in another subsystem. queues help in processing these messages in order . to learn more about message queue sources, refer here. understand sources with a use case before creating a source, ensure that you understand the different data source categories mentioned above and accordingly choose the category and data source as per your requirement. for example, for a website, you can choose the source type as web javascript, web pixel, web pixel or iframe. similarly, for mobile apps, you can choose the source type as android native sdk or ios native sdk. to understand better, let us take an example. assume that you want to track events on one of your websites. you can create a source for your website with the following details: 1. select source category as website. 2. select source type as web js. 3. add the details relating to your website. such as source name website a, url a.com and so on. as a result, a source named website a is created in the sources module. this source is listed in the source listing page. note that if you have two websites to track, then these two websites are considered as two web js sources. if you have three flat files based on the crm files present in each, then those three flat files are considered as three different sources. the following are examples of three such flat files: page 4 a flat file containing user login information a flat file containing user classifications flat files for tracking purchases on the two different websites key features and benefits of sources the following are some of the key features and benefits that the sources module offers: realtime data streaming: sources continuously collect and process data as it becomes available. realtime data allows you to respond promptly to customer actions and deliver personalised experiences at the moment. customisable data mapping: the sources module allows you to map data fields from different sources to create a standardised schema. this customisation ensures that the data is structured in a consistent manner across the platform, making it easier to analyse and leverage. improved data accuracy: data integration and mapping within the sources module help eliminate data discrepancies and improve data accuracy. this enables you to make informed decisions based on reliable and consistent data. efficient data management: sources streamlines the data collection process, making it more efficient and less resourceintensive. it saves you time by automating data integration and management tasks, allowing you to focus on analysing and acting on insights. data standardisation data from different sources is merged into a catalogue that is maintained at the user profile level. related topics to learn about all the options available on the source listing page, refer here. to learn about the various source types available in the system and how to create them in sources, refer here. for the best practices to be followed while creating different sources, refer here. integrate previous page components of the source listing page 4 page 5 next page need support? questions? problems? need more info? contact us, and we can help! raise a ticket extracted text from pdf24.pdf page 1 table of contents 1. validating data before onboarding..........cccccesccesecsecsscssscssscssecsseceseceseccsecseeseseceseseeecsseseceeeeseseeseaesesesesesenes page 2 1. validating data before onboarding the following are the best practices for validating data before onboarding: e all sources need to share the sample files to validate the data coming in with the data discovery sheet. e all data should be first uploaded into the sandbox environment and tested, before onboarding onto the production environment. all clients are given one sandbox environment. best practices for using zeotap cdp previous page setting up organisation and user next page need support? questions? problems? need more info? contact us, and we can help! raise a ticket extracted text from pdf25.pdf page 1 table of contents 1. website events ce.......ccccccesccsscssecssscssecssecesecesecssecseessseseeesseeseseseecseseseessesesesaeceseceesceesceeeseeeseeseseceeseeeeneees page 2 1. website events ce website events refer to specific actions or interactions that occur on a website that are tracked to gather valuable data about user behaviour. events can capture a wide range of user activities, such as clicks, form submissions, video plays, page scrolls, downloads, and more. event tracking is often utilised to measure the effectiveness of marketing campaigns, optimise website performance, improve user experience, and personalise content. the following are the two commonly used methodsfiles to implement website event tracking: e web javascript e web pixel source integrations batchrealtime previous page web javascript sdk next page need support? questions? problems? need more info? contact us, and we can help! raise a ticket scraped from: https:docs.lytics.comdocsuserprofilehealth understanding profile health quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy because lytics user profiles are created by stitching data across sources the stability of a given profile depends on the quality and quantity of data used to create it. lytics is now surfacing the health of user profiles to improve your account performance and reliability. healthy profiles are stable and actionable, with strong identifiers connecting data fragments across sources to the right user. unhealthy profiles are unstable and not actionable, as they are either missing entirely or missing a subset of data. lytics is providing visibility into user profile health to facilitate the resolution of various issues caused by unhealthy profiles, which can include missing data, inconsistent audience counts between lytics and other tools, and duplicate profiles in audience exportstriggers. unhealthy profiles a user profile is considered unhealthy if the traversal through all data fragments fails for any reason. unhealthy profiles are mostly commonly hitting one of the following limitations: unhealthy profiles in lytics are no longer processed for audience evaluations, audience exports or triggers, and enrichment processes, including behavioral scoring and content affinity calculations. because unhealthy profiles are unusable in lytics or any of your connected marketing tools, this change will reduce latencies and ensure that only valid user profiles are surfaced for activations. what information is contained for unhealthy profiles? unhealthy profiles only contain unique identifiers by fields and meta fields such as profile size, an indication of why the profile is unhealthy, the data stream names, etc. numaliases numevents numstreams numdays numconflicts conflicts nummaxneighbors maxtraversals numnestedvalues totalsz custsz internalsz elasticsearchsize streamnames profileprocessingfailure brokenprofilebrokemaxsize brokenprofilebrokemaxfragmentsize brokenprofilemaxneighbors brokenprofilenestedcount lytics is not destroying or losing any data about your users. rather, unhealthy profiles simply no longer surface the portion of a profile that can be retrieved as it may result in inaccurate audience counts and activations. how can i identify unhealthy profiles? a new, predefined audience called unhealthy profiles will be made available in all accounts. if user profiles become unhealthy, then we should see an increase in this audience at the same rate that we see a decrease in other audiences. will other audiences be affected by this change? the only fields that we drop from the profiles when they become unhealthy are the nonby fields. so an audience that only has conditions on those by fields will not be affected. for example, filter exists email wouldnt be affected, but the audience filter exists email and statewa would be affected. the latter would be affected because lytics will drop the state field for the unhealthy profiles. filter exists email filter exists email statewa how can i resolve unhealthy profiles? contact your lytics account manager to discuss a strategy to resolve or reduce the number of unhealthy profiles. the approach will vary depending on your identity resolution strategy and the requirements of your marketing use cases. can i export unhealthy profiles? yes, the unhealthy profiles audience can be exported via bigquery, sftp or s3. however, unhealthy profiles cannot be exported to other provider tools for activations since they are excluded from the audience evaluation process. profile limits user profiles become unhealthy by hitting one or more of the following limits. max neighbors max neighbors refers to the number of edges associated with a node. a node is created in lytics graph when an identifier is introduced to a user profile. audience impact: from an activation perspective, this means that what would have been one user profile in the audience is now increased to how many fragmented nodes there were. above is an example of when max neighbors could impact a profile. the profile on the left has one email and many web cookies. each time a user gets a new web cookie and they are logged into the site, lytics will update the graph with a new node. over time, the node with an email will have more and more cookies associated with it until it hits the max neighbors limit. once the profile hits max traversals, the email node will drop off. then, instead of having one user profile in the lytics audience builder, there will be as many profiles as there are fragmented nodes off the original four in this example. max traversals max traversals refers to the number of edges that need to be traversed in order to retrieve the user profile for the audience. once this hits the limit specified on the account, the profile will no longer update. audience impact: if a user hits max traversals, their profile will no longer be updated for audience building and will fall into a stale state. see the following examples: if a user has opted out for emails and they hit max traversals, they will not be opted out even if the event is received to opt them out. they will be stuck in that audience unless the criteria changes and they no longer qualify. if a user has opted out for an email and they do not qualify for the audience originally but eventually opt in, that will not be reflected on the profile in the audience builder. profile size when a profile becomes too large, it cannot be pulled in for audience building at all. at this point the profile is unusable. there are a few different points in our pipeline which can trigger a profile to be marked unhealthy due to size: updated 11 months ago scraped from: https:docs.lytics.comdocsfindingauser finding a user quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy there are a number of circumstances where you may need to search for a user. for example, you built a test user to validate a campaign is working correctly, you want to see how your marketing efforts are influencing individual users behavior, or you need to viewdelete a profile for gdpr compliance. follow these steps to search for a user profile in lytics: if you find multiple profiles for a single email or any other identifier, this may indicate data fragments havent been merged together yet or that your identity resolution strategy is not configured correctly. please contact lytics support for assistance. updated almost 2 years ago scraped from: https:docs.lytics.comdocsaudiencegroups audience groups quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy learn how audiences groups can help you organize your audiences welcome to the documentation for audience groups, a powerful new feature that allows you to organize and manage your audiences effectively. with audience groups, you can group related audiences together, making it easier to navigate and work with your audience segments within the lytics platform. this document will guide you through the setup and management of audience groups, empowering you to leverage this feature to its fullest potential. audience groups can improve how your audiences are organized within lytics. think of an audience group as a folder of audiences, where an audience can belong to multiple audience groups. this comes in handy when grouping audiences based on a campaign, based on an author, or for setting up an ab test. managing groups creating groups the audience group management feature offers a userfriendly interface for creating new groups. users can easily access the group creation functionality from within the audience management experience. the following steps outline the process for creating a new group: editing groups the audience group management feature allows users to modify and update the definition of their audience groups. this ensures that groups accurately reflect the desired characteristics and targeting criteria. heres how to manage an existing group: deleting groups the audience group management feature provides users with the ability to delete individual or multiple audience groups as needed. the following steps outline the process for deleting groups: bulk actions the bulk actions feature allows users to perform actions on multiple audiences simultaneously, enhancing the audience management experience. while the functionality initially supported only bulk deletion of audiences, it has been reworked to also include bulkadding audiences to a group. this documentation provides a comprehensive guide on utilizing the bulk actions feature. prebuilt audience groups out of the box, lytics provides a set of prebuilt audience groups, a set of predefined groups that offer convenient categorization and management of audiences within the lytics platform. these prebuilt audience groups are designed to save you time and provide quick access to commonly used audience segments. using audience groups in the audience builder when creating new audiences, audience groups can be accessible in the audience builder. under the existing audiences tab, the filter by group option allows you to display only the audiences that exist in a specific group. this can streamline your audience building process. the image below shows how one can access the audiences groups via the audience builder. updated over 1 year ago scraped from: https:docs.lytics.comdocsprebuiltaudiences prebuilt audiences quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy learn about lytics prebuilt audiences and how to use them out of the box, lytics provides a set of prebuilt behavioral audiences powered by data science and machine learning. these prebuild audiences are designed to provide a highlevel behavioral breakdown of your users. the prebuilt audiences can also be used as building blocks for your custom audiences to incorporate behavioral characteristics. the following list details each of the lytics prebuild audiences. lytics engagement audiences to learn more about lytics prebuild audiences and for more detailed information about each prebuilt audience, please refer to the documentation in the behavioral audiences section. updated over 1 year ago scraped from: https:docs.lytics.comdocsreportsintroduction what are reports? quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy the new customizable lytics reports do not replace the reporting capabilities you are familiar with in decision engine. they are simply another configurable tool you can use to understand your audiences. lytics reports provide a powerful dashboarding tool to visualize and gain critical insights from your customer data. powered by our lightingfast segmentation engine, reports allow you to create personalized views into how your audiences evolve, the distribution of user attributes within a segment, and so much more. of course, activating the uncovered insights is also always within a few clicks. to create your first report, click on the reports item from the navigation bar in the lytics app. reports to create your first report, click the create new button. you will then be directed to the creation wizard, where you can name your report and provide an optional description. once youve set up your report, you will be redirected to the report page, where you can add components to your report by clicking the add new component button below. to learn more about reports and components, check out the following links: updated almost 2 years ago scraped from: https:docs.lytics.comdocsreportscomponents components quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy reports can consist of one or many components. think of a component as a method of visualizing data. each component provides different ways of displaying and drilling into your data. supported components: creating components you can select from a list of component types by clicking the add new component button on the report page. click on the desired component to add it to your report. managing components lytics provides a handful of methods of managing your report components. size comparison size comparison components allow you to display multiple audience sizes on one chart. this component can help compare multiple audiences simultaneously, commonly used for variation testing. creating and editing size components to create a size component, select one or many audiences in the edit component menu. in the configuration menu, you can set the components name, description, and audiences based on the selected table. after selecting and saving the audiences, the component will display a timeseries chart of the audience sizes. customizing size components once a size component is created, the timerange and the stacked options can be selected. by default, the size component will chart the last seven days of data. to customize the start and end dates, click on the dates in the upperright section of the component. to display the audience sizes on separate charts, toggle the stacked option in the lower left section of the component. size components can also be visualized as a single number by selecting the number chart type. audience overlap audience overlap components allow you to understand the intersection or lack thereof between two or more lytics audiences. creating and editing overlap components to create an overlap component, select one or many audiences in the edit component menu. in the configuration menu, you can set the components name, description, and audiences. after selecting and saving the audiences, the component will display a venn diagram representing each audience and their collective overlap. activating insights these insights can be quickly activated by clicking directly on any intersections. this will take you to a prepopulated audience definition with rules defined representing the portion of the diagram you clicked on. for instance, in the above example, clicking on.multi session visitors within the has visited web audience will produce the following audience definition. multi session visitors has visited web data flow data flow components allow you to see how data flows through your connected providers, data streams, audiences, and destinations. creating and editing data flow components to create a data flow component, select up to 5 audiences in the edit component menu. in the configuration menu, you can set the components name, description, and audiences. after selecting and saving the audiences, the component will display a bipartite diagram containing each of the selected audiences and the connected providers, streams, and destinations. the edit component menu also provides options to hide or display the providers, streams, or destination columns. this allows the user to zoom in on different aspects of the data flow diagram. activating insights insights for this component can be accessed by hovering over or clicking directly on any line in the diagram. hovering over a line will prompt the dataflow tooltip, which contains information about the job such as when it was created and the number of users exported. clicking directly on the line will take you to the jobs page for the corresponding job. for instance, clicking on the yellow line between all and google cloud will redirect you to this page. composition composition components allow you to visualize a field in the user or content schema across one or many audiences. this can be useful to compare a field across audiences or to analyze the data distribution for a group of users. user content creating composition components to create a composition component, click on the edit component button. in the configuration menu, the name, description optional, audiences, field and subfield can be selected. depending on the field type, lytics provides a variety of ways to visualize the data. the data can be viewed as a bar chart, line chart, pie chart, table, or stats view for numeric fields. the following table shows all of the chart types for each field type. chart types managing composition components: once youve created a composition component, the bottom navigation bar allows you to customize your component. here, you can select your desired chart type, toggle between stacked and unstacked charts, download data, and customize your component. example: charting utm data as a first example, well walk you through creating your first composition component using device data. example: using subfields as a more complex example, well chart the distribution of users affinity for a content topic across two different audiences. updated 7 months ago scraped from: https:docs.lytics.comdocsreportsmanaging managing reports quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy reports come fully equipped to manage your reports across your team. editing and deleting reports users can quickly edit and delete reports at the top of the report page. the edit option allows you to rename the report name and description. sharing your reports creating audience reports in lytics can be a collaborative process. by default, reports within lytics are shared across all users with the.reporting role. to prohibit users from accessing reports globally the.reporting role can be disabled for specific users. reporting reporting in addition to the global role, a report creator can further restrict access to their reports by setting the report asprivate and adding the specific list of users who have access. the screenshot below outlines the simple setup process to finetune collaboration and access to individual reports. private change logs lytics records a log of all changes made to a report in the logs tab. this record lets users keep track of who updated a reports components. downloading reports each component in lytics provides the option to download the associated data in a csv format. the download icon is located in the bottom right of each component: updated almost 2 years ago scraped from: https:docs.lytics.comdocsdashboardreport dashboard report quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy how to interpret and configure your dashboard report once data is flowing into your account, lytics will automatically generate a dashboard report on the homepage of the decision engine app. while the dashboard report is fully customizable, the outofthebox report consists of 4 report components that incorporate some of lytics behavioral audiences : to modify or edit the dashboard report, simply click on the edit dashboard button located on the topright corner of the page. clicking this button will redirect you to the report page, where you can add or delete components, and edit existing components. updated over 1 year ago scraped from: https:docs.lytics.comdocstopics topics quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy to view topics in your account, click on the standard interest engine and select the topics tab. the table above displays the top 500 topics associated with the standard interest engine. each row indicates the number of documents tied to that topic, as well as the number of users who have a topicscore for it. for the standard interest engine, topics are generated using lytics natural language processing nlp tools, as well as metatags from webpages. to learn more about how topics are enriched, visit our enrichmentdocumentation. blocking topics in some cases, lytics nlp tools may identify irrelevant topics. to address this, you can block unwanted topics by selecting the checkbox next to each one. once blocked, the topic will appear in the blocked topics table and will no longer show up on user profiles or in documents. when you click on a topic in the topics table, youll be taken to the topic summary page, which provides detailed insights about the selected topic. new audience content map and related topics the topic summary page also displays related topics, which are identified based on their frequent cooccurrence with the selected topic. these related topics appear in the bottomright corner of the page and are also visible in the content map view. the topic taxonomy or content map is built by analyzing documents and the topics they contain, revealing patterns of cooccurrence and topic prevalence. you can use the slider at the bottom of the content map to highlight strong connectionstopics that frequently appear together. in the example below, we can see that the furniture topic is highlighted, as well as each of its related topics such as house, and faux fur bean bag. segmentation with topics when you click the new audience button, youll be redirected to the audience builder. the topic score distribution chart can help you decide on an appropriate threshold when creating segments. new audience to include all users who have shown any interest in a topic, use the exists operator. exists updated 5 months ago scraped from: https:docs.lytics.comdocsaffinities1 affinities quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy to view your affinities, select interest engines under content in the lytics navigation bar. at the bottom of the page, youll find a table displaying the affinities in your account. you can filter the list by interest engine or search for a specific affinity. this table presents the name, description, topics, last modified date, and interest engine associated with each affinity. to view the affinities tied to a particular interest engine, simply navigate to the interest engine page. affinities allow you to group related topics into a targetable unit, making it easier to reach your customers based on their interest groups. this document will walk you through how to use the affinity builder stepbystep. once an affinity is created, you can use it to build audiences and content collections. to create an affinity, navigate to the interest engines page under content, select the standard tile, and click on create new. create new the search option gives you access to any topic in your taxonomy, even if its not included in your list of common topics. this means you dont need to allow or block certain topics to make the ones you care about available in the ui. as you select topics, they will be added to the assigned topics list on the right. in this example, we created an affinity named concert affinity that included topics such as concerts, live music and performances. there are two ways you can remove selected topics from your affinity. once an affinity has been created, you can add or remove topics at a later time, and edit the name or description. each affinity has a summary page that provides an overview of how many users are interested in this group of topics and how much of your brands content or product inventory is represented. after creating an affinity, or when clicking any row in the affinity list, youll be directed to this summary page. at the top of the affinity summary page, youll find the following details: chart after creating an affinity, it may take a few days for the chart to fully populate as lytics evaluates users interest in the selected topics. to the left of the chart, youll find key metrics:: on the chart, the x axis shows affinity scores on a scale from 0100, where 0 represents no interest and 100 represents the highest interest. the y axis displays the number of users. you can click on individual bars to see how many users fall within specific affinity ranges. this data helps your marketing team identify which users to target based on their interest levels. for example, you can focus on users with mediumtohigh affinity for certain topics in an ad campaign, increasing the likelihood of relevance and conversions. details this section outlines how your affinity is configured. default sample documents at the bottom of the summary page, you will see a paginated list of sample documents for this affinity. your content must be classified before it will appear in this list. updated 5 months ago scraped from: https:docs.lytics.comdocsusingtopicsaffinities using topics affinities quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy the affinity engine enriches user profiles based on behavior, but its true value lies in how you act on that data. in lytics, affinity engine insights can be applied in three key ways: segmentation, recommendation, and lookalike models. segmentation lytics integrates affinity engine scores directly into user profiles, allowing you to use these scores in the audience builder to segment users based on their affinities. this unlocks a variety of strategic use cases. for example, if youre a shoe retailer launching a new line of boots, you can create an audience of users with an interest in boots, ensuring that your ad spend targets the right users instead of those with little to no interest. beyond targeting, affinities can help you track trends over time. you may notice that interest in boots spikes during the winteror perhaps it begins to rise in the fall, as customers start exploring options before the colder months. affinities provide valuable insights as user interests shift over time, making them a powerful tool for trend analysis. recommendation and personalization lytics recommendation system blends ai with marketing strategy to help you easily suggest relevant content and products to your users. by setting up personalized recommendations, you can deliver more engaging user experiences, which in turn boosts time onsite and overall engagement. lytics makes it easy to get up and running with content recommendations. upon configuration, our recommendation system: leverages over 500 behavioral signals to deliver highly relevant recommendations. operates autonomously, with models that retrain and optimize on a weekly basis to keep recommendations fresh. works effectively right outofthebox for new users, requiring little to no prior data to provide valuable content suggestions. scales rapidly to meet the demands of large volumes and high user traffic. to get started, navigate to the personalize page under using profiles, or consult the content recommendation api documentation personalize using profiles lookalike models affinity engine scores are also highly valuable when building lookalike models. these scores offer a standardized way to compare users, creating data consistency across profiles, which is crucial for machine learning models. this consistency allows affinity scores to often emerge as key factors in lookalike models. if your team is working on predictive audiences, leveraging affinities can significantly enhance the accuracy and effectiveness of your models. updated 5 months ago scraped from: https:docs.lytics.comdocsclassification classification quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy the classification page offers insight into how your content is being scraped, indexed, and categorized by the lytics content affinity engine. you can access this page under the content menu, and it includes two main sections: the classification dashboard and content classification. classification dashboard the classification dashboard displays the classification activity, content flow, and the content dashboard components. learn more about each component below. classification activity the classification activity section shows how many documents the lytics content engine is classifying. by default, lytics will classify up to 20,000 documents per month, which includes new documents as well as periodic reclassification of existing ones. this section helps you track whether youve reached your monthly quota and ensure the content engine is functioning as expected. the lytics content engine runs multiple workflows in the background, including the content classification workflow, which updates hourly. as long as you havent exceeded your monthly quota, you can expect hourly updates to the classification activity. if your account has hit the content classification quota in previous weeks, you may notice a lack of data in the chart for the current week. adjust the date range to see when content was last classified. content flow visualization the content flow chart visualizes how your documents are ingested, processed, and enriched with topics by the lytics content engine. it also highlights any errors or problematic urls such as urls that are blocked by robots.txt directives, account settings, or urls that encounter non200 status codes. robots.txt each state in the flow diagram is described in detail below: content dashboard the content dashboard displays various attributes of your content, including site name, author, url path, and topic information. the classification dashboard is intended to provide a highlevel understanding of your content and can be used as a starting point for your contentoriented use cases. manual content classification the manual content classification module allows you to manually add a url to lytics or preview how a specific document will be classified. this feature is useful for troubleshooting any setup issues on your page before its added to the lytics content corpus. to use it, simply enter the url of the document you want to preview and click classify. youll be able to see the extracted topics as well as any metadata that lytics scrapes from the document. adding and removing topics adding and removing topics when previewing a document, you can manually add or remove topics from the classification. to add a topic, click the add topic button, select the desired relevance score between 0 and 1 and then click add topic to document. add topic add topic to document to remove a topic, simply click the x next to the topic tile. once youre satisfied with the results of the classification, click complete classification to add the documentto the content corpus. the document and its associated topics will then be available for use in personalization efforts, such as recommendations or content affinity. url normalization as lytics ingests webbased content, it attempts to resolve duplicate urls and create links between documents, much like a search engine would. as such, lytics does things like respect robots.txt directives, resolve canonical urls when present, etc. robots.txt lytics attempts to sanitize urls as much as possible before ingesting them into the content affinity engine. sanitization includes removing all url parameters and cleaning url syntax. this happens via an lql function called urlmain. urlmain updated 5 months ago scraped from: https:docs.lytics.comdocsenrichment enrichment quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy to better understand how users are engaging with content, lytics first needs to understand that content itself. one way lytics does this is by analyzing the urls that are passed to lytics to determine the topics that best describe the url. when lytics receives data about actions taken by a customer, it is called an event. each event has fields that store pieces of information describing the event, including the url. by associating topics with urls, lytics is also able to understand which topics a user has engaged with. in doing so, the lytics content affinity engine can find relevant content for users, as well as find relevant users for content. when lytics receives an event with a url in it specifically when an event with a field named url comes in on any data stream lytics determines whether the url is new or not. a new url is one that lytics has not previously handled. url lytics then creates a new event and writes that event to the data stream lyticscontentenrich, called the content enrichment stream. an lql query named lyticscontent handles events written to the content enrichment stream. this results in a new entity being created in the content table. lyticscontentenrich lyticscontent lytics listens for events with new urls on the content enrichment stream. when a new event is available, lytics runs the url enrichment process. data enrichment is a common practice in lytics. it refers to the ability to add data onto inbound data to improve its quality. this process is also used in user profile enrichment. enrichers enrichment is handled by components called enrichers. each enricher performs a specific task. a common task for an enricher is to associate topics with a url, but there are other tasks that enrichers can perform. whatever its specific purpose, the result of an enricher running is that additional data may be added to the inbound data event. after the enrichers run, another new event is written to the content enrichment process. this time, the new event is not enriched because the url is not new. but the event includes all of the data that was previously added during the enrichment process, so when the query lyticscontent runs, it is able to map that new data to the corresponding entity in the content table. lyticscontent the specific enrichers that lytics uses depends on how your account is configured. the account setting enrichcontentsources controls which enrichers are used. your lytics representative can help you change the enrichers that are enabled on your account. enrichcontentsources meta enricher the meta enricher is always used by lytics for content enrichment. the metaenrichment process begins with lytics sending a request for the url. the response allows lytics to collect some information to improve the efficiency of the overall enrichment process. examples of information collected are: natural language processing the following natural language processing nlp services are available in lytics for content enrichment. each link takes you to the language support page for that service, if applicable. the setting column denotes the account setting change needed to enable the service, which must be enabled by lytics support. googlenlp googlenlpentity google diffbot diffbotmeta textrazor topic extraction since lytics collects and stores every event without any aggregation, automatic topic extraction becomes a possibility. for every url seen, lytics uses a bot called lyticsbot to fetch the web page at that url. the content, metadata and images of the url is analyzed and boiled down to a set of topics. lyticsbot lytics content authorization if some of your content is premium and requires a login to access, then youll need to create a new authorization so lytics can access this content. to do this: basic authentication cookies lyticsbot directive configurations when lyticsbot scrapes your content, you can identify it with some http headers that will be present on every request, namely: lyticsbot useragent lyticsbot lyticsid this will allow you to identify requests from lytics to scrape that content to enhance your topic graph. for some websites it is desirable to allow lyticsbot to crawl everything as fast as possible. however, some web administrators would like more flexibility and control over how fast and where the bot attempts to pull content from. the bot will follow a set of directives that would be located at the root of the website, for instance https:www.lytics.comrobots.txt. lyticsbot https:www.lytics.comrobots.txt below you can see three common robots.txt configurations. robots.txt lyticsbot admin useragent: lyticsbot disallow: admin useragent: lyticsbot crawldelay: 10 useragent: lyticsbot disallow: admin disallow: private crawldelay: 10 you must specify the lyticsbot user agent. a wild card will not work in this case. lyticsbot providing custom topics lytics will automatically extract topics from the main content at a url, but sometimes domain specific topics are also desired to track. in this case, lytics supports a special meta tag for annotating custom topics. provide a commaseparated list of topics in a lytics:topics meta element in your html source. lytics:topics here is an example from a lytics blog post: omeda and lytics team up to offer allinone audience engagement platform additionally, your lytics account can be configured to also scrape other meta tags to feed into your topic graph by setting the accounts contentcustomprops setting to the names of the meta tags youd also like to include. contentcustomprops for example, if you wanted your lytics topic graph to include topics from your article:tag meta tags, you could update your account settings with the following api request. article:tag curl xput https:api.lytics.ioapiaccountaccountid h contenttype: applicationjson h authorization: liokey d settings : contentcustomprops: article:tag now, after adding the article:tag topic, any values from article:tag meta tags will also appear in the topic graph which means theyll be eligible for content affinities, targeting and personalization, and inform content recommendations. article:tag article:tag lytics will track these custom topics in addition to the automatically extracted topics. do not specify generic topics, there is no need. viewing topics assigned to a document each document is assigned a url as a unique identifier. you can use the lytics content api to retrieve a document and view the topics assigned to it. get the information about the url for the lytics website home page curl s xget https:api.lytics.ioapicontentdoc?urlswww.lytics.com h authorization: liokey this will return a json object of the requested document: data: total: 1, urls: url: www.lytics.com, https: false, title: , description: , topics: cdp, customer data, topicrelevances: cdp: 1, customer data: 1 , primaryimage: , author: , created: 20181024t23:10:06z, id: 7169839995045099096, stream: , updated: 20181024t23:14:09z, fetched: 20181024t23:14:09z , message: success, status: 200 you can see the topics assigned to the requested content and the relevancy range of those topics from 0 to 1. topics: cdp, customer data, topicrelevances: cdp: 1, customer data: 1 , manually assigning topics in most cases topic extraction automatically assigns the expected topic to your content. if, however, you find that to not be the case or you would like to expand the topics assigned to content, lytics allows you to manually assign topics to your content. content is stored in an entity called a document. each document is a collection of fields each storing a specific piece of information about that content. each document may have multiple fields that are used to store the topics for that particular entity. the process of manually assigning topics involves updating one of those fields. assigning topics manually manually assigning topics can be done in several ways: the data in either csv or json format can be sent to lytics using any of the methods available for importing data, including csv file or json file integrations and the lytics bulk upload api. just be sure that you send the data to the correct data stream: lyticscontentenrich. lyticscontentenrich the uploaded data must be formatted in the following ways: url,topicportland,topicoregon https:www.lytics.com,1,.96 url: https:www.lytics.com, topicportland: 1, topicoregon: .96 when data is sent to the data stream lyticscontentenrich, the lql function urlmain is applied to the value. you can see this in the query lyticscontent. the result is that : and everything before it is removed. this is important to understand because if you ever need to find a url, you should exclude : and the protocol before it: lyticscontentenrich urlmain lyticscontent : : the content corpus endpoint can be used to associate topics with a url. the corpus api does not allow you to specify the relevance. topics will be assigned a relevance 1. the following command demonstrates how to use this api to set topics on content: curl s xpost https:api.lytics.ioapicontentcorpus h authorization: liokey h contenttype: applicationjson d url:www.lytics.com, topics:portland, oregon the topic curation endpoint can be used to add topics to content. however, this approach is a bit more complicated because you must know an identifier for the content you want to add new topics to. by default, the following fields are identifiers on content table: content contentid fbid hashedurl how to generate a hash for a url there are many hash functions available, but lytics uses a specific one when it hashes urls: sip hash. the following command demonstrates how to use the lytics query test evaluation endpoint to generate a sip hash for a url. in this example, the value that is used is https:www.lytics.com https:www.lytics.com curl s xpost https:api.lytics.ioapiquerytest?valuewhat20you20want20to20hash h authorization: liokey h contenttype: testtextplain d select hash.sipvalue as hashed from test into test by hashed alias test the result of this command will be something like the following. the value of the field hashed is the hashed value hashed data: created: 20181105t22:00:15.688307117z, modified: 20181105t22:00:15.688307117z, hashed: 7394646926640356587 , message: success, status: 200 if you are using the visual studio code extension for lytics, there is a command that you can use to generate a sip hash without having to write any api calls. above you determined the sip hash for https:www.lytics.com is 7394646926640356587. the following command will associate the topic cdp with a relevance of 1.0 with this hashed url: https:www.lytics.com 7394646926640356587 cdp 1.0 curl s xpost https:api.lytics.ioapicontentdochashedurl7394646926640356587topiccdp?relevance1 h authorization: liokey h contenttype: applicationjson removing topics manually when a topic is associated with a document, a new field is created on the entity. the field stores a value from zero no relevance to one highest relevance. in lytics, you cannot delete fields from documents. so, technically, there is no way to remove a topic from being associated with a content entity. instead, what you do is set the relevance to zero. since zero indicates no relevance, it effectively removes the topic from the document. removing a topic is not the same as blocking a topic. blocking a topic acknowledges that a topic may be relevant but is too generic to be useful. for example, at lytics we block the topic data because that topic is relevant on almost all of our content, and for that reason it is not useful at all. topics can be removed from content using one of the following approaches. as described above, csv or json data can be sent to lytics. the following examples demonstrate how to remove a topic from content by setting the relevance for the topic to zero: url,topicportland https:www.lytics.com,0 url: https:www.lytics.com, topicportland: 0 the topic remove endpoint allows you to remove a topic associated with content. this api sets the relevance for the topics to zero. it does not actually delete any the topic from the content. above you determined the sip hash for https:www.lytics.com is 7394646926640356587. the following command will remove the topic cdp from this hashed url: https:www.lytics.com 7394646926640356587 cdp curl s xdelete https:api.lytics.ioapicontentdochashedurl7394646926640356587topiccdp h authorization: liokey h contenttype: applicationjson updated 10 months ago scraped from: https:docs.lytics.comdocsrecommendations recommendations quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy content recommendations are a powerful tool that can be used to increase user engagement by presenting users with relevant content that resonates with them. by utilizing the lytics topic taxonomy and powerful machinelearning algorithms, lytics can make datadriven content recommendations within milliseconds. powered by over 500 features, and the flexibility to customize your recommendations, lytics content recommendation api removes the heavy lifting when it comes to content personalization via web, email, etc. out of the box, lytics content recommendations can be leveraged in 3 ways: content recommendations in the lytics ui the lytics ui offers multiple ways to test, validate, and get started with content recommendations. both the interest engine page and the content collection page provide options for fetching recommendations for individual users. recommendations via the interest engine page to access recommendations from the interest engine page, navigate to the interest engine page under the content navigation menu, select an interest engine, and click on the recommend tab. after entering a user id and clicking recommend, lytics will generate recommendations tailored to that specific user. recommend recommendations via the collections page to access collectionspecific recommendations, navigate to a collection under the collections page in the content menu and select the recommendations tab. this page provides options for creating recommendations experiences, links to the content recommendation api, code snippets, and a search bar to fetch collectionspecific recommendations for a user. recommend content with lytics personalization you can create personalized, customizable recommendation experiences on any site where the lytics javascript tag is installed. recommend content with pathfora js pathfora js is capable of taking a content collection and fetching and displaying the recommended content for the current user. read more about it in the pathfora js docs. recommend content with lytics apis the lytics content recommendation api takes a handful of parameters, including the user the recommendation is for and an optional content collection to recommend from. using content collections in recommendations content collections are commonly used in content recommendations. by selecting a content collection as the source of your content recommendation, youre limiting which content the recommendation engine can select from. this is desirable for most use cases, because by default lytics will scrape any and all pages of your website that users are visiting. without a content collection defined, the recommendation engine will return what is most relevant to the user regardless of the context. for example, in a marketing context you wouldnt want to promote your careers page, even if it is the most relevant piece of content to the user in question. size of the collection is also an important factor when using a collection for recommendation. the more content the recommendations engine has access to, the better, more personalized recommendations it can serve. if you can, try to make sure your content collection is of a decent size, and at the minimum fits the following requirements: if none of the documents in your collection fit these conditions, the recommendation engine may have issues returning recommendations as it doesnt have enough information to make a recommendation. if you would like assistance setting up these filters in your collection or to set up a collection with content published less than 1 week in the past, contact lytics support. content collections in lytics experiences when setting up a recommended content experience through the lytics ui, you will be asked to select an existing content collection or build a new collection to use in the experience. using collections in custom content recommendations if youre using the lytics api to recommend content, you can use the contentsegment query parameter to define the content collection you want to use. the value of this parameter should be the id of the content collection. contentsegment you can locate the id of the collection from the url on the summary page. the id will come after collections in the url, and will end before the beginning of the query parameters. collections affinity alignment score when building a recommendationbased experience, you have access to the affinity alignment score automatically calculated by lytics. the alignment score measures the similarity between the selected content collection and the audience. this similarity is based upon the vector similarity of the most prevalent topics in the collection, and the most prevalent topics in the audience. if the collection and the audience share many of the same topics, the alignment score will be high. conversely, if the collection and the audience share few or no topics, then the alignment score will be low. for successful content recommendations, we recommend pairing collections and audiences with moderate to high alignment. if there is low alignment, content recommendations may have low conversion rates and may be irrelevant to users in the selected audience. updated 5 months ago scraped from: https:docs.lytics.comdocsdocuments documents quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy lytics considers every piece of content as a document, and calls the collection of every document a corpus. lytics automatically processes every web page for an accounts site, where each web page is a document. in the lytics app, documents are surfaced in association with topics and in content recommendations. keep in mind that the lytics content affinity engine isnt limited to your website content. all sorts of content can be sent to lytics, including product catalogs to power product recommendations. to search for a document in lytics, go to the search page under the content menu. enter a url or search term, then click search to find the relevant documents. clicking on a row will lead you to the document summary page, which contains more details about the document. details tab on an individual document page you are able to see all the fields associated to that particular document within the details tab. collections tab to see if this document is used in any content collections, you can view the collections tab. if you make adjustments to any of your documents, such as updating a blog post or refreshing a product landing page, you can request lytics to manually reclassify the document. this will ensure the lytics content corpus has the most uptodate information to serve in any of your content or product recommendations. by default, lytics observes new urls in all data streams to identify content with which the user is registering activity. new urls are enriched as theyre observed in incoming data that is, lytics can crawl a domain, but proactively indexing a domain and looking for new content is not part of the content affinity engines workflow. to add new documents to your corpus: you can send new documents directly to our content corpus api. the corpus api respects three parameters: url text text topics you can also use the manual classification module to preview the enrichment of a url and then add it to the content corpus. adding custom properties to a document allows you to create advanced content collections based on those custom properties. imagine creating recommendations from a pool of custom categories, promoting pages with custom seasonality components for particular holidays, adding a custom sku hierarchy to better reflect your product catalog within lytics, etc. the lytics crawler can detect custom document properties from custom meta tags any meta tag with a name property prefixed with lytics: will be ingested and appended onto a document. for example, a meta tag with the name lytics:sku will update the sku field on that document. lytics: lytics:sku sku when lytics scrapes new documents, it will append those properties on the newly generated content entity within lytics. when adding a field that isnt currently represented within the content schema, the content query will need to be updated with the new property. updated 5 months ago scraped from: https:docs.lytics.comdocscreatingcontentcollections creating content collections quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy you can build a content collection in lytics by navigating to content collections from the dashboard. then click the new collection button above the list of current collections in your account. collections can either be dynamic or locked. in the collection builder you can toggle this between these two options. dynamic content collections dynamic collections are rulebased, so individual pieces of content may enter or exit the collection over time. dynamic collections are built by selecting and setting content filters which determine what content is present in the collection at any given time. preview content currently in the collection by hovering over a content card and clicking preview this content. for example, you can create a collection that includes all articles published in the last 7 days. this setting is ideal for making sure your content collection remains fresh. locked content collections a locked content collection is a static collection of documents that you handselect using the collection builder. content filters can be used as a searching mechanism to help find the specific documents you would like to add to your custom collection. you can add content items to the collection by hovering over a content card and clicking select this content. selected content items are indicted by cards with a blue border. once you save the collection, documents are not added or removed until you edit and save the collection again. there are a number of filters available when navigating the content collection builder. these filters can be combined in such a way that narrow down the documents in the collection using and logic. filter content by title, description, or url this filter acts as a catchall text search for the title, description, and url of the document. it can be a full or partial match to any of these fields, and the input is case sensitive. you can enter multiple values for this filter type, and the results are not mutually exclusive. that is, if you have entered the search terms dog and cat, each of the documents returned should contain dog or cat, but not necessarily both or logic. perhaps the most common use of this field is to filter by a url path. for example, say youre looking to build a dynamic collection of all blog posts from your website, and the url of your blog posts match the following pattern: http:yourdomain.comblognameofyourpost. to build this collection you can enter blog into this filter. http:yourdomain.comblognameofyourpost blog content type the type of a document is derived by the lytics content affinity engine while analyzing and indexing the document. a document maybe be classified as any of the following: these checkbox filters allow you to view content of only the selected types. for example, if you want to build a dynamic collection of videos to recommend to users, you might select the video content type. published date this filter allows you to limit documents based on their date of publication. you may filter documents published after a date relative to the current time or after a static date in time. relative dates may be set by increments of days, weeks, months, or years. using a relative date filter allows you ensure that the content in your collection is evergreen. for example you may want to only recommend content that was published in the last week: if you launched a new product recently, you may want to build a collection of content relevant to that launch. using the static date filter set to the day before the launch, combined with other filters could help achieve this. features the features filter allows you to select whether or not the content should have a description andor a thumbnail image. both the image and description fields are extracted from html meta tags during the scraping process of the content affinity engine. enabling these settings may be especially useful if you are looking to implement content recommendations into a space on your website and you wish to include more than just a title and link to the content. author the author filter allows you to include only articles written by the authors selected. if you have a long list of authors, you can use the search bar to find the authors you are looking for. affinities the affinities filter lists all the curated affinities in your account. since affinities are a group of related topics, this option can save you time when building collections for a campaign targeting users interested in several related subjects or products. topics the topics filter lists all the topics present in your account. you can select one or multiple to filter the content by, and use the search bar to find specific topics. building a collection filtered by topic can be especially helpful when running a topic specific campaign. the filters above cover many common use cases. however, in some cases a more advanced set of filtering criteria is required. this is achievable via the advanced editor which is accessible from the triple dot menu on a collection summary or from the collection editor view. advanced editor from the advanced editor you can leverage the inclusion of other content collections or a custom rule just as you would with custom rules in the core audience builder. these custom rules surface the full set of fields associated with your content during the classification process. word count, language in addition to all filters referenced above are accessible through this advanced editor. advanced editor filters currently the advanced editor is only accessible for existing collections. if you are looking to create a net new collection, simply create a new collection with a basic filter and save. from there you can access the advanced editor. updated 5 months ago scraped from: https:docs.lytics.comdocsviewingcontentcollections viewing, using managing collections quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy collections list on the collections page, located in the content menu, youll find a list of the collections available in your account. by default, lytics automatically generates three key collections: documents with images primaryimage og:image all documents: default recommendation collection collection summary page when you click on a collection from the list, youll be taken to the collection summary page, which includes the following details: documents tab the documents tab displays a subset of up to 100 documents from the content collections. each document card includes the following details: primaryimage title description additionally, each card provides links to the documents url and its identityview. recommendations tab the recommendations tab offers a variety of tools and resources to help you create, test, and implement content recommendations within lytics. this section is designed to streamline the process of using lytics powerful recommendation engine, enabling you to generate personalized content experiences for your users. recommendation playground the recommendations tab also features a playground where you can test and preview recommendations for individual users. by entering a users unique identifier such as their user id, you can see realtime results of what lytics would recommend for that specific user. this feature is particularly useful for validation and testing purposes. after clicking the recommend button, lytics will return a list of personalized content suggestions for the user. each recommendation will be displayed in a table, showing: this playground offers a handson way to understand how lytics recommendations align with user interests, providing a deeper level of insight and confidence before rolling out personalized content to your audience. managing content collections when working with content collections in lytics, you have several management options available via the ... menu located next to the collection title at the top of the page. these options allow you to efficiently duplicate, edit, delete, or refresh reenrich your collections, ensuring that they remain relevant and aligned with your evolving content strategy. warning: be cautious when deleting collections that may be tied to essential processes. always ensure that theyre no longer in active use to avoid unintended disruptions in personalization or recommendations. warning: be cautious when deleting collections that may be tied to essential processes. always ensure that theyre no longer in active use to avoid unintended disruptions in personalization or recommendations. updated 5 months ago scraped from: https:docs.lytics.comdocsdefaultinterestengine default interest engine quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy the default interest engine works by analyzing webdata collected via the lytics javascript tag. outofthe box, lytics ingests and keeps track of all new urls that we receive. for each url, we use a variety of nlp natural language processing tools to extract topics. for more information about the topic enrichment process, please refer to the enrichment documentation. the header displays information about how the default interest engine is configured. though these values are uneditable, additional customization can be achieved with custom interest engines. for the default engine, webpages are saved in the content table with a unique identifier known as a hashedurlsimply a hash function applied to a parsed url. for items in the content table, topics or features are saved in the global field. on the user side, we keep track of the urls a user has browsed in the hashedurls field, and we output topics that a user has interacted with to the lyticscontent field. content hashedurl content global hashedurls lyticscontent the ui for default interest engine shows the summary, topics, and affinities. in the summary section, we display a topic taxonomy a forcedirected graph that maps how topics interact with each other. the table next to the taxonomy displays statistics about your users, and topics. expanding the rows in the table will display sample items and users and the associated topics. topics affinities topics affinities can be viewed under their respective tabs. the topics view shows all of the topics associated with the default interest engine. clicking on a topic will present more information a specific topic. similarly, the affinities tab displays all the affinities associated with the default interest engine. using the default interest engine since the default interest engine is designed to work quickly outofthebox for new accounts, you can quickly take advantage of all of the offerings provided. lyticscontent lyticsrollup lyticscontent updated about 1 year ago scraped from: https:docs.lytics.comdocscustominterestengines custom interest engines quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy if you have your own set of attributes, features, or topics for your inventory, then you may consider creating a custom interest engine. these engines allow you to configure any field on the content table to be a topic that gets outputted onto user profiles. consider the following examples of custom interest engines content user shopifyproducttags wool, cotton, polyester, etc shopifyproducttags shopifyaffinitiestag creating a new custom interest engine to create a new custom interest engine, click on the new interest engine button on the interest engines page. once the modal opens up, clicking on the affinity customization tile will open a wizard. the first step requires selecting the unique identifier and features or topics from your inventory ie the content table. in the screenshot below, we are using the shopify example from earlier, and using the shopifyproductid and shopifyproducttags fields. content shopifyproductid shopifyproducttags once the inventory and features have been identified, the next step requires mapping the data to your user profiles. the inventory field refers to the field on the user table that contains a set of shopifyproductids ie a set of unique identifiers from the content table. in this case, the shopifyproductids field is an array of ids that a user has purchased. next, configure the name of the output field this field will contain the shopifyproducttags that a user has expressed interest in based on their shopifyproductids. user shopifyproductids content shopifyproductids shopifyproducttags shopifyproductids updated about 1 year ago scraped from: https:docs.lytics.comdocscollaborativefilters collaborative filters quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy to successfully utilize lytics default and custom affinity engines, it is necessary to have a robust set of topics or features. however, in some cases, it becomes difficult to add topics to your items, which may prevent you from executing interestbased use cases. to address some of the limitations of contentbased filtering, collaborative filtering uses similarities between users and items simultaneously to provide recommendations. this allows for serendipitous recommendations; that is, collaborative filtering models can recommend an item to user a based on the interests of a similar user b. for example, consider the screenshot above, which shows a collaborative filter engine based on shopify purchase data. by creating a collaborative filter engine based on user purchase data, we can drive users also boughttype recommendations. in this case: the bar chart shows the most popular shopifyproductid ids across your users. how to use collaborative filter engines the primary way to leverage collaborative filter engines is through recommendations. while lytics recommendation api, and experience toolkit allow you to deploy recommendation campaigns onsite or via email, the collaborative filter ui allows you to experiment and test out lytics recommendations. in the recommend tab, you can fetch recommendations for any user. simply select the identifier field uids in this case and click on the recommend button. this will request recommendations for the selected user using the lytics recommendation api. the recommended items are displayed below. uids the item recommendation input allows you to fetch similar items for any given item. in the example below, we are finding items similar to the item with shopifyproductid 631195017394. this can be used to find similar products based on user behavior. shopifyproductid 631195017394 creating a new collaborative filter engine to create a new collaborative filter engine, click on the new interest engine button and click on collaborative filter. this will open a wizard with the following options. when creating a collaborative filter engine, there are 4 fields: content user once you create your new collaborative filter wizard, lytics will train a collaborative filter model within minutes. once the model has finished training, the ui will be available to use, as well as lytics recommendation api. updated about 1 year ago scraped from: https:docs.lytics.comdocsgettingstarted1 getting started quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy to demonstrate how lookalike models can bring your marketing teams segmentation strategy to the next level, weve outlined how you could segment users by hand compared to how a lookalike model would accomplish this task. for our example use case, the goal is to identify users who are likely to buy a travel package for the summer. how a marketer may create segmentation rules by hand: targeting users that match these three rules will certainly improve the efficacy of a campaign, but why stop at these? are there more factors that can be used to refine this group of users? how lytics lookalike models would do it instead: the key difference here is that a marketer may use a handful of criteria using logical heuristics to define a segment of users while lookalike models will look at hundreds of factors including potential nonobvious, impactful criteria to define an audience of users for the same purpose. in traditional supervised machine learning, we collect samples of data with one field represented as the target. this is what we look to model or classify. some examples of realworld classifiers are predicting whether an email is spam or not, predicting if the weather tomorrow will be cloudy or sunny, classifying a news article as happy or sad, or classifying an image as a dog or not a dog. we do this same thing in lytics lookalike modeling but you get to choose what the target we want to predict is. this works by providing two audiences: a target and source. target the target audience is the group of users you want to model and predict, i.e. the users you want to find more of. most of the time this audience will represent users that have done something favorable that is worth repeating by finding more users highly likely to repeat this favorable event. for example, users who provided their email on a newsletter signup, high ltv users, users who purchased a product, etc. getting the target audience in order is step 1 before creating the model. this audience defines the basis of the model or the models objective. aside from positive events, you can also model users you might want to suppress like users who have churned. if the data is in lytics, you can model it. source the source audience is the group of users to find lookalikes for, i.e. the users you want to target in campaigns after creating the model. for example, if we chose users with email as the target, then targeting unknown users would make sense as the source. you could also make the source audience users who have viewed a certain line of products, users who have been active within the last 90 days, etc. the source should be adjacent to the target. more information on choosing the right source and target audiences can be found here. once a source and target audience are chosen, lytics handles all the heavy lifting of building, optimizing, and deploying the model. for each model, lytics automates the 1 feature selection, 2 model training and validation, and 3 scalable realtime prediction, which allows you to spend less time wrangling data to build models, and more time to create better experiences for your customers. feature selection feature selection is the process of identifying which features or fields to include in a machinelearning model. while all user profile data is accessible for use in building lookalike models, manually identifying which features to include can be a tedious and timeconsuming process if there are hundreds or thousands of fields. lytics provides an option called autotune which uses intelligent feature engineering to select the most predictive features across all the available features. model training when training models, lytics utilizes three model types: random forests, gradient boosting machines, and logistic regression. under the hood, lytics trains dozens of models using different parameters and hyperparameters to optimize the modelbuilding process. realtime prediction lytics lookalike models update user scores in realtime, so you can start targeting users immediately once the model is building, but also as their behavior changes or new users are added. rather than using a static list, predictive audiences built from lookalike models provide a dynamic pool of users that will respond best when they are ready for ads or other marketing messages. you can also adjust your targeting criteria to make the best tradeoffs between reach and accuracy to maximize your marketing budgets. to create your first lookalike model, please refer to the building lookalike models document. you may also choose to use the lytics lookalike model api to create a model. an example of creating a model via the api is provided below. export liokeyyour api key export sourceidyour source id export targetidyour target id echo source: sourceid, target: targetid, name: my lookalike model, config: autotune: true, buildonly: true, collect: 5000, rerun: true, usecontent: true, usescores: true http post https:api.lytics.ioapiml keyliokey for a more detailed stepbystep walkthrough of creating, analyzing, and deploying the model to export the best users to bigquery check out this python notebook for more detailed documentation on the api go here for guidance on navigating lookalike modeling in the ui, and more detailed tips and information, continue to the next page. updated about 1 month ago scraped from: https:docs.lytics.comdocsbuildinglookalikemodels building lookalike models quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy the lookalike model builder provides an interface to quickly build custom machinelearning models and predictive lookalike audiences based on your data. in the lytics ui, the model builder is located under the lookalike models tab within the using profiles section. to get started, click create new this opens up the lookalike model builder. basic configuration for most use cases, building a model by setting the basic configuration parameters is sufficient. the only required parameters are the selection of a source and target audience, which are very important for building a usable model. if you select audiences that are too dissimilar, the model may be unable to find lookalikes in the source audience. learn more about selecting the right audiences for your use case. the size of each audience is also important. when building a lookalike model, your source and target audience must have a minimum of 25 users and a maximum of 20 million users. if your selected audience exceeds the maximum size, you can add filters to refine it. for example, if the source audience is unknown users you could add a filter for active in the last 30 days to ensure you arent targeting unknown users with stale cookie identifiers. the basic model parameters are defined below. userswhosignedupforemail userswhomadeanonlinepurchase sourcesegmentslugname::targetsegmentslugname mymodelname advanced configuration for additional model configuration, select the advanced options. for manually built models without auto tune, one or more features must be selected for the model build, such as use scores, use content, additional fields. use scores use content additional fields visitcount emailview 5000 updated about 1 month ago scraped from: https:docs.lytics.comdocsevaluatinglookalikemodels evaluating lookalike models quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy the lookalike model summary view is divided into four distinct tabs, each offering valuable insights about your model: model summary the model summary dashboard highlights the essential metrics of your model, including accuracy, reach, and the features it leverages. it also enables you to create impactful predictive audiences, using lytics predefined options or custom definitions tailored to your needs. the summary section at the top of the image above features three key tiles. model exploration the model exploration section provides detailed insights into your model and visualizes how users in your source audience are categorized based on their likelihood to convert to the target audience. the left side of the image above displays the audience sizes for the source, target, and lytics predefined audiences: clicking any predefined audience updates the venn diagram on the right, providing a dynamic visualization of audience overlap. advanced model exploration the advanced model exploration feature provides a deeper dive into the relationship between the source and target audiences. when open, it displays a detailed chart that highlights the overlap in predictions between the two audiences. a slider bar within this feature lets you dynamically adjust the similarity score threshold. as you move the slider, the chart updates in realtime to show only users in the source audience whose similarity score meets or exceeds the selected threshold. this interactive tool not only offers a clear visualization of how your model performs but also enables you to refine the threshold to better suit your predictive lookalike audience. by finetuning this setting, you can precisely target the users most likely to convert, enhancing the effectiveness of your predictive audiences. creating new lookalike audiences the create new lookalike audience button, located above the venn diagram, provides a convenient way to build predictive audiences directly within the lookalike model interface. clicking the button opens a modal with two editing options: feature importance and correlation the feature importance and correlation charts highlight the key features influencing user conversions from the source to the target audience. the importance chart ranks these features by their relative importance, as determined by the model, from most to least significant. the correlation chart ranks features by their correlation to the target audience. by analyzing this data, you can gain valuable insights into the characteristics shared between your source and target audiences. these insights can help you refine your audiencebuilding strategy, including the option to incorporate specific highimpact features into new audience definitions for improved targeting. lookalike audiences once your model becomes active, you can start creating lookalike audiencespredictive audiences that utilize your lookalike model. the audiences tab serves as a centralized hub, displaying all your existing lookalike audiences and providing the option to create new ones directly from the interface. model configuration the configuration tab displays the settings used when creating the model. model diagnostics the diagnostics tab provides an advanced view into a lookalike models performance and can help provide assurance that the model has a robust statistical foundation. this information is helpful for technical users looking to gain extra insight into the data science behind this lookalike model. when youre ready to start using your model, you can activate it to calculate and assign similarity scores to your user profiles. once activated, the process may take up to an hour to fully calculate and save the scores. deactivating a model stops the scoring process, meaning that neither new users nor existing users will be scored by the model moving forward. however, any users who already have scores will retain those values, even after the model is deactivated. updated about 1 month ago scraped from: https:docs.lytics.comdocscreatingpredictiveaudiences creating lookalike audiences quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy once your lookalike model is built and users are scored ensure the model training only option is unchecked, or press activate in the top left of the model summary page, you can begin creating lookalike audiences with varying decision thresholds or percentiles based on model predictions. to create a new audience, click the create new lookalike audience button on the modal page. this opens a dialog where you can choose between the quick editor and the advanced editor. when setting up the new audience, you will use either the lookalike model predictions field or the lookalike model percentiles field explained further below. lookalike model predictions lookalike model percentiles model predictions are expressed as probabilities on a 01 scale, with values closer to 0 indicating a low likelihood of resembling users in the target audience, and values closer to 1 signifying a higher likelihood. you can adjust the threshold as you like or add additional rules before saving the audience. see improving lookalike models for tips on adjusting the decision threshold. any audiences built using the audience prediction score for your model will display in the model usage module. using lookalike model percentiles another option to build a predictive audience is by using the lookalike model percentiles field. similar to the lookalike model predictions field, the lookalike models are keys for the lookalike model percentiles field. lookalike model percentiles lookalike model predictions lookalike model percentiles the percentile for a model represents the value at which a percentage of the predictions fall below. for example, the 80th percentile represents the prediction score at which 80 of all other scores fall below, or more simply put; the top 20 of users. percentiles help account for the shape of a models prediction distribution, as it can sometimes be hard to determine who the best users are based solely on the prediction scores, if the distribution is skewed is any direction. updated about 1 month ago scraped from: https:docs.lytics.comdocsimprovinglookalikemodels improving lookalike models quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy building effective lookalike models can be an iterative process. to help with these iterations, lytics provides diagnostic messages that identify ways to improve your model performance. in general, these model messages can help you both select the best source and target audiences for your model and create the best predictive audience from your model. selecting the right source and target audience most of the time, lookalike models are built to drive users toward conversion. this flow should be reflected in how you consider selecting the right source and target audiences to build your model. when defining your source and target audiences, try selecting audiences that are at adjacent stages in your customer lifecycle, rather than at divergent ends of your funnel like brand new visitors and multipurchase premium customers. the diagram below represents audience similarity on a spectrum of model performance. consider the following examples of audiences with different levels of similarity. if youve selected divergent or overlapping audiences in your model, you will see diagnostic messages to guide you to select audiences that are more adjacent. adjusting the decision threshold most predictive audiences are built by identifying users in the source audience who have a high model score. this high score is called the decision threshold, and in most cases is around 0.5. to adjust the reach of the audience youre building, you might consider using a different decision threshold. diagnostic messages may suggest creating audiences using different decision thresholds, but you are always free to make the decision threshold whatever makes the most sense to generate a predictive audience of the right size. improving unhealthy models most of the time, diagnostic messages for unhealthy models will suggest building a model with different source or target audiences. occasionally, you might see a diagnostic message suggesting that a model should be built with different features. the model could not find sufficient signal in the features provided. try either using auto tune or providing additional features. in these cases, the underlying model didnt have enough of a signal in the data provided to be able to predict target behavior. imagine that youre an online retailer trying to build lookalike models for multipurchasers, but you build a model that doesnt have any purchase data. or you want to build an email churn model, but you dont have any email data in your model. in these cases, regardless of how your source and target audiences are constructed, your model would be missing the underlying signal required to make a successful model. you could either manually select additional fields to include in the model builder, or you can use the auto tune option on the model to have the machine attempt to automatically identify the best field candidates to include in the model. all lookalike models try to balance a tradeoff between accuracy and reach, which are two of the most important indicators of how your model will perform. as a general principle, you cannot optimize for both accuracy and reach. deciding which one to focus on will depend on your marketing use case. optimize for accuracy optimizing your lookalike model for accuracy is typically used for targeting later stages of your funnel. this enables you to be more precise, with the tradeoff of reaching fewer users. by identifying users who are most likely to convert, you can optimize their hightouch experiences to drive engagement, improve conversion rates, and increase customer lifetime value. in the example above, the model has a high accuracy score of 9 and a low reach score of 1. the shape of the model predictions graph has little overlap between the source and target audience, which indicates less similarity between the users of those audiences. however, for the select users that fall into the area of overlap, they have a higher likelihood of converting. optimize for reach optimizing your lookalike model for reach is most applicable for targeting users in earlier stages of your funnel. this will allow you to reach more users, with the tradeoff of being less precise. you can think of this as casting an intelligently wide net. by identifying users who are least likely to convert, you can focus your marketing resources on the users who are likely to convert, improving conversion rates and maximizing your budget spend. in the example above, the model has a low accuracy score of 2 and a high reach score of 8. the shape of the model predictions graph has a good amount of overlap between the source and target audience, which indicates more similarity between the users of those audiences. therefore, you will be able to reach more users in the source audience, but they have a lower likelihood of converting compared to a model with higher accuracy. balancing the tradeoff when balancing the tradeoff between accuracy and reach, consider the sum of accuracy and reach to determine a models fitness to be used. see the table below for a quick estimation of your models fitness to be used. in the first two screenshots shared, each model had a sum score of 10 for accuracy and reach 9 and 1, 2 and 8 respectively. therefore, both models would be considered good but they are optimized for different use cases. for a comparison, see the model below that has a moderate accuracy score of 5 and a moderate reach score of 5. updated about 1 month ago scraped from: https:docs.lytics.comdocsdestinations destinations quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy destinations are where the rubber meets the road. they provide push button ease for delivering the highly targeted audiences constructed from rich behavioral insights to all of your marketing delivery tools. this section will cover the tools and interface for managing data sync with your preferred destination. the landscape in which your consumers interact is broad. to ensure lytics makes it easy to both collect and deliver essential data, insights, and audiences, we have a variety of integration options. data destination integrations are managed primarily by decision engine but may also be available in conductor to streamline the delivery of raw events and profiles to your warehouse. creating a destination is simple: new source integrations are added from the decision engine interface by first navigating to the jobs option under the data section in the main navigation. jobs data from there, youll click create new job at the top of the list and enter the wizard to guide you through the creation process. select a provider each destination is first categorized by the provider, making it easy to narrow down the channel youd like to integrate with. to select a provider, click the tile representing your desired provider, such as google. select job type with the provider selected, well surface the various ways you can integrate with that particular provider. this will vary significantly by the provider. select authorization method each provider and job type may require additional authorization to finalize the connection. on the select authorization step, you can either select an existing authorization or create a new one. when creating a new authorization, you will be asked to provide the required credentials, such as key and secret, to proceed. configure destination the final step lets you provide the specific configuration details for your chosen provider and job type. again, the options supported by each provider will vary greatly, and providerspecific integration details should be leveraged to determine the optimal approach. once you have one or more destination jobs running, they will be accessible from the jobs list view, as pictured below. this view provides quick access to essential details: job status detailed states are provided to understand better what is happening in the background during a jobs lifecycle. these states will vary by job but include: for more information on job states or troubleshooting failed jobs, see job processing. job summary clicking on any of the items in the source list will navigate to its dedicated summary view for greater detail. this summary provides all the relevant information about each job youve created in lytics and an entry point to alter the configuration or status. at the top of the job summary page, youll find the following quickaccess information: you can edit the name and description of an existing job from its summary page to improve the organization and clarity of your accounts list of jobs. configuration the details section displays your jobs current configuration. this includes details such as the authorization used, where data is coming from, which data is being pulled in, etc. logs the logs section records the history of events for this job, details about the work completed, and the time each job was run. the logs are helpful to ensure your work is running as expected and for troubleshooting if any issues arise. below are descriptions of the job events you may see in the logs. updated almost 2 years ago scraped from: https:docs.lytics.comdocsgoals goals quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy lytics defines goals as highlevel objectives that measure marketing success, such as improving conversion rates, decreasing churn, and increasing customer lifetime value ltv. the lytics goals canvas provides a framework for moving your customers through their lifecycle and driving them towards your brands toplevel marketing goals. this approach allows you as a marketer to focus on defining your goal and creating relevant experiences that allow customers to take any path to get there. the lytics goals canvas is oriented around a few concepts: goals, experiences, and stages. lytics goals allow you to guide users through highly personalized experiences that increase customer lifetime value and drive progress towards your primary business goals. you will use the lytics canvas to create, edit, and monitor your goals. navigate to goals and click new goal in the top right of your screen to create a new one. choose an audience the first step in configuring a goal is picking your starting audience. select this audience by clicking the audience button blue circle on the lefthand side of the lytics canvas. you can choose an existing audience or you can build a new one by clicking new audience. lytics recommends keeping this starting audience as broad as possible, as there will be ample opportunity to add filters and refine your targeting through stages and experiences. you will be able to change this audience at any time, although that may drastically affect target audiences and reporting metrics. stages each stage represents a significant step towards your goal, as defined by a conversion event. a common conversion event is finding known users by collecting email addresses on a newsletter signup or registration page. stages contain experiences that give users opportunities to complete conversion events in various channels, such as on your website, through an email newsletter or facebook ad. a goal can have one or more stages, based on the needs of your use case. after defining the first conversion event, you will see an option to add a stage. moving through conversion events users move to the next stage when they complete the selected conversion event. goals are always configured so that users move through your stages from lefttoright as they complete conversion events. keep in mind that users dont always complete conversion events in the order anticipated, which means they may end up skipping a stage. if you want to prevent this, you can include additional logic in your conversion event definition. and remember, users can never be in more than one stage per goal at a time. experience drawer the experience drawer contains experience templates and experiences that arent part of a goal yet. add any of these to your goal by dragging them into a stage. open the drawer by clicking the icons labeled experience template or experience. if you drag an experience that is activated into your goal, you will need to pause it before being able to save your goal. you can do so from the lytics canvas by hovering over the experience card and then clicking the pause button. once youre ready to reactivate the experience in the context of the goal, click resume. prioritizing goals much like prioritizing experiences within stages, you can also prioritize goals. since membership is not mutually exclusive that is, users can be moving towards multiple goals at a time, it is possible that a user will be eligible for two simultaneous experiences from two different goals. when that happens, lytics will take into account goal priority when determining which experience will be delivered. the higher on the goal list, the higher the priority. stages are the individual steps that move customers towards your goal. users in your overall audience will automatically enter the first stage. when a user completes the first conversion event, they will move to the second stage, and so on through the last stage within your goal. a user can only be in one stage at a time. conversion events conversion events within lytics are behaviorbased audiences such as purchasers, highly engaged users, known users, etc. these determine which users are eligible for the experiences within a stage. when a user fulfills conversion event applied to a stage, they move to the next stage until they reach the last conversion event for that goal. if users have completed the conversion event applied to a stage, they will not be eligible for any of the experiences in the stage. this prevents overlapping targeting such as continuing to show a facebook ad to a user who has already converted on a particular campaign. to assign a conversion event to a stage, click the trophy icon to open a menu of available conversion events. a red dot next to the trophy icon indicates a stage that still needs to be assigned a conversion event. a conversion event is required to populate the metrics for a stage. stage metrics lytics tracks three key metrics to measure stage performance: potential reach, converted, and conversion rate. the table below defines these metrics and how often they are updated in the lytics ui. adding and rearranging experiences experiences can be added to a stage in several ways. you can add an experience directly to a stage by clicking add a new experience along the bottom of a stage. you can drag an experience template into a stage, which will create an experience with boilerplate settings to use as a starting point. you can also drag an existing experience into a stage, either from another stage or from the experience drawer. note: this will significantly change the audience for that experience. prioritizing experiences to ensure that your audience receives the right amount of messaging, each user will only be eligible for one experience per stage at a time. that means that no matter how many web modals you add to a stage, your audience members will only see one at a time. lytics wont just choose one at random though; there is a sophisticated decision engine that determines which experience will be delivered to the end user. part of the input into that engine is the order of experiences within a stage. the closer to the top of a stage an experience is, the more likely it is that it will be delivered to the end user. you can think of the lytics canvas as an intelligent audience builder. when you create new stages and add conversion events, you are effectively building new audiences based on the desired flow of users towards achieving your goal. unlike drip campaigns or more traditional journey builders, lytics doesnt require you to build complicated audiences. keep them simple, and the lytics canvas will do the heavy lifting for you. selecting audiences for your goals the following tips will assist you when building and selecting audiences for the conversion events and experiences within your goals. building block audiences when youre creating audiences to use in a goal, its helpful to think about each audience as a building block rather than a complete definition of who you want to be targeting. below are examples of good building block audiences: all users users who have shared their email address users who have purchased something excluding users building block audiences work best when they dont exclude anyone. it can be tempting to try to exclude users from a building block audience for example, excluding known users from all in order to target only anonymous users but the lytics canvas will do this for you automatically if one of your future conversion events is the audience known users. note: if you do end up doing this yourself, you can create a deadend path, where users are unable to progress due to the definition of the automatically generated audiences, which are explained in more detail below. there is one important exception to this rule. if you want to suppress a subset of your entire audience from your goal, you can exclude them from your overall audience. this will work so long as the exclusion rule is not related to the downstream conversion events. an overall audience is the first audience you select on the lytics canvas, which is represented by the leftmost blue circle as shown below. for example, if you used the audience below as your overall audience, you would exclude realtors from the entire goal. this will not create a deadend if none of the subsequent stages have conversion events dependent on the realtors criteria. all users, excluding realtors filter audiences if you want to target with more granularity than a stage audience allows, you can add a filter on individual experiences within a stage. this is a great point to target users based on behavior, such as engagement level. the audience you choose here will be added to your stage audience with an and statement. experience conversions a common pattern may be that you want any users who converted on the experiences in your stage to enter the next stage. learn how to target users based on experience interactions to build a conversion event like this. how audiences are used in the canvas now that youve learned how to select and build audiences, take a deeper look into how the lytics canvas uses these audiences to help users flow towards your goal. automatically generated audiences the lytics canvas automatically generates audiences based on your selected conversion events. for example, lets say you want to create a the first stage to target users who havent shared their email address, and the second stage will target known users who havent purchased yet. even though you havent explicitly created an audience for users who havent shared their email, or users who havent purchased, lytics will create these audiences for you. this example would look as follows in the ui: the following graphic shows how the lytics canvas takes your simple building blocks and creates more refined audiences. see the full size diagram. remember, if you exclude converted users on a building block audience, it can cause problems and prevent users from moving between stages. let the lytics canvas do the work for you! mutually exclusivity stage audiences are mutually exclusive, meaning a user can only be in one stage at a time. this is a result of the automatic audience building done by the lytics canvas. if a user in unknown users shares their email and now qualifies for known users they will automatically move into that stage without any manual intervention. the goal intelligence report provides a holistic view of how your existing campaigns are contributing towards the marketing goal you selected during the onboarding process. this report will help you answer questions about how effectively your marketing campaigns are driving conversions, how many engaged customers you have, and whether you are producing the right content for your users. the goal intelligence report contains modules that focus on answering a specific marketing question. each module is described below, along with definitions for the associated metrics and how often they are updated. goal progress the goal progres module contains three toplevel metrics for the campaigns youve imported into lytics. use this summary to measure how your campaigns are working together to achieve your goal. experience performance the experience performance module gives a summary view of all the campaigns you imported into lytics including the conversion rate of each campaign, and how much this conversion is impacting your marketing goal. use these metrics to prompt discovery into your campaign performance. select an experience from the list to view its specific intelligence report to surface additional actions and recommendations. connected customers the connected customers module shows what percentage of your users are actively engaged on multiple channels. lytics defines connected customers as users who are identified on two or more channels, and who are consistently engaged with your brand as measured by the lytics momentum score. connected customers are valuable to your business. use this kpi to create a stronger base of users who are more likely to convert on current and future campaigns. content classification the content classification module shows a list of the topics, documents, and relevance scores for the content used across your connected channels. lytics automatically analyzes the topics within your website content and determines userlevel affinities for those topics, which can help you decide what content to produce. use these metrics to ensure you are creating the right content to engage your target audience. content recommendation the content recommendation module surfaces content that is likely to resonate with your users based on their past behavior. lytics analyzes the content on your website through the lytics javascript tag. the relevancy percentage is calculated by determining the total number of users with various content affinities and comparing this to the total number of users within the audience that visited your site. create and deliver content that aligns with your users interests to drive engagement and conversions. connected customers represent the number of active users who are engaged with your brand across multiple channels. this is a lytics proprietary kpi that offers a quick and comprehensive understanding of the performance of your accounts identity resolution strategy. lytics defines a connected customer as a user who has: you can find the connected customer kpi in the goal intelligence report. these metrics come from a lytics audience that is automatically generated underthehood using the criteria described above. why is this kpi important? connected customers are valuable to your business. this kpi helps you understand how many connected customers you have and the impact of your marketing programs on user engagement. lytics helps you gain more connected customers by refining your audience targeting and messaging strategies to improve engagement and ultimately increase the roi for your marketing efforts. what can i do with it? track your connected customers for a pulse on engagement levels across your active campaigns. use this kpi to create a stronger base of users who are more likely to convert on current and future campaigns. you can also activate this connected customer audience in your downstream marketing tools such as facebook, salesforce marketing cloud, and more. updated almost 2 years ago scraped from: https:docs.lytics.comdocsexperiences experiences quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy lytics experiences have replaced the legacy personalize functionality, as such personalize campaigns are no longer supported by lytics. if your account has not yet migrated from personalize to experiences, you may access our legacy documentation here. overview experiences allow you to present tailored messaging to specific audiences. these messages can be on your website using lytics experiences, appear on a thirdparty ad platform like facebook, or emails delivered through an email service provider like sendgrid. since you can control the audience being targeted, you can deliver experiences that make sense for some users but not others. you can even recommend products or content based on each users unique content affinities and behaviors. experiences allow you to treat your customers as individuals rather than overmessaging them with generic offers and newsletters. common uses of experiences include: you can create and import new experiences as well as browse existing experiences including those that are associated with a goal by navigating to the experiences tab add your first experience the set up for each experience is slightly different, so whether you are creating an onsite lytics experiences or connecting to one of our outofthebox integrations, the simplest way to get started with experiences is to create a new experience and allow our experience editor to navigate you through each of the steps. experience list view within the experiences tab in the lytics app, you will find a list of all of your experiences. from this view you can see below for more information around each of these activities. adding experiences create a new experience within the experiences tab by clicking the add experiences button near the top of the page. selecting the new option will guide you through the experience editor steps for any experience lytics, outofthebox integrations, or generic the new option allows you to select from the full list of providers. if you want to import external marketing initiatives from outofthebox integrations that support experiences, you can do so within the experiences tab as well. click the add experiences button near the top of the page and select the import option to be guided through the experience editor steps for these supported integrations. the import option navigates you to directly to a list of the outofthebox integration providers. browsing experiences search: the easiest way to find an experience is to use the search box to the right of the experience list. as you enter your search term, the experiences list will display any matching experiences. filter: to the right of the experience list you can filter experiences based on their... imported experiences that havent been activated yet will be tagged as externally managed sort: by default, the list of experiences is sorted by name. if you choose to sort by date last modified instead, your preferences will be saved for the next time you visit. experience statuses an experience status indicates what lifecycle the experience is in and the health of whether it is exporting a lytics audience to a thirdparty tool in realtime. below is a complete list of experience statuses: draft: a complete, or partially complete experience that has not yet been activated. active: an experience that can be seen by your users and has not passes its end date. paused: an experience that was activated but has been paused. to edit an experience that has been running, you must pause it first. scheduled: an experience that will be activated at a start date set in the future. externally managed: an imported experience from a third party tool that hasnt been configured with a lytics audience. complete the configuration by editing the experience in the experience editor. ended: an active experience that has passed its end date. error: an experience that failed to export audience data to the appropriate tool. warning: an experience that encountered issues when attempting to export audience data to a third party tool. it may recover on its own, or proceed to an error status after a number of failed attempts to export data. experience summary view once you have created or imported experiences they will be populated as a list within the experiences tab. click on an experiences from the list to enter the experiences summary. from this summary page you can... see below for more information about each of these activities. experience overview within the experience summary you will find important information about your experience. the summary includes: activating experiences from the experience summary page you are able manage the activation of individual experiences using the following buttons on the righthand side of the summary page. editing experiences within the experiences section of the lytics app, you can edit an existing experience by clicking on the desired experience. this will navigate you to the experience summary page. here you can click on the edit button to enter the experience editor for this experience. you cannot edit an experience while it is activated, so you will not be able to enter the experience editor at that point. if you need to edit an active experience, you can pause it from the experience summary page. once you have made and saved your edits, you can then resume the experience from the experience summary page deleting experiences to delete an experience from lytics, click on the experience you wish to delete from within the experiences list. you will be navigated to the experience summary page where you can delete the experience from the options menu. you will be asked to confirm that you wish to delete the experience, click the delete permanently button to complete the deletion. the experience editor is used to create, save, edit, and activate marketing experiences. combined with audiences created in the audience builder, sophisticated experiences can be built to deliver the right message to your users at the right time. to enter the experience editor, you can either add an experience or edit an experience. the available steps of the experience editor are dictated by the experience provider you select. experience providers lytics experiences use lytics experiences to personalize a website with modals, slide outs, sticky bars, etc. this does not require connecting to another tool. experience editor steps for lytics experiences if you are creating or importing a new experience, you will be given a choice of experience providers providers are the tools that deliver your experience. the selection of lytics as a provider will prompt you to select a tactic from a list of options. the following tactics are available for lytics experiences: once youve chosen your tactic, the experience editor will take you though each step. choosing a url to promote is the first step in the experience editor for a drive traffic experience of the lytics provider. this step does not apply to experiences of any other tactic or provider. this step allows you to enter the destination you want to link as a call to action in your drive traffic experience. the protocol is preset to but can be changed to . enter the rest of the url into the input field. https: http: specific to the collect leads tactic of lytics experiences, this step is for selecting what information you collect on the form. here you will configure which form fields will be shown and thus what information you capture about your user. the recommend content experience type is used to surface content an article, a blog post, a product page to visitors. each recommendation is personalized to the individual. this could be used to keep visitors engaged by surfacing what to read next, or assist casual perusers by prompting a starting point. furthermore, the experience can be configured to only a set of pages or a subset of your total audience. this allows you to limit the recommendations from showing up on auxiliary pages like the home page, company page, or checkout page. it can also only be shown to users with high usage. this way low usage visitors can be targeted with a more aggressive campaign. choosing content is the first step for a recommend content experience. lytics provides a variety of filters and options for tailoring what content can be recommended and how the recommendation algorithm should rank the content. a content collection allows you to limit the scope of documents that can be recommended by your experience. if youre creating a summer seasonal experience, for example, you wouldnt want articles that are out of season. a content collection is how you define which articles are appropriate to recommend. use the select a content collection dropdown to choose an existing collection or to create a new one. clicking on new content collection will take you to the collection builder. the state of your experience will be saved, and after creating your new collection, you will automatically be returned to the experience editor. the lytics content recommendation engine allows you to configure the way in which the experience will recommend content. the recommendation settings work within the limits set by the content collection. if your content collection includes articles published six to twelve months ago and you choose the recently published method, then the experience will begin recommending sixmonthold articles, which are the most recentlypublished articles within the collection. recommendation method the recommendation methods control how the recommendation engine ranks the content. new content only the only recommend content the user has not seen? checkbox controls whether previously viewed content will be included among the recommendations. leave this setting checked to ensure that a user wont be recommended content they have viewed in the past. shuffle content the shuffle content on each page load? checkbox controls whether a user should be recommended different content if they encounter the experience again e.g. after reloading. check this setting to vary the content that is recommended on subsequent requests or leave it unchecked to show the single best recommendation. the design step is present in the editor for all experiences of the lytics provider. it lets you customize the appearance and content of the widget displayed to your users on your website. this step offers layout and position options as well as a custom css option. first, choose your experiences layout. there are five different layout options, and their availability depends on the tactic you selected. different content fields are supported by different experience layouts or tactics. including an image for your experience is optional, the image will be shown above or beside the messaging of the experience. any image you include must be hosted publicly as this input takes a url but not an upload. experience widgets can be displayed in different positions depending on their layout. the available positions will show up as a darker gray while the active position is blue. there are three ways to theme your experience. choose a theme: lytics provides light and dark themes you can select. build a custom theme: there are eight color options available to build a theme in the experience editor: use custom css: use your own custom css to match the experiences styling with your sites design. experiences are built using the pathfora sdk. read about using custom css with pathfora. the audience step in the experience editor allows you to configure the target audience for your experience. use the dropdown to select from among your prebuilt audiences. click new audience at the bottom of the dropdown menu if you need to build a new one and you will be directed to the audience builder. this step is required for standalone experiences, such as a oneoff email campaign or a seasonal ad promotion. once you have selected an audience, potential reach displays how many people will be eligible to receive this experience. this step is optional if your experience is part of a goal on the lytics canvas. here you can add additional filtering to your experience. any audience you choose will be combined with the stage audience, resulting in a more targeted experience with a smaller audience. you can also leave this blank if you dont want additional targeting. the potential reach displays how many people will be eligible to receive this experience. the stage coverage metric displays what percentage of the stage will be eligible to receive this experience. if your experience is not providing the desired results, you can pause your experience and target a new audience. just keep in mind that the metrics are for the experience and wont explicitly indicate changes you made. you can also make changes directly to the target audience, and the potential reach of the experience will be adjusted automatically. when creating a new recommend content experience, you will see a content alignment score in the target audience step of the experience editor. lytics calculates the alignment percentage by comparing the affinities of your target audience to the affinities in your selected content collection. use the alignment score information to ensure you are recommending content or products that are relevant for your customers, which can help improve engagement and conversion rates of your marketing campaigns. for a quick demonstration of using the affinity alignment score while building a content recommendation, watch this video. the display step is present in the editor for all experiences of the lytics provider. use the display step to set where and when the experience will show up for your users. set rules for which urls an experience can be displayed on, the triggers for displaying it, the frequency with which it appears, and the date range during which the experience is available to show. prior to activating your experience, ensure that the display url is included in your url allowlist for orchestrate account setting. appears on rules are an optional tool used to specify which webpages within your site the experience will or will not be shown on. this can be a fixed list of pages or expressions that match many pages. there are four urlmatching strategies offered, and rules can be used in combination. each of these urlmatching strategies can be used to create a rule determining when the experience is shown with show on selected, or to specify the cases where the experience should not be displayed with hide on. consider the following use cases, and some suggested solutions using the appears on rules. only show the experience on the home page solution: show on a simple match for www.lytics.com. www.lytics.com displays for: https:www.lytics.com http:www.lytics.com http:www.lytics.com?adcampaign1ed387faed does not display for: http:www.lytics.comblog https:activate.getlytics.com show the experience on the blog, but nowhere else solution: show on url contains blog blog displays for: http:www.lytics.comblog http:www.lytics.comblogsomepostinthepast https:www.lytics.comblogtagcustomerdataplatform?referrerthegreatgoogle does not display for: https:www.lytics.com http:www.lytics.comcareers gate a specific resource solution: show on an exact match for https:www.lytics.comresources?ida763efd12c displays for: https:www.lytics.comresources?ida763efd12c does not display for: http:www.lytics.comresources?ida763efd12c https:www.lytics.comresources?ida763efd12csomethingthatwill404 show a referral thank you message on integration pages solution: show on a match for the regular expression integrations.??.?refourpartner integrations.??.?refourpartner displays for: http:www.lytics.comintegrationscampaignmonitor?refourpartner http:www.lytics.comintegrationscampaignmonitor?session125929refourpartner http:www.lytics.comintegrationsadroll?refourpartner https:www.lytics.comintegrationssegment?refourpartnerqptrue does not display for: http:www.lytics.comintegrationscampaignmonitor?refsomestranger http:www.lytics.comintegrations?refourpartner http:www.lytics.comblogadroll?session125929refourpartner show the experience anywhere on the site except the careers pages solution: use two rules in combination. show on url contains lytics.com hide on url contains careers lytics.com careers displays for: http:www.lytics.comblog https:www.lytics.com does not display for: http:www.lytics.comcareers http:www.lytics.comcareerscontentwriter the advanced display options allow you to respond to the users interaction with your site, using the state of their active web session to determine when an experience is displayed. these are all optional and can be used in combination. when using triggers in combination, note that the conditions function as an all conditions met, not any condition met. if you set an experience to show after 5 page views and to show after 70 page scroll, when your user scrolls to 70 of the third page, the experience will not show because only one of the two conditions have been met. show after 5 page views show after 70 page scroll if you set an experience to show up to 5 times per user, ever and show up to 2 times per user per session, and a user is shown the experience twice each on their first two sessions, for their third session they will be shown the experience for the final time. show up to 5 times per user, ever show up to 2 times per user per session if you set an experiences frequency to show up to 2 times per user per session and a trigger of show after 5 page views, your user will see the experience upon reaching the sixth page, again on reaching the seventh page, and not again during their session. show up to 2 times per user per session show after 5 page views these are the same options as above, but applied to when the user has seen the experience but not clicked the cta. pathfora is a very powerful clientside personalization sdk. in addition, it is opensourced. because anyone is invited to contribute to pathfora, providing clear controls for all features is impossible. as a result, we provide a technical but friendly interface for using overrides. overrides allow you to leverage the full power of pathfora for functionality such as custom load and callback handlers and advanced customization and styling. in the coming weeks, well extend our documentation to outline several common patterns for leveraging overrides. for those looking for initial information, dont hesitate to contact your primary account contacts. for others that are already leveraging overrides directly via our apis, you will also see a representation and area for managing those overrides directly within the advanced settings in the experience wizard, as pictured below. set the date range for when the experience will be shown to your users. the start date will be populated with the current day, so that your experience will start immediately without you needing to set anything. you can set a start date in the future and activate the experience ahead of time; it will have a scheduled status and then switch automatically to active on the start date. the end date is not required. if you do not set an end date, the experience will run indefinitely as long as it is active. end dates cannot be in the past, nor can they precede the start date. the date range is inclusive, so if you set a start date of 3212022 and an end date of 3232022, the experience will run for 3 days. on 3242022, the experience will switch from active status to ended on its own and users will no longer see it. the review step is the final step for all experiences of the lytics provider. it provides an ataglance view of everything youve chosen for your lytics experience. though there is nothing you need to do on this step, it does give you an opportunity to check that all your selections are listed as you expect before you activate the experience. use the preview button in the top right of the screen to see how your experience will look when its active on your site. simply enter the url you want to see the preview on in the modal that appears, and youll be directed to your site but with the experience running as it will once it becomes activated. from this preview state you can get a better idea of howwhen the experience is triggered and see what your users will see. the url you enter must match the appears on conditions you set in the display step. the target audience, however, is ignored for previewing ease. when you first enter the experience editor, you will see a save button in the top right corner which allows you to save your progress at any time. if you have made it to the final step within the experience editor review for lytics experiences, the next step button will now be a save and exit button. click on this button to save and exit your experience. if you have previously saved your experience and made no other changes this will simply allow you to exit the experience editor. after the experience has been saved once, the button will become a save button that is only enabled when you make further changes. if you have not yet named the experience at the time you save, you will be prompted to do so at this time, and the experiences tactic will be suggested as a default. when creating a new experience within lytics, name it including the purpose and any categorization e.g. drive sales: holiday. when working with existing experiences from your third party tools, name the imported experiences on lytics with the samesimilar names as in the external tool. unsaved experiences at any point you may discard your unsaved experience by clicking on the close button x in the top left corner of the experience editor and then clicking confirm when asked are you sure you want to discard unsaved changes? you may also select the delete option from the more options ... menu in the top right corner of the experience editor and click delete permanently when prompted. both methods will remove you from the experience editor without saving. previously saved experiences if you have previously saved the experience and have returned to make edits, you may discard all changes youve made since your last save by clicking on the close button x in the top left corner of the experience editor and clicking confirm when prompted or by selecting the discard changes option from the more options menu ... in the top right column. the former will discard changes and exit the experience editor. the latter method will allow you discard changes, but stay within the experience editor where you can make new edits. you may permanently delete previously saved experiences from within the experience editor by selecting the delete option from the more options menu ... in the top right column and clicking delete permanently when prompted. activating your new lytics experience prior to activating your experience, ensure that the display url is included in your url allowlist for orchestrate account setting. once you have completed the steps in the experience editor and saved your experience you can activate it from the experience summary page. you will be directed to this page once you exit the experience editor. outofthebox integration experiences you can import existing marketing initiatives currently managed by your channel providers, which enables you to quickly gain insights about the campaigns youre already running. you can then take action by connecting these experiences to your lytics audiences. to import an experience, click the add experience button in the manage experience page. you can select either the new or import options in order to import experiences from our outofthebox integrations. both options will direct you to the same flow. you can monitor outofthebox integration experiences and activate audiences for them within lytics, but the management of external experiences still happens inside your channel tools. pausing or deleting imported experiences in lytics simply stops the audience syncing process. it does not pause or delete the campaign in your channel tool. experience editor steps for outofthebox integration experiences from the provided list, you will choose a provider from the list of outofthebox integrations. each provider has a single tactic selection associated to it. you will need then need to configure the experience based on that selected provider. below you will find an overview of the steps for navigating the experience editor for outofthebox integrations. for more detailed information for specific integrations, use the links below to navigate to the the integration documentation for your selected provider: after selecting your provider, you will be prompted to select or create an authorization to connect the integration. existing authorizations will be shown. if you have multiple accounts within a particular channel, you will choose which account to import from. if you dont have an authorization, you will be prompted to add one before continuing. to create a new authorization, click the add new authorization button, which will take you to a modal where you can enter the appropriate credentials. for providerspecific documentation on creating authorizations, see the below documentation for your selected integration. after you have selected your authorization you will then select the campaigns you want to import into lytics you may select up to 10 at a time. the total number of experiences you can bring into lytics is only limited by the amount in your connected provider. however, to have meaningful and accurate reporting, its recommended to only import experiences that will add value to the use cases you are activating within lytics. completed campaigns that have already ended, will not show on the list of importable experiences. those that are saved as drafts, paused, or currently running, those will be available to import. once you have imported an experience, you will activate it by adding a lytics audience. this lets you enrich your existing campaigns with lytics behavioral audiences, content affinities, and delivery optimization. the target step in the experience editor allows you to configure the target audience for your experience. use the dropdown to select from among your prebuilt audiences. click new audience at the bottom of the dropdown menu if you need to build a new one and you will be directed to the audience builder. the configure step in the experience editor allows you to configure how the audience for your experience will be exported to the thirdparty tool you are using. in order to successfully activate and export an experience to your provider tool, you will need to correctly configure the experience. see below for links to the configuration documentation for each supported integration. delivery optimization takes the guesswork out of deciding who to send what message, and when. for the third party experience providers builtin to the lytics canvas, you can use this option to automatically determine when to deliver messages to individual users on those channels. this step in the experience editor allows you to toggle delivery optimization on or off. if you do nothing, it will remain off. you can turn it on to help prevent overmessaging and let you focus on creating great experiences, rather than manually building delivery logic rules that are complicated to manage and execute. lytics uses data science to determine the optimal delivery time of an experience for individual audience members. the delivery time is based on when an audience member last interacted with your brand via the channel of the experience, as well as their behavior when that interaction happened. experience decisioning also uses the experiences performance measured by clickthrough rate ctr to estimate engagement likelihood. the actual delivery of an experience is always controlled by the downstream tool, where there could be additional logic necessary to deliver the experience. lytics delivery optimization simply controls when the user data is triggered to send to your thirdparty tool. consider the experience tactic when determining if you want to turn delivery optimization on. for experiences using realtime delivery such as iterable triggered emails, delivery optimization can help improve user engagement by reaching each user at the ideal time. in other circumstances such as sending a onetime blast email through sendgrid for a sale promotion, youll likely want to leave delivery optimization off and simply push a list of all current users into the audience so that you can send the campaign as soon as youre ready. if delivery optimization was enabled, you may not see users exported to sendgrid until they need to receive a message the most optimal time. when you first enter the experience editor, you will see a save button in the top right corner which allows you to save your progress at any time. if you have made it to the final step within the experience editor configure delivery for outofthebox integrations, the next step button will now be a save and exit button. click on this button to save and exit your experience. if you have previously saved your experience and made no other changes this will simply allow you to exit the experience editor. after the experience has been saved once, the button will become a save button that is only enabled when you make further changes. if you have not yet named the experience at the time you save, you will be prompted to do so at this time, and the experiences tactic will be suggested as a default. when creating a new experience within lytics, name it including the purpose and any categorization e.g. drive sales: holiday. when working with existing experiences from your third party tools, name the imported experiences on lytics with the samesimilar names as in the external tool. unsaved experiences at any point you may discard your unsaved experience by clicking on the close button x in the top left corner of the experience editor and then clicking confirm when asked are you sure you want to discard unsaved changes? you may also select the delete option from the more options ... menu in the top right corner of the experience editor and click delete permanently when prompted. both methods will remove you from the experience editor without saving. previously saved experiences if you have previously saved the experience and have returned to make edits, you may discard all changes youve made since your last save by clicking on the close button x in the top left corner of the experience editor and clicking confirm when prompted or by selecting the discard changes option from the more options menu ... in the top right column. the former will discard changes and exit the experience editor. the latter method will allow you discard changes, but stay within the experience editor where you can make new edits. you may permanently delete previously saved experiences from within the experience editor by selecting the delete option from the more options menu ... in the top right column and clicking delete permanently when prompted. activating your new outofthebox integration experience once you have completed the steps in the experience editor and saved your experience you can activate it from the experience summary page. you will be directed to this page once you exit the experience editor. utm tracking in addition to tracking default utm variables when users visit your website, lytics supports two custom utm parameters specific to tracking clicks from outofthebox integration experiences. you may be recommended by the lytics experience insights to add utm parameters to an experience. the lytics insights system will check the target url of your eligible imported experiences to see if it has these parameters configured and advise you on which experiences are missing them. for most email providers click information is collected automatically through webhooks or apis, but for ads this is the only way of recording user attribution. lytics highly recommends using these utm parameters in your ad experiences as it will not only allow you to build and target audiences of users who have converted on your ad in lytics, but it will also inform lytics decisioning and insights as the platform builds a better understanding of which users respond to which experiences. use the following custom utm parameters in any links coming from your external experiences. utmlyticsexperience utmlyticssource facebook these variables allow us to link the website visit with a click from the experience. thus recording what users clicked on what experience. using these utms will only work if the website linked from your ad has the lytics javascript tag installed. the steps for adding these parameters may differ by provider, but generally speaking you can add them directly to the target url of your experience, for example: https:www.lytics.com?utmlyticssourcefacebookutmlyticsexperience23847 https:www.lytics.com?utmlyticssourcefacebookutmlyticsexperience23847 please refer to the providers that have specific docs on how to add these parameters: the utm parameter is recognized by lytics and mapped to the user field converted on experience url param which is available in the audience builder so that you can create audiences of users who have clicked on your experiences. the key of this map field will contain the external id of your experience and the value will be the number of times the user clicked on the ad set. if you find that the number of users in an audience utilizing this field does not match the conversion count you are seeing on your experience summary page, it is important to remember the source of these numbers is different. usually for ad providers, metrics are aggregate numbers imported directly from the api of the ad tool. if you added the tracking url parameter to your ad after publishing, you may see a smaller user count when utilizing the user field. generic experiences generic experiences allow you to export your lytics audience to any tool you choose. since you will not be able to monitor the performance of this experience from lytics, its primary use cases are to act as a placeholder for.. generic experiences allow you to produce audiences with all the nested targeting rules of a stage within a goal to make the audience mutually exclusive of the other stages. as such, generic experiences should only be used for experiences included in a stage of a goal. unlike outofthebox integration experiences, delivery optimization and experience reporting is not available for generic experiences, so generic experiences should only be used for experiences included in a stage of a goal. experience editor steps for generic experiences from the provided list, you will choose a generic as the provider. generic experiences only support one tactic: the target step in the experience editor allows you to configure the target audience for your experience. use the dropdown to select from among your prebuilt audiences. click new audience at the bottom of the dropdown menu if you need to build a new one and you will be directed to the audience builder. this step offers a quick overview of the additional steps you will need to take to activate your audience for this experience upon saving and naming you experience, an audience will be generated. if you are still within the experience editor the tutorial screen will be updated to provide a link to the generated audience. if you have exited the experience editor you can find this link by clicking the help link in the experience summary page. when you first enter the experience editor, you will see a save button in the top right corner which allows you to save your progress at any time. if you have made it to the final step within the experience editor tutorial for generic experiences, the next step button will now be a save and exit button. click on this button to save and exit your experience. if you have previously saved your experience and made no other changes this will simply allow you to exit the experience editor. after the experience has been saved once, the button will become a save button that is only enabled when you make further changes. if you have not yet named the experience at the time you save, you will be prompted to do so at this time, and the experiences tactic will be suggested as a default. when creating a new experience within lytics, name it including the purpose and any categorization e.g. drive sales: holiday. when working with existing experiences from your third party tools, name the imported experiences on lytics with the samesimilar names as in the external tool. unsaved experiences at any point you may discard your unsaved experience by clicking on the close button x in the top left corner of the experience editor and then clicking confirm when asked are you sure you want to discard unsaved changes? you may also select the delete option from the more options ... menu in the top right corner of the experience editor and click delete permanently when prompted. both methods will remove you from the experience editor without saving. previously saved experiences if you have previously saved the experience and have returned to make edits, you may discard all changes youve made since your last save by clicking on the close button x in the top left corner of the experience editor and clicking confirm when prompted or by selecting the discard changes option from the more options menu ... in the top right column. the former will discard changes and exit the experience editor. the latter method will allow you discard changes, but stay within the experience editor where you can make new edits. you may permanently delete previously saved experiences from within the experience editor by selecting the delete option from the more options menu ... in the top right column and clicking delete permanently when prompted. activating your new generic experience once you have completed the steps in the experience editor and saved your experience you will need to navigate to the created audience and export it to the integration of your choice. navigate to the created audience in the experience summary page by clicking on help and then the link found within the finish setting up your generic experience section. export the audience from the audience summary page. experience performance reporting on the experience summary page, you will find experience performance reporting. this reporting contains three metrics for the campaigns youve created or imported into lytics. use these metrics to establish a baseline of your campaign performance and measure growth over time. experience intelligence report in addition to the experience performance reporting, the lytics view feature can be enabled in your account to give you additional information about your experiences via an experience intelligence report. the experience intelligence report, once enabled, will add the following modules to your experience summary page as well as in the goal intelligence report for experiences within goals: your account must have lytics view feature enabled in order to see this module. reach out to your customer success manager to have this feature enabled. experience insights the insights module displays how many insights have been generated for your account in the last week. lytics experience insights are the combination of a fact and an action. the fact is a metric about your audience or campaign data, and the action is a recommendation on how to improve engagement and conversions. leverage insights while evaluating and planning your campaigns to make datadriven iterations and achieve your goals more efficiently. see insights introduction to learn more. goal attribution the goal attribution module displays metrics around how conversions on your experience relate to conversions on your goal. the percentage compares the total number of people reached by the experience and those who converted. for facebook ads with utm parameters included, lytics will capture conversion data at the user level. use this metric to determine which campaigns to invest more or less in based on how they contribute toward your larger marketing goal. content recommendation the content recommendation module surfaces content that is likely to resonate with your users based on their past behavior. lytics analyzes and classifies the content on your website through the lytics javascript tag. the relevancy percentage is calculated by determining the total number of users with various content affinities and comparing this to the total number of users within the audience that visited your site. create and deliver more content that aligns with your users interests to drive engagement and conversions. see content affinity introduction and content recommendation to learn more. data science performance simulation the data science performance simulator module displays simulated conversion rate metrics that compare how your campaign audience performed against lytics outofthebox, behavioral audiences. this gives a riskfree preview of how effective your campaigns could be when leveraging behavioral scores and custom models powered by lytics data science. you can activate lytics data science audiences and custom models in your campaigns by upgrading from the free trial. see data science introduction to learn more. updated over 1 year ago scraped from: https:docs.lytics.comdocsbehavioralscores1 behavioral scores quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy learn about lytics behavioral scores lytics uses data to create more data through a suite of data science techniques. the purpose is to synthesize raw, impenetrable data into something insightful that can be utilized effectively in campaigns. this synthesis, like all data in lytics, is applied at the user level. while aggregated analysis may be helpful for looking at trends in customer behavior, userlevel analysis can be utilized for onetoone marketing. much like how the lytics content affinity engine allows for immediate targeting and execution using content affinity audiences, lytics descriptive and predictive modeling allows for immediate targeting and execution using behavioral analysis. lytics constantly tracks events from many data sources: web, email, salesforce, and custom sources as well. looking at this data directly is not useful; it needs to be summarized. lytics allows for simple summaries by means of user fields and for use in audiences, but there is more to be learned from all this behavioral data. behavioral analysis asks many questions and computes the answer. questions such as: after finding the answers to these questions, for every user, lytics uses these answers to inform nine lowlevel scores that get assigned to every user who has shown enough behavior for a confident assessment. these scores are computed automatically, they are kept up to date continuously, and they are assigned relative to other users in the account. there is a lot to learn looking at userlevel behavioral data. lytics evaluates behavioral data automatically in realtime and reports nine scores for each user. these scores each represent a distinct behavioral quality and can be composed to build rich audiences. please note, users must have behavioral data to make confident measurements before they have scores. if a user was added to lytics via email upload, for instance, they would have no scores. note: if you are already a customer of lytics, you can see the following explanation of each score alongside your own data in the app on the scoring page. quantity quantity measures a users cumulative activity over their lifetime of brand engagement. the more activity the user registers, the higher the score. this score measures the user relative to all other users. it is a common tactic to target a user based on the number of times the user has visited a website or has performed some other behavior. this becomes increasingly difficult as marketers add more data sources to their stack, and the user continues to engage over time. quantity takes into account a users behavior on all data sources and measures that relative to how the most and least active users are engaging so a users score will always be between 0 and 100. you can think of this as a test score. what is the point of scoring between 0 and 100? this is how we can ensure that any audience created with a score will always stay relevant. perhaps 1,000 page visits seems like a lot for a user now, but the number will only grow larger as your site grows older and the amount of content you have increases. another benefit of having a bounded score range is the ability to see the complete distribution of all users. this is an example of how scores look like across an entire audience. the xaxis is the score ranging from 0 to 100; 5 to 95 in the example for clarity and the yaxis is the number of people who have that value as their score. frequency frequency measures how consistent a user is interacting with your brand over time. more frequent interactions means a higher score. this score measures the user relative to all other users. this serves as a measurement of user regularity. do they visit once a week? once a day? once a lifetime? since this score is relative to all your users, you can easily target your most frequent users, rather than something like users who visited in the last week, which will vary wildly in size. again, this score has a fixed range of 0 to 100. all the scores are like this. it is how we can continuously update user profiles without having to update audience definitions. recency recency measures how recently the users general interaction has been. more recent activity means a higher score. this score measures the users recent activity relative to the users past activity. without scoring, this would be achieved by looking at the last time a user visited. although better than nothing, that approach is kind of crude. maybe an atrisk user opened your email by accident? itd be an expensive oversight to assume that the user had recent activity and didnt need any nurturing. intensity intensity measures the depth of a users typical interaction with your brand. more sustained intensedeep usage means a higher score. this score measures the user relative to all other users. the behavior a user exhibits during a single session is very telling of them as a consumer. if they have high interaction in a session high intensity they are more likely to be a deeper researcher or more curious. if they have low interaction in a session low intensity they are more likely to be casually browsing or engaged with a certain piece of content, but not your overall brand. momentum momentum measures the rate at which users are interacting with your brand. users who are interacting more than usual with your brand will have a higher score. this score measures the users recent activity relative to the users past activity. its easy to confuse how momentum and recency differ, but they are actually very different. universally speaking, weve found them to have a 5 correlation. recency measures absolute recency of activity, but just because a user has recent activity, doesnt mean theyre not at risk of churning. if a user maintains a constant rate of activity, their momentum score will be 50. if they are more active than they used to be, their momentum will be greater than 50 and might warrant a loyalty offer. if they are less active than they used to be, their momentum will be less than 50 and might warrant a winback campaign. propensity propensity predicts how likely a user is to return with subsequent activity. users exhibiting positive interaction patterns are more likely to return and have higher scores. this score measures the user relative to all other users. there are many reasons why users churn changing interests, competition from competitors, bad experience, etc. but from a data perspective, attrition of any kind starts to look similar. propensity employs an ensemble of statistical models to identify any patterns it can find for detecting how and when attrition starts to occur. with time, its able to find more patterns in your data and become increasingly accurate in identifying when users start to exhibit those behaviors. consistency consistency measures the regularity or stability of a users engagement pattern. users who engage with your brand at a regular cadence will have higher scores. for example, a user that registers behavior every 7 days will have high consistency, and would have the same consistency as a user who registers behavior every 30 days. as users behavior starts to vary sometimes every 7 days, sometimes every 30 the users consistency score will decrease. consistency scores can be paired with propensity scores to build more accurate lookalike models to predict customer churn and target users with winback programs before they bounce. learn more about how to reduce churn using lytics lookalike models. maturity maturity is a normalized measure of how long a user has registered behavior. this indicates how old a customer is relative to your other users. a user who has registered behavior over 5 years will likely have high maturity. a user who registered behavior over 3 years, but hasnt registered any in 2 years will have less maturity. a user who registered behavior over the most recent 3 years will have the same maturity as the user previously mentioned. as an example, you could target users with high maturity scores inviting them into a loyalty rewards program via ads, emails, and inapp notifications. you may also want to consider suppressing engaged users who are likely to make their next purchase without additional advertising. low maturity users, on the other hand, could be served more onboarding or educational content to nurture them into highvalue, longterm users. volatility volatility measures how stable vs. sporadic a user is interacting with your brand. it represents the stability of the volume of data that a user is generating, and serves as a slightly more nuanced version of the intensity score. consider a user where 100 of their daily sessions are considered intense. their intensity score would be 100, but the score doesnt yield any information regarding the volatility of a typical session. utilizing scores each lytics score is accessible as a custom rule in the audience builder. they can be added to any audience definition as an intelligent filter when the size of the audience is larger than desired. for example, when crafting an audience to be used to buy ads against, the size of the audience is critical. the size can be arbitrarily shrunk by taking 10 of the matching users, or it can be intelligently shrunk by creating a threshold with a lytics score such as propensity or momentum. this way, the best fit users remain. interfacing with scores directly can be difficult. they are lowlevel building blocks that require expertise to use to their fullest. to make these scores more readily usable, lytics offers outofthebox behavioral audiences that use scoring underthehood. customizing data sources lytics scores work best on behavioral data that is, data that was generated by a user, like a web view or email open, rather than on list imports or other nonbehavioral data. every supported 3rd party integration is already configured to include only behavioral data in user scores. by default, any custom integration is assumed to be nonbehavioral. if youre using a custom integration that you want to contribute to behavioral scores, contact your account manager to update the setting. since using lytics behavioral scores directly requires a level of understanding of the data science at play, lytics also offers outofthebox behavioral audiences. these audiences are essentially blends of scores that can be used alone or as a rule in a custom audience. for example, casual visitors describes users who come and go without showing much activity per session. the definition using lytics scores is users with an intensity score less than 25. now, instead of needing to know exactly what intensity means in this context, or what the significance of the number 25 is, the audience casual visitors can be used as a building block. users with an intensity score less than 25 in this example audience, the casual visitors characteristic is being used to filter the users who have a high affinity for computing. note that the number of people who have a high affinity for computing is 276,046, but the number of people who have a high affinity for computing and qualify as a casual visitor is 185,747. reducing the size of an audience is a great way to make a campaign more efficient. additionally, splitting an audience based on behavioral properties is a great way to introduce different modes of communication for different archetypes of users. for instance, casual visitors may not stick around long enough to answer their own questions, try engaging them with a slideout on early page visits. the opposite of casual visitors, deeply engaged visitors, are probably determined to find that information on their own and would find a popup to be annoying. splitting the audience keeps both archetypes engaged without accidentally detering anyone. the full set of outofthebox behavioral audiences lytics behavioral audiences the follow outofthebox audiences are prefixed with lytics and can be found in the audiences list. taking it further remember that each of these outofthebox behavioral audiences can be recreated with the audience builder using lytics scores user fields. mastering lytics scores will open up the possibility of new combinations of score thresholds that result in new behavioral audiences to be used as building blocks in campaigns. updated over 1 year ago scraped from: https:docs.lytics.comdocslyticsintegrationoptions lytics integration options quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy if you dont see your ads or marketing tool in our integrations list we offer the following options to connect with other platforms. with our integration options we enable flexibility across the various channels where your data exists and tools to build customized integrations. lytics integration options include: read on for details on each option. if you have questions about any of these options or would like to further explore an integration with a specific platform reach out to your lytics contact. webhooks webhooks offer one of the simplest and most flexible options for enabling applications to share data. theyre broadly used in the cloud computing world for integrations that can be stood up quickly with a minimal amount of code. the lytics webhook integration supports triggered delivery of audience membership enters and exits to a destination url where another application can pick it up. see our webhook documentation for full details. interested to understand how a webhook integration might be applied? look at this mobile messaging use case for a simple lytics webhook integration to support onetoone personalized messaging with twilio. servertoserver file transfer servertoserver file transfer is a very common mechanism for data sharing. lytics file service options enable lytics to pull or push user data from an sftp secure file transfer protocol using a csv commaseparated values or json file. if you have another platform that also enables integrations via sftp, this integration path helps to automate the process of passing data between your system and lytics. check out the lytics file service documentation to learn about setting up the file transfer job. in addition to sftp, amazon s3 is a very commonly used medium for servertoserver file transfer. refer to our amazon s3 documentation for data import and export options. javascript layerjs tag the lytics js tag is the default option for clientside website data sharing with lytics. the js tag supports the delivery of user behavior attributes from your website into lytics and can also return back to the website user profile data such as audience members or content recommendations from lytics for real time personalization. refer to our js tag documentation for full details. mobile sdk the lytics mobile sdk facilitates inapp data sharing with ios and android. similar to the js tag, the mobile sdk supports user behaviorbased data delivery to lytics and profile data to the app for realtime personalization. see our mobile sdk documentation for more details. using your data warehouse if you already use a data warehouse to manage your data this is a great option for data import and export with lytics. lytics integrates with each of these providers offers options for importing data into databases which can then be imported into lytics using cloud connect or our data import integrations. lytics can also export data into each of these data warehouses. updated 10 months ago scraped from: https:docs.lytics.comdocsacoustic acoustic quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy acoustic previously known as silverpop is a marketing platform that enables email, sms, and mobile campaigns, among other marketing analytics and automation solutions. integrating lytics with acoustic allows you to import users and their activity data to build behavioral audiences and gain insights in lytics. you can then export lytics audiences back to acoustic to refine your targeting and deliver personalized messaging. if you have not done so already, you will need to set up an acoustic account before you begin the process described below. your acoustic account user must have permission to create application access via the api. for this integration, it is recommended to create a new acoustic user specific for lytics with the option do not enforce password expiration policies for this user selected so the authentication is long lasting, and can be revoked on an account by account basis. do not enforce password expiration policies for this user import acoustic users and their activity information into lytics so you can leverage that data to build behavioral audiences and gain insights powered by lytics data science. integration details this integration utilizes the acoustic xml api to import acoustic users and their activity to lytics. each run of the job will proceed as follows: silverpopusers silverpopactivity fields the following fields are included in the default mapping for the silverpopusers stream. note this integration was formerly named silverpop, hence the data stream name, but this job will receive your current acoustic data. silverpopusers uniqueid opted out opted out date opt in date opt in details similarly, the following fields are included in the default mapping for the silverpopactivity stream: silverpopactivity unique id configuration follow these steps to set up an import audiences and activity data job for acoustic. send lytics user profiles and audience membership to your acoustic database to refine your targeting and deliver personalized messaging across channels. all existing users and new users of the selected lytics audiences are exported. integration details this integration utilizes the acoustic import list api to export user data from lytics to the acoustic database. once the export is started, the job: fields you can export any lytics user fields to acoustic database columns that are present in the selected acoustic database. lytics allows you to map user fields with the corresponding acoustic database columns as part of the job configuration described below. to export using the acoustic recipient id as the sync field use the acoustic export audience with recipient id export. configuration follow these steps to set up an export audiences job for acoustic. send lytics user profiles and audience membership to your acoustic database to refine your targeting and deliver personalized messaging across channels. all existing users and new users of the selected lytics audiences are exported. this export is specifically used for exporting using the acoustic recipient id as the sync field. it will only update users already in acoustic with a recipient id. in order to add users to acoustic or if you would like to use other sync fields use the standard acoustic export audience export. integration details this integration utilizes the acoustic import list api to export user data from lytics to the acoustic database. once the export is started, the job: fields you can export any lytics user fields to acoustic database columns that are present in the selected acoustic database. lytics allows you to map user fields with the corresponding acoustic database columns as part of the job configuration described below. configuration follow these steps to set up an export audiences job for acoustic. send lytics user profile data and audience membership to your acoustic relational tables. all existing and new users of the selected audience are exported in realtime. integration details this integration utilizes the acoustic relational table management api to export user data from lytics to the acoustic relational table. once the export is started, the job: true false lyticsvalue lyticskey lyticsvalue fields you can export any lytics user fields to acoustic relational table. lytics allows you to map user fields with the corresponding table columns as part of job configuration. the job also has option to create column in acoustic table for lytics user field. configuration follow these steps to set up an export audiences job for acoustic. create new table lyticsusertabletimestamp timestamp yyyymmddhhmmss create new table create new table lyticsaudiencestabletimestamp timestamp yyyymmddhhmmss create new table lyticsfieldnametabletimestamp example below is an example of a user exported and how they may look in acoustic. sample user email: email protected, firstname: james, lastname: mcdermott, channels: web, email , hourly: 17: 1, 21: 1, 23: 2 , segments: all, highlyengaged user table: lyticsusertable20240408220715 audience table: lyticsaudiencestable20240408220715 nonscalar table for channels: lyticschannelstable20240408220715 nonscalar table for hourly: lyticshourlytable20240408220715 updated 26 days ago scraped from: https:docs.lytics.comdocsadobe adobe quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy overview this guide offers an overview of options for leveraging lytics standard integration capabilities to connect with adobes ads products. the decisioning capabilities of lytics can be combined with the personalized activation proficiency of the adobe products: options for sending data from lytics to adobe: options for sending data from adobe to lytics: further details on integrating with adobe products adobe campaign lytics can export file based data to adobe campaign via hourlydaily csv file export to an sftp location. adobe campaign can import csv files from its own secure location. this is more of a limitation on the adobe campaign side regarding limited api access. there are many ways to get data into lytics from adobe analytics. one common method is to use the lytics javascript tag jstag.send function to send evars directly from the data layer into lytics on page load. this method will simplify the onboarding process and minimize the need to do a data transfer from adobe analytics on a regular basis. as part of onboarding, we can help identify which data you would like to pull in for activation and identify the right mechanisms for ingestion. lytics provides ways to ingest this data from scheduled sftp pickups to bulk api imports or transactional apis. our javascript tag will communicate user audience membership to adobe target, which will respond with the appropriate site personalizations. this lytics data will be returned from our platform on page load and the users profile, segmentation information and content recommendations will be placed in the web page data layer. you can pass this information simply by leveraging the lio.data.segments object that is loaded onto every page where the lytics js tag is deployed. similar to other adobe products, adobe ad cloud supports the consumption of user audiences via sftp, json file upload, api lytics can create a webhook to stream data but more conversation with adobe will be needed as well as direct from other adobe products like adobe audience manager. lytics can also directly send audiences from lytics into ad platforms e.g. facebook, google, linkedin, twitter, snapchat, instagram, liveramp. with our google partnership, lytics has early access to googles apibased integration for custom audiences within dv360. we can deliver audiences into audience manager via sftp or data warehouse integration, similar to how the above integrations have been documented. updated 2 months ago scraped from: https:docs.lytics.comdocsadroll adroll quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy connecting lytics and adroll allows you to leverage lytics audiences in your adroll campaigns. when a visitor accesses your site lytics identifies who they are and what audiences they are currently a member of. this information is passed to adroll via the adroll smartpixel, which is tied to the campaigns within your adroll account. as a result, your ads can target audiences based on any aspect of lytics audiences such as behavioral scores or content affinity, in realtime. before you begin first, you need to authorize lytics to use your adroll account. log into your lytics account. open adroll integration or click data integrations and select adroll from the integrations list. click authorizations. click add new authorization. in the admin username box, enter your adroll admin username. in the admin password box, enter your adroll admin password. in the account pixel box, enter your adroll smartpixel id. your id can be found within your smartpixel. access that pixel by following adrolls documentation. once you have the pixel pulled up your id can be found following adrollpixid in your pixel tag. adrollpixid in the description box, enter any description. click authorize to save the authorization. lytics will begin adding audiences to adroll immediately but it may take a few minutes to see all audiences depending on total number. updated almost 2 years ago scraped from: https:docs.lytics.comdocsairship airship quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy airship provides mobile push and locationbased messaging services that enable brands to strengthen relationships with their customers. connect airship to lytics to gain greater insight into your users activities in your app. create lists to target in airship using advanced audience insights and data science from lytics to drive your conversion rates. if you havent already done so, you will need to setup an airship account before you begin the process described below. select airship from the list of providers. select the method for authorization. note that different methods may support different job types. airship supports the following authorization methods: enter a label to identify your authorization. optional enter a description for further context on your authorization. complete the configuration steps needed for your authorization. these steps will vary by method. click save authorization. event stream authorization this authorization is used for the import events job. you will need to create an access token for lytics to use to authorize with airship. you will need to add the direct connection or the lytics integration your airship account to use the event import. after creating an access token and retrieving your app key, follow these steps configure the authorization between lytics and airship. basic master authorization this authorization is used for the import compliance events and the export audiences jobs. by importing your airship events into lytics, youll be able to use lytics powerful insights into the channels your users use the most. integration details this integration utilizes the airship apis to receive event data. on each run of the job, it will: airshipevents fields the following fields are included in the default mapping of the airshipevents stream: airshipevents occurred type device.devicetype occurred type device.devicetype device.devicetype unique id device.devicetype unique id type device.devicetype type device.devicetype type device.devicetype body.name type device.devicetype body.name type device.devicetype body.name type device.devicetype body.name type device.devicetype type device.devicetype body.name type device.devicetype type device.devicetype type device.devicetype body.name type device.devicetype body.name type device.devicetype body.name type device.devicetype type device.devicetype body.name type device.devicetype body.name body.viewedscreen type device.devicetype device.devicetype unique id device.attributes.pushoptin device.devicetype type device.devicetype type type device.devicetype configuration follow these steps to set up and configure an import job for airship in the lytics platform. if you are new to creating jobs in lytics, see the data sources documentation for more information. select airship from the list of providers. select the import activity data job type from the list. select the authorization you would like to use or create a new one. enter a label to identify this job you are creating in lytics. optional enter a description for further context on your job. complete the configuration steps for your job. optional in the maximum event age days text box, enter the maximum age of a message to be imported. if left blank, only new events will be imported. optional using the device types input, select the types of device you would like events from. devices on the right will be included. if no devices are selected, all device types will be imported. optional using the event types input, select the types of events you would like to import. event types on the right will be included. if no event types are selected, all event types will be imported. optional in the push id text box, enter the id of a push message to only import events related to that push. note: you may only include one of group id and push id. optional in the group id text box, enter the id of a group to only import events related to that group. note: you may only include one of group id and push id. click start import. export your lytics audiences to airship to use lyticspowered insights in your mobile campaigns. use your advanced data science driven audiences in lytics as your push targets in airship. integration details this integration utilizes the airship apis to create a static list of users. users without an email channel will be registered and associated with a named user id if available. once the user initiates an export, the job will: note: large audiences may take longer than an hour to export. fields the following fields can be mapped for export as part of the audience export to airship. configuration follow these steps to set up and configure an export job for airship in the lytics platform. select airship from the list of providers. select the export audiences job type from the list. select the authorization you would like to use or create a new one. enter a label to identify this job you are creating in lytics. optional enter a description for further context on your job. required from the audience dropdown, select the lytics audience you want to export to airship. optional from the list dropdown, select an existing static list to export the audience to. either a list must be selected or a list name must be entered below. optional in the list name text box, enter the name of a list to create in airship. either a list must be selected or a list name must be entered. optional from the email field dropdown, select the lytics field that contains email address of the user. this email will be used to register new users in airship. optional from the named user id field dropdown, select a field to use when associating a newly registered email with a named user. if a field is selected, its value will be used as the named user id to associate the registered email with the named user. if left blank registered emails will not be associated with a named user. use the identifier field mapping input to map the lytics user field on the left to the airship channel right. note: the lytics field must contain the users airship channel uuid. optional select the commercial optin field which represents the datetime when the user subscribed to the commercial emails. optional select the commercial optout field which represents the datetime when the user unsubscribed from the commercial emails. optional select the transactional optin field which represents the datetime when the user subscribed to the transactional emails. optional select the transactional optout field which represents the datetime when the user unsubscribed from the transactional emails. optional select the keep updated checkbox to run the export at a configurable frequency. the default is daily at midnight utc. optional from the list update frequency dropdown, select the frequency you would like the export to run. note: large audiences may take more than an hour to export to airship. optional from the time of day dropdown, select the time of day the export should run for daily, weekly, and monthly exports. optional from the timezone dropdown, select the timezone to use for time of day. click start export. updated about 1 year ago scraped from: https:docs.lytics.comdocsamplitude amplitude quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy amplitude is a product analytics service that tracks user events and builds rich crosssectional and longitudinal profiles. it primarily sources users and events from websites and mobile apps. amplitude provides a powerful ui enabling you to create cohorts of users based on event histories. integrating lytics with amplitude provides crosschannel profile resolution, expedited marketing activation, and journey orchestration. lytics can import the cohorts and events stored in amplitude, enabling users to further refine targeting logic. if you havent already done so, you will need to setup an amplitude account before you begin the process described below. follow the instructions on setting up an account and starting a new project. each project has its own unique api key and secret, which you will need to authorize this integration. if you are new to creating authorizations in lytics, see the authorizations documentation for more information. importing cohorts from amplitude results in new andor updates to existing user profiles in your lytics account containing fields from amplitude. once imported, lytics can use this amplitude data to inform its mlbased enrichments, and this data enables you to target your multichannel campaigns orchestrated by lytics. integration details this integration utilizes the amplitude behavioral cohorts api to retrieve data. on each run of the job, it will request a list of cohorts, and for each cohort selected by the user during the configuration step, it will: amplitudecohorts fields the following fields are included in the default mapping of the amplitudecohorts stream: amplitudecohorts unique id unique id configuration follow these steps to set up and configure an import job for amplitude in the lytics platform. if you are new to creating jobs in lytics, see the data sourcesdocumentation for more information. select amplitude from the list of providers. select the import cohorts job type from the list. select the authorization you would like to use or create a new one. enter a label to identify this job you are creating in lytics. optional enter a description for further context on your job. complete the configuration steps for your job. optional from the amplitude id field input, select the field name that contains the users amplitude id. if left blank, cohorts will only be added to profiles. optional from the amplitude cohort names field input, select the field name that contains the users cohort membership. if left blank, cohorts will only be added to profiles. optional from the cohorts input, select select the cohorts you want to import. if left blank, all cohorts will be imported. optional select the keep updated checkbox, to import cohorts daily. optional from the time of day input, select time of day to start import. optional from the timezone input, select timezone for time of day. click start import. importing events from amplitude enables you to refine audience definitions within lytics. beyond just basic event triggers, lytics can apply its mlpowered behavioral scores and content affinities to enhance user profiles. integration details this integration utilizes the amplitude export api to retrieve event data. on each run of the job, it will: amplitudeevents fields the following fields are included in the default mapping from the amplitudeevents stream: amplitudeevents insertid schema unique id unique id additional fields the following fields are not included in the default mapping, reach out to the customer support team to add them to the mappings for your account. configuration follow these steps to set up and configure an import job for amplitude in the lytics platform. if you are new to creating jobs in lytics, see the data sourcesdocumentation for more information. updated about 1 year ago scraped from: https:docs.lytics.comdocsamazonads amazon ads quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy amazon ads is a demandside platform that allows advertisers to programmatically buy display and video ads at scale. connect lytics with amazon ads to leverage the behavioral scoring, content affinities, and insights from lytics to improve your targeting for amazon ads. if you have not already done so, you will need to set up an amazon advertising account before you begin the process described below. authorizing as an amazon advertising user will guide you through an oauth process to authorize access to ad accounts tied to that particular amazon advertising user. if you are new to creating authorizations in lytics, see the authorizations documentation for more information. send lytics user profiles to amazon ads to reach and connect with your customers. integration details this workflow uses the amazon ads api to create an audience in the amazon ads platform and uploads the lytics user profiles for matching. once the job is created, the job will: lyticsaudienceslugcurrenttimestamp fields the export job gives you an option to send following fields to amazon ads: configuration follow these steps to set up and configure an export to amazon dsp from lytics. if you are new to creating jobs in lytics, see the destinations documentation for more information. send lytics user profiles to amazon dsp to reach and connect with your customers. integration details this workflow uses the amazon dsp api to create an audience in the amazon ads platform and uploads the lytics user profiles for matching. once the job is created, the job will: fields configuration follow these steps to set up and configure an export job for amazon dsp in the lytics platform. if you are new to creating jobs in lytics, see the destinations documentation for more information. send lytics user profiles to amazon dsp conversion api to improve your ad campaign performance. integration details this workflow uses the amazon dsp conversion api to send the lytics audiences events. once the job is created, the job will: email phone firstname lastname address city state postal maid or rampid fields you can send lytics user fields to amazon capi api. the job constructs the payload according to the configuration. but below is the sample of payload: eventdata: conversiondefinitionid: 123456789, countrycode: us, matchkeys: type: state, values: 7175517a370b5cd2e664e3fd29c4ea9db5ce17058eb9772fe090a5485e49dad6 , type: city, values: d8e3ee0e3af6b79059d69fab6d0a5567930fb10cc91f61b16c0ec58412770fea , type: email, values: 979e0b2b1c9400d6ed429e008ae41c1d6662fc0310e6ebac3e59a73da141cd7a , type: firstname, values: 4906fbbadc0e9ef34db9e378da4086c17ddbc0ed76eacfc72870c0b99085e753 , type: lastname, values: 5014438bc08e5a2261c9ab3b5c5b3876d8a9cd67830cdba61862c76e0f0d64cc , type: postal, values: 1da798654fe99888c85d2a6d730909e10bebcf5d640d8d19d27545cf9e4a1d4c , type: address, values: 051098eb5e73408f7583204c0ccaef7c885f89c271317cd919bee65980a9a84b , name: lytics audiences, timestamp: 20240805t11:42:0907:00 , source: servertoserver configuration follow these steps to set up and configure a amazon dsp capi export job in the lytics platform. if you are new to creating jobs in lytics, see the destinations documentation for more information. the amazon marketing cloud export uploads an audience of users to an amazon marketing cloud dataset. this dataset can then generate insight reports based on your lytics audience. authorization in addition to the amazon ads authorization, to use the amazon marketing cloud: advertiser data upload workflow, you must delegate access to lytics to your amazon marketing cloud instance s3 bucket. follow these directions to add the delegated authorization: docs. integration details stepspattern: the export will only send users currently in the audience. as profiles fall out of the audience in lytics they will be removed from the audience being exported to amazon. fields the identifier fields listed below in the table will match the profiles of amazon users. you must select at least one identifier. the identifiers will be normalized based on the chosen country and hashed before being sent to amazon. the listed lytics user fields are the default fields in lytics. your account may use different fields for storing identifiers. identifiers: configuration follow these steps to set up and configure an export job for amazon marketing cloud amc in the lytics platform. if you are new to creating jobs in lytics, see the destinations documentation for more information. if using the address, phone, or postal code identifiers, the audience should contain users from only one country. if you want to export an audience containing profiles from multiple countries, you can create audiences for each country based on the full audience. you can then export multiple audiences to the same amc dataset. updated 7 months ago scraped from: https:docs.lytics.comdocsawskinesisoverview amazon kinesis quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy amazon kinesis data streams is a realtime data streaming service provided by amazon. integrating lytics with aws kinesis allows you to bring in data from kinesis data streams and leverage lytics to enrich user profiles and build behavioral audiences in lytics. you can then send lytics audience events as triggers back to your kinesis data streams. if you have not already done so, you will need to set up an aws kinesis account before you begin the process described below. you can authorize the integration in one of two ways: providing your aws keys follow the steps below to authorize aws with lytics using your aws keys. for more information on obtaining your keys, see amazons documentation on secret and access keys. lytics delegated authorization for export below is a set of instructions for how to set up delegated authorization. this method of doing authorization is more complicated than the aws keys method but some people prefer it: here are related amazon reference documents: stream name: the name of the kinesis stream that you will be writing to. it can be any alphanumeric string plus underscores, periods, or dashes. this example shows lyticstriggersstream. lyticstriggersstream 1 setup aws cli permissions, change name of myawsaccount assumes you have setup the aws cli https:aws.amazon.comcli aws configure profilemyawsaccount .... 2 ensure we have a kinesis stream, create stream if need be aws kinesis liststreams profilemyawsaccount aws kinesis describestream streamname lyticstriggersstream profilemyawsaccount aws kinesis createstream streamname lyticstriggersstream shardcount1 profilemyawsaccount cleanup if needed aws kinesis deletestream streamname lyticstriggersstream profilemyawsaccount 3 replace your aws accountid below wherever you see 111111111 3a create a role in nonlytics account that allows a lytics user to assume identitypermission inside this account in order to write to kinesis. aws iam listroles profilemyawsaccount pathprefixlytics aws iam createrole profilemyawsaccount rolenamelyticswritetokinesis pathlyticswritetokinesis assumerolepolicydocument version: 20121017, statement: effect: allow, principal: aws: arn:aws:iam::358991168639:usergce1 , action: sts:assumerole 3b nonlytics account: create a policy allowing access to a specific kinesis stream aws iam listpolicies profilemyawsaccount scopelocal aws iam getpolicy profilemyawsaccount policyarnarn:aws:iam::111111111:policykinesisassumewrites aws iam createpolicy profilemyawsaccount policyname kinesisassumewrites policydocument version: 20121017, statement: effect: allow, action: kinesis:putrecord, kinesis:putrecords , resource: arn:aws:kinesis:uswest2:111111111:streamlyticstriggersstream 3c attach that new policy to previously created role aws iam attachrolepolicy profilemyawsaccount rolename lyticswritetokinesis policyarn arn:aws:iam::111111111:policykinesisassumewrites 3d view that role policy aws iam getrolepolicy profilemyawsaccount rolename lyticswritetokinesis policyname kinesisassumewrites cleanup kinesis streams cost when idle, so cleanup if test aws kinesis deletestream streamname lyticstriggersstream profilemyawsaccount detach rolepolicy aws iam detachrolepolicy profilemyawsaccount rolename lyticswritetokinesis policyarn arn:aws:iam::111111111:policykinesisassumewrites if you need to delete aws iam listrolepolicies profilemyawsaccount rolenamelyticswritetokinesis aws iam deleterolepolicy profilemyawsaccount rolenamelyticswritetokinesis policynamesimulateinboundlytics aws iam deleterole profilemyawsaccount rolename lyticswritetokinesis aws iam listpolicies profilemyawsaccount scopelocal aws iam getpolicy profilemyawsaccount policyarnarn:aws:iam::111111111:policykinesisassumewrites aws iam deletepolicy profilemyawsaccount policyarnarn:aws:iam::111111111:policykinesisassumewrites aws iam deleterole profilemyawsaccount rolename lyticswritetokinesis contact lytics to grant permission lytics will need the role amazon resource name arn from your aws account to grant permission to. contact lytics support with the arn. it will look like this example if you followed the instructions above. arn:aws:iam::111111111:rolelyticswritetokinesislyticswritetokinesis import data from your aws kinesis data streams to use lytics data science scoring and insights to build rich, behavioral audiences. integration details fields any fields that are present in the event data in your aws kinesis stream are posted in your lytics stream that is configured during the job setup. you can map them to lytics user profiles using custom lql. please contact lytics support to find out more about this option. configuration follow these steps to set up and configure an aws kinesis import job in the lytics platform. if you are new to creating authorizations in lytics, see the authorizations documentation for more information. send lytics audience event triggers to your aws kinesis data streams to trigger a message when a user enters or exits a lytics audience. integration details fields the fields included depend on the raw event in lytics. all user fields will be included in the data published to your aws kinesis data stream unless specified in the export fields selection. configuration follow these steps to set up and configure the aws kinesis trigger job in lytics platform. updated 10 months ago scraped from: https:docs.lytics.comdocsamazonpinpointoverview amazon pinpoint quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy amazon pinpoint is an aws service that you can use to engage with your customers across multiple messaging channels. you can use amazon pinpoint to send push notifications, emails, sms text messages, and voice messages. integrating lytics with amazon pinpoint allows you to import email, sms, and push activity from amazon pinpoint and then export lytics crosschannel, behavioraldriven audiences to deliver personalized marketing campaigns. if you have not already done so, you will need to set up an amazon pinpoint account before you begin the process described below. for more information on obtaining your aws keys for authorization, see amazons documentation on secret and access keys. if you are new to creating authorizations in lytics, see the authorizations documentation for more information. import email, push, or mobile activity from amazon pinpoint to add behavior data to your user profiles in lytics. integration details importing pinpoint activity into lytics requires setting up an export of pinpoint events to kinesis. see amazons documentation for more information, and follow these instructions to set it up via the pinpoint ui. this integration uses the amazon kinesis data streams service apis to get pinpoint activity records from the kinesis stream in near realtime. you can then start the pinpoint import job in lytics. pinpointactivity fields the following fields are included in the default mapping of the pinpointactivity stream: pinpointactivity endpoint.status endpoint.id unique id configuration follow these steps to set up and configure an import of pinpoint activity in the lytics platform. if you are new to creating jobs in lytics, see the data sourcesdocumentation for more information. useast1 if activity data is avaiable in the kinesis pinpoint stream, the data will appear in lytics within a few minutes. export lytics audiences to add audience membership data to your pinpoint endpoints, and send targeted notifications using pinpoint as users enter or exit lytics audiences. integration details this integration utilizes the pinpoint rest apis to send pinpoint events that set an attribute for a pinpoint endpoint as a user enters or exits an audience in lytics. lyticsaudience lyticshighlyengaged true false lytics audience true fields by default, lytics exports the following fields to pinpoint. configuration follow these steps to set up and configure an export of a lytics audience to pinpoint. useast2 if the existing users option is selected, audience data should be in pinpoint within minutes. otherwise, data will be seen when a new user enters or exits the exported audience. in pinpoint, segments can be created to target the lytics audience by defining a segment with the exported lytics segment slug name set to true. you can use lytics to send messages to your customers via various aws pinpoint channels emailsmsgcmapns when they enter a lytics audience. integration details the job uses the pinpoint api to send the email, sms, apns and gcm messages. before you can set up this job, you must enable the desired channel in your aws pinpoint project via which you would like to send a notification message. once started, this job will proceed as follows. user fields user fields fields as mentioned above, the notification message can be configured with user fields so that it can be populated with corresponding user field values and the resulting message will be sent to the user using your chosen aws pinpoint channels. user fields please refer to the templating section for more information on configuring messages with the desired user fields. configuration follow these steps to set up and configure an export from lytics to amazon pinpoint. select amazon aws services from the list of providers. select the export audiences triggers job type from the list. select the authorization you would like to use or create a new one. enter a label to identify this job you are creating in lytics. optional enter a description for further context on your job. from the audience input, select the lytics audiences to send the notification message. optional select the existing users checkbox to send the message to users who are already in the selected lytics audiences. using the region dropdown, select the aws region that your pinpoint project resides in. using the pinpoint project dropdown, select the aws pinpoint project you would like to use. using the pinpoint channel type dropdown, select the pinpoint channel type to send the message. from the endpoint address field dropdown, select the lytics user field that contains the address that uniquely identifies the device or email to send messages to. required except for sms in the title field, enter the title to display above the notification message on the recipients device. if you are sending an email notification, this would be the subject for the email. in the body field, enter the body of the notification messageemailsms. click show advanced options to set additional configuration option, all of which are optional: for email channel only in the from email address field, enter the verified email address to send the email message from. the default value is the fromaddress specified in pinpoint for the email channel. for email channel only in the reply to address field, enter the replyto email addresses for the email message. if a recipient replies to the email, each replyto address receives the reply. for multiple email addresses, please separate them using a comma. for applegcm channel using the action dropdown, select the action to occur if the recipient taps the notification message. please refer to pinpoint documentation on action for more information. for applegcm channel in the url field, enter the url to open in the recipients default mobile browser, if a recipient taps the notification message and the value of the action property in above input is url. for applegcm channel using the priority dropdown, select the priority for the message you are sending. for applegcm channel check the silent push checkbox to send the message as a silent notification message . for applegcm channel in the sound field, enter the name of the sound file in your apps main bundle or the librarysounds folder in your apps data container. if the sound file cannot be found or you specify default for the value, the system plays the default alert sound. for sms channel only in the keyword field, enter the sms program name that you provided to aws support when you requested your dedicated number. for sms channel only in the origination number field, enter the number to send the sms message from. the phone number or short code that you specify has to be associated with your amazon pinpoint account. using the message type dropdown, select the sms message type. if you plan to send timesensitive content, specify transactional. if you plan to send marketingrelated content, specify promotional. please refer to pinpoint documentation on message type for more information. for sms channel only in the sender id field, enter the sender id to display as the sender of the message on a recipients device. click start export. updated about 1 year ago scraped from: https:docs.lytics.comdocsamazonredshiftoverview amazon redshift quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy amazon redshift is a cloud data warehouse service provided by amazon. with redshift, you can query and combine structured and semistructured data across your data warehouse, operational database, and data lake using standard sql. redshift lets you easily save the results of your queries back to s3. connect lytics to your redshift database to leverage lytics data science and audience building capabilities on your data. if you havent already done so, you will need to set up an amazon web services aws account before you begin the process described below. see the redshift getting started guide to get started with amazon redshift. you will also have to add lytics ips to your clusters ingress rules. contact your account manager for the current list of lytics ip addresses. there are two redshift authorization types: redshift only to set up aws redshift user you will need the following credentials: database username and password. see the redshift users guide for instructions on how to create a user in amazon redshift. if you are new to creating authorizations in lytics, see the authorizations documentation for more information. select amazon web services aws from the list of providers. select the aws redshift user method for authorization. enter a label to identify your authorization. optional enter a description for further context on your authorization. in the username text box, enter your redshift cluster admin user name. in the password password box, enter your redshift cluster admin password. optional in the db url text box, enter your redshift database endpoint. follow these instructions to obtain your db url; copy the endpoint from the general information page the area as the jdbc url in the amazon instructions. this will be of the form: redshiftclustername.vcpcluster.region.redshift.amazonaws.com:portdbname. if left blank the import will need to have the db url set in the job configuration. redshiftclustername.vcpcluster.region.redshift.amazonaws.com:portdbname optional from the ssl mode input, select your ssl mode credential. leave blank to disable ssl verification. click save authorization. redshift and s3 to setup the aws s3 keys and redshift user you will need the following credentials: see the redshift users guide for instructions on how to create a user in amazon redshift. if you are new to creating authorizations in lytics, see the authorizations documentation for more information. redshiftclustername.vcpcluster.region.redshift.amazonaws.com:portdbname the redshift export lets you create and keep updated an aws redshift table based on a lytics audience. with your audience in redshift you can use your already existing analytics tools to gain new insight into your users. integration details stepspattern: the redshift export is a two step export, it first exports users to amazon s3, then executes a copy operation to load those files to a table in redshift. the tables name is based on the lytics audiences slug name. the files are setup to expire after 72 hours, so no manual cleanup is needed. executing the copy command to a temporary table allows the main table to continue to be used while data is loading; only the brief time when the main table is removed and the temporary table is renamed will the data be unavailable. the temporary table is a full table and not an actual temporary table in redshift. fields by default, lytics exports all user fields to redshift. you may configure the fields to export by selecting them in the fields to export input. fields in redshift will match their lytics field names. the type depends on the field type in lytics: configuration follow these steps to set up and configure an export job for amazon web services aws redshift in the lytics platform. select amazon web services aws from the list of providers. select the export job type from the list. select the authorization you would like to use or create a new one. enter a label to identify this job you are creating in lytics. optional enter a description for further context on your job. select the audience to export. complete the configuration steps for your job. optional from the region input, select aws region that s3 resides in useast2, etc.. optional in the s3 bucket name text box, enter select or enter the bucket that you would like to save your files in. in the directory text box, select or enter the directory that you would like to save your files in. the directory suggestions can be slow on large s3 buckets. these files will expire after 72 hours. optional from the fields to export input, select a list of user fields to export. optional in the user limit numeric field, enter the maximum number of users to export. leave blank to export all users. optional select the keep updated checkbox, to select to run this export continuously. optional from the file export frequency input, select how often a continuous export should update the table. default is daily. optional from the time of day input, select the time of day to start the update. this only affects daily and slower frequency exports. exact time of the update will vary based on load and audience size. optional from the timezone input, select the timezone to use for the time of day. click the start job button to start the job import user data directly from your aws redshift database into lytics, resulting in new user profiles or updates to fields on existing profiles. integration details this integration connects to your redshift table through a postgresql connection and then completes the following steps: fields because of the nature of redshift data, there is no default mapping defined. by default the data will be added to the redshifttablename data stream, where tablename is the name of the redshift table imported. redshifttablename tablename configuration follow these steps to set up and configure an import redshift table job for amazon web services aws in the lytics platform. if you are new to creating jobs in lytics, see the data sourcesdocumentation for more information. redshiftclustername.vcpcluster.region.redshift.amazonaws.com:portdbname yyyymmdd hh:mm:ss redshifttable updated over 1 year ago scraped from: https:docs.lytics.comdocsawss3overview amazon s3 quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy the amazon simple storage service amazon s3 is a flexible object storage service optimized for scalability and security. common use cases for s3 include storage for web applications, backup and recovery, disaster recovery, data lakes, and hybrid cloud storage. integrating lytics with aws s3 provides the ability to import or export files to your s3 buckets. this makes your data available to the amazon ecosystem for reporting, analytics and archiving. many tools provide the ability to import and export tofrom s3; therefore, by linking lytics and s3 you gain access to the wide ecosystem of tools that integrate with s3. if you have not already done so, you will need to set up an amazon s3 account before you begin the process described below. if you are using an ip allowlist for your aws s3 account, contact your account administrator to add lytics ip addresses to your allowlist. reach out to your lytics account manager for the current list of ip addresses. you may authorize in one of two ways: providing your aws keys follow the steps below to authorize aws with lytics using your aws keys. for more information on obtaining your keys, see amazons documentation on secret and access keys. if you are new to creating authorizations in lytics, see the authorizations documentation for more information. aws keys with pgp encryption to create an authorization with aws keys and pgp encryption, follow the steps as described above, and then select either the private or public pgp keys option. note that currently the ed25519 method of encryption is not supported delegating access via aws iam you can also authorize using aws identity and access management iam. for more information see amazons documentation on iam. you will need to enter the following policy in your s3 bucket: statement: sid: grant lytics access, effect: allow, principal: aws: arn:aws:iam::358991168639:root , action: s3:getbucketlocation, s3:listbucket, s3:putobject, s3:getobject, s3:deleteobject , resource: arn:aws:s3:::yourbucket, arn:aws:s3:::yourbucket before you start a job, please let lytics support know the name of the bucket you are going to use. this will allow delegated access to the given bucket. when you start the job, just select use delegated access to lytics with aws iam instead of an aws keys authorization. delegated access with pgp encryption to create an authorization with delegated aws access and pgp encryption, follow the steps as described above to set up delegated access, and then select either the private or public pgp keys option. import audiences and activity data into lytics via a csv file directly from your aws s3 bucket. integration details this integration uses the amazon s3 api to read the csv file selected for the import. each run of the job will proceed as follows: please see custom data ingestion for more information on file naming, field formatting, headers, timestamps, etc. fields once you choose the csv file to import from your s3 bucket during configuration, lytics will read the file and list all the fields that can be imported. you can then select the fields that you want to import. configuration follow these steps to set up and configure an aws s3 csv import job in lytics. if you are new to creating jobs in lytics, see the data sourcesdocumentation for more information. , t many applications let you write json files to amazon s3, you can easily import this custom data to lytics. once imported you can leverage powerful insights on this custom data provided by lytics data science to drive your marketing efforts. integration details this integration uses the amazon s3 api to read the json file selected. each run of the job will proceed as follows: fields fields imported via json through s3 will require custom data mapping. for assistance mapping your custom data to lytics user fields, please reach out to lytics support. configuration follow these steps to set up and configure the s3 json import job in the lytics platform. if you are new to creating jobs in lytics, see the data sourcesdocumentation for more information. note: for continuous imports, files should be in the following format: prefixtimestamp.json. the workflow will understand the sequence of files based on the timestamp. if no next file is received, the continuous import will stop and a new export will need to be configured. prefixtimestamp.json a wide ecosystem of tools can read json or csv files from amazon s3. this export allows you to bring lytics user profiles based on your identity resolution strategy into other tools or to simply store as an archive of user data. integration details this integration uses the amazon s3 api to upload data to s3. each run of the job will proceed as follows: fields the fields exported to the s3 file will depend on the fields to export option in the job configuration any user field in your lytics account may be available for export. configuration follow these steps to set up and configure an export of user profiles to s3 in the lytics platform. join key join key timestamp lyticseventstimestamp export events into s3 so you can access, archive, or run analysis on lytics events in the aws s3 ecosystem. integration details this integration uses the amazon s3 api to read the csv file selected. each run of the job will proceed as follows: fields the fields included depend on the raw event in lytics data. all fields in the selected stream will be included in the exported csv. configuration follow these steps to set up and configure an export of event data to aws s3 in the lytics platform. timestamp lyticseventstimestamp updated about 1 month ago scraped from: https:docs.lytics.comdocsawssqsoverview amazon sqs quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy export lytics trigger events to aws sqs. a lytics trigger is an event that is raised and available for export when a user enters or leaves a segment for example user x just entered atrisktochurn. aws sqs can be used for a variety of use cases such as: if you havent already done so, you will need to set up an aws sqs account before you begin the process described below. if you are new to creating authorizations in lytics, see the authorizations documentation for more information. export triggers to aws sqs when users enter or exit selected audiences. integration details fields configuration follow these steps to set up an importexportenrichment job for provider name. https:useast2.queue.amazonaws.com111111111testlyticssqs message format for subscription events when a user profile is updated, it may be due to: when the user gets updated, segment membership is reevaluated and segments a user has moved into or out of triggers a message. here is an example of the message that is produced. data: created: 20160629t18:50:16.902758229z, modified: 20170318t06:12:36.829070108z, email: email protected, userid: user123, segmentevents: id: d3d8f15855b6b067709577342fe72db9, event: exit, enter: 20170302t06:12:36.829070108z, exit: 20170318t06:12:36.829070108z, slug: demosegment , id: abc678asdf, event: enter, enter: 20170302t06:12:36.829070108z, exit: 20990318t06:12:36.829070108z, slug: anothersegment , meta: object:user, subscriptionid: 7e2b8804bbe162cd3f9c0c5991bf3078, timestamp: 20170318t06:12:36.829070108z updated over 1 year ago scraped from: https:docs.lytics.comdocsansira ansira quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy this integration facilitates the enrichment of lytics user profiles with valuable information, including identifiers, activity data, and more from ansira. enabling the ansira and lytics integration empowers customers to enhance lytics user profiles by incorporating userlevel data from the ansira platform. this integration facilitates the enrichment of lytics user profiles with valuable information, including identifiers, activity data, and more. by seamlessly connecting ansiras extensive data resources with the lytics user profiling capabilities, customers gain a comprehensive understanding of their users preferences, behaviors, and interactions. this enables more personalized and targeted marketing strategies, fostering deeper customer engagement and driving better business outcomes. to successfully send and receive requests to the ansira apis, we must create an oauthbased authorization using the key and secret outlined below. select provider this integration leverages our webhook integration to connect with and retrieve data from ansiras endpoints. begin by selecting the webhooks provider tile. select method next, select an authorization method. if this is your first time creating an authorization for ansira, select oauth 2.0 client credentials grant. configure authorization next, well configure the authorization to use a key and secret provided by ansira. please configure all fields as outlined below and leave any additional fields blank unless you fully understand the implications. label create a label for your authorization. this will only be surfaced in the lytics ui for easy navigation when selecting the authorization in additional workflows or reviewing the details and logs. description add an optional description to provide context for where the auth came from and how it will be used. token url ansira will provide the token url. this is the endpoint used to get a token. in most cases, your specific instance information will replace the xxxx in the example. client id enter the provided client id from ansira. client secret enter the provided client secret from ansira. additional request parameters lastly, since the ansira api expects an access token as part of the request, we must configure the additional parameters to include the token generated via oauth, as described below. accesstokentoken integration details this integration utilizes the lytics enrichment webhook to send a secure request to ansira when a user enters a source audience. this request is sent to the postserviceapiv2user endpoint and contains user data configured within the template described below to enrich a lytics profile with extended information about that user within ansira. well outline the following steps to achieve enable this enrichment workflow: postserviceapiv2user v2user authorization select or configure a new authorization as outlined above. audience this integration works by ensuring every member of an audience has been enriched with ansira data. in most cases, the desired outcome is matching a uuid or ansiraspecific identifier with an email address. though the source audience is ultimately up to each customers discretion, we recommend keeping the following in mind: create webhook template webhook templates are created using jsonnet. these templates provide a flexible way to reformat the outbound payload of a webhook request to meet the recipients requirements. in this case, well create a template that provides the proper formatting of email and source ids for the ansirauser api. this step may appear to be technical. as such, if you need assistance, please get in touch with your technical account manager or primary point of contact. user echo local event import lytemplates.libsonnet; email: event.getemail, , sourcecode: keyname: xxxxxxxxxxxxx http v post https:api.lytics.iov2template nameansiratemp typejsonnet accountidyouraccountid keyyourapitoken jsonnet is highly customizable. in the example above, we pull the email from the profile and hard code a sourcecode keyname if the desire is for the keyname to be dynamic, this could also leverage the.event.get method to pull the value from each profile. if you use the dynamic method, the field name in the event.get function is the field name that has been defined in the lytics schema, as seen in the example below: event.get echo local event import lytemplates.libsonnet; fieldnametosendtoansira: event.getlyticsfieldname, , http v post https:api.lytics.iov2template nameansiratemp typejsonnet accountidyouraccountid keyyourapitoken schema to handle the response from the enrichment apis, youll need to add the necessary fields and mappings within conductor to process the data correctly. in our example, we will only be mapping the email back as an identifier to ensure data is correctly stitched to the profile, along with a new field called ansira uuid, which will hold the uuid for each user. when altering schema it is always best to consult with your internal technical resources or those available to you from lytics to ensure everything is configured adequately. create the new ansira uuid field create new mapping for the ansira uuid field create new mapping for the existing email field remembering what stream you decide to use for the mappings is important, as it will be needed when configuring the enrichment workflow. configure enrichment webhook in the final step, we will bring it all together by configuring our webhook destination to use the auth, template, and audience to enrich a lytics profile. select provider this integration leverages our webhook integration to connect with and retrieve data from ansiras endpoints. begin by selecting the webhooks provider tile. select job type next, select the webhook user enrichment job type. this webhook job will pass the response of each request to the configured data stream to be mapped back to a users profile. configuration select one or more audiences to enrich with ansira data. we highly recommend following the suggested approach in the audience of this doc to prevent invalid or unnecessary calls to the ansira api. define the stream youd like the response data passed to. this stream must align with your updated schema and mappings as outlined above. select the template that was created in the previous step. enter the full v2users endpoint provided by ansira. v2users select profile fields to export. at a minimum, you must select all fields necessary to fulfill your jsonnet template. in our example, this would only be email. email finally, determine if youd like to send all existing audience members to the enrichment endpoint. selecting this box may send many api requests immediately upon saving your configuration. updated over 1 year ago scraped from: https:docs.lytics.comdocsbigcommerce bigcommerce quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy bigcommerce is an ecommerce platform that allows for online store creation, hosting, marketing, and security for small and mediumsized businesses. integrating lytics with bigcommerce allows you to import customer, order, and product data into lytics so that you can run segmented marketing campaigns and offer relevant recommendations for your bigcommerce customers. if you havent already done so, you will need to set up a bigcommerce account before you begin the process described below. to generate a long term access token, follow the obtaining store api credentials instructions on bigcommerce. the minimum scopes required are customers and orders. if you are new to creating authorizations in lytics, see the authorizations documentation for more information. importing your bigcommerce customers and their order activity data into lytics enables you to run personalized marketing campaigns for your bigcommerce customers. integration details this integration utilizes the bigcommerce api to import user and activity data. once the import is started, the job will: bicommerceusers bigcommerceorders fields the following fields are included in the default mapping of the bigcommerceusers stream: bigcommerceusers unique id unique id addresses.address1 addresses.address2 addresses.addresstype addresses.city addresses.countrycode addresses.country addresses.id addresses.postalcode addresses.stateorprovince attributes.attributeid attributes.attributevalue attributes.datecreated attributes.datemodified formfields.name formfields.value the following fields are included in the default mapping of the bigcommerceorders stream: bigcommerceorders unique id configuration follow these steps to set up and configure an import job for bigcommerce in the lytics platform. if you are new to creating jobs in lytics, see the data sourcesdocumentation for more information. yyyymmdd 20210125 sync lytics audiences with bigcommerce to identify and target the right users to improve the performance of your store campaigns. integration details this integration utilizes the bigcommerce api to export lytics users. once the export is started, the job will: lyticsaudiencejobid fields by default, lytics exports the following fields to bigcommerce: configuration follow these steps to set up and configure an export job for bigcommerce in the lytics platform. select bigcommerce from the list of providers. select the export job type from the list. select the authorization you would like to use or create a new one. enter a label to identify this job you are creating in lytics. optional enter a description for further context on your job. select the audience to export. complete the configuration steps for your job. from the id field input, select the field that contains the users bigcommerce id field. optional from the audience attribute field input, select the attribute field that should contain the lytics audience name. if none is selected, one with the name lyticsaudiencejobid will be created for your account. optional from the map standard fields input, map standard fields from lytics to bigcommerce by selecting the lytics field on the left, and its bigcommerce destination on the right. optional from the map custom attributes input, map custom attributes from lytics to bigcommerce by selecting the lytics field on the left, and its bigcommerce destination on the right. optional select the existing users checkbox to send users who already exist in the selected lytics audience. click the start job button to start the job updated about 1 year ago scraped from: https:docs.lytics.comdocsbluekai bluekai quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy bluekai is a data management platform owned by oracle that enables marketing personalization through audience targeting, optimizing media buys, and improving campaign performance. integrating lytics with bluekai enables better ad targeting by exporting lytics audiences enriched with behavioral scores, content affinities, and datadriven insights. if you havent already done so, you will need to setup a bluekai account before you begin this authorization process. following are the two ways that allows you to export your file on an sftp server before importing into bluekai. if you are new to creating authorizations in lytics, see the authorizations documentation for more information. bluekai sftp server with username and password for this authorization method, you will need the following credentials: sftp host name or ip address, port number, username and password for the sftp server. you may also want to have your partner name the name that bluekai recognizes your account with, and the site id of your bluekai container. if you have a specific folder you want data loaded into in bluekai, you should have the full path to it from the root of your sftp login. bluekai sftp server with username and private key for this authorization method, you will need the following credentials: sftp host name or ip address, port number, username and private key for the sftp server. you may also want to have your partner name the name that bluekai recognizes your account with, and the site id of your bluekai container. if you have a specific folder you want data loaded into in bluekai, you should have the full path to it from the root of your sftp login. exporting user profiles and audience membership to bluekai allows you to use behavioral data, content affinities, and insights from lytics for better ad targeting. export your audiences to sftp to stage your files in a compressed, tabseparated value tsv format before importing into bluekai. integration details this workflow uploads two files in the selected sftp server: offline file and trigger file. in short, the offline file contains all the exported data about the user, and the trigger file contains meta information about the offline file. we follow bluekais documentation in creating the offline file. data is exported in tsv format with two columns and no header row. each line in the offline file represents a unique user. the first column contains the identifier for the user whereas the second column contains the user profile fields that are separated by a pipe . an example of an entry in exported file is given below: someuniqueid field1valuefield2valuefield3value someuniqueid field1valuefield2valuefield3value the offline file follows the partnersiteidyyyymmdd.gz naming conventions. lytics collects partner name and site id during the authorization step. partnersiteidyyyymmdd.gz the trigger file contains the information about the exported offline file such as file name, size in bytes and md5 checksum. this file is used for verification purposes so that all the user data is transferred without any corruption. the trigger file is created and uploaded only after the offline file is uploaded. the trigger file follows the partnersiteidyyyymmdd.gz.trigger naming convention. partnersiteidyyyymmdd.gz.trigger once the job is started, it will: fields fields that are exported for this integration are entirely defined by the fields to export step in the configuration. configuration follow these steps to set up and configure an export job for bluekai in the lytics platform. select bluekai from the list of providers. select the export audiences job type from the list. select the authorization you would like to use or create a new one. enter a label to identify this job you are creating in lytics. optional enter a description for further context on your job. from the audiences input, select the audiences that contains the users to export. optional from the identifier input, select the field that will be placed in the first column of the file. this field should be a unique identifier and be selected in the fields to export. optional from the audience membership input, select the audiences to include in the audience membership. if left blank, all audiences will be included. optional select the append segment name checkbox to add a field with the audience name. optional select the additional segments checkbox to add a column containing all the audiences a user is a member of. optional from the fields to export input, select the user fields to export. optional from the custom join input, enter a custom join character for fields that have multiple values. the default is . e.g. email protectedemail protected. email protectedemail protected optional select the continuous export checkbox to export the audience repeatedly. optional from the file export frequency input, select how often a continuous export should create a new file. optional from the time of day input, select the time of day to start the export each day. this applies to daily, weekly, and monthly frequencies only. optional from the timezone input, select the timezone for the time of day. click start export. updated almost 2 years ago scraped from: https:docs.lytics.comdocsblueshift blueshift quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy blueshift is the customer engagement platform that allows you to execute crosschannel campaigns such as email, sms, push notifications, and cloud app notifications. blueshift offers endtoend campaign reporting, segmentation, and predictive scoring. you can export your lytics user data to blueshift for use in email or mobile campaigns. using lytics advanced audience builder and data science insights, you can make send intelligent audiences downstream to target with blueshift campaigns. using lytics experiences on the journey canvas you can optimize the time that users receive the content and personalize of an experience to be executed with campaign tools such as blueshift. if you havent already done so, you will need to setup a blueshift account before you begin the process described below. in order to authorize blueshift workflow, you will be required to provide the user api key which lytics uses to send data to your account. to obtain blueshift user api key please refer to the blueshift documentation. if you are new to creating authorizations in lytics, see the authorizations documentation for more information. exporting lytics audiences to blueshift will allow you to supplement your blueshift user base with new users or update existing users with crosschannel data from lytics. integration details this integration utilizes blueshift api to send user data. once the user initiates the workflow, it will run a backfill of users if configured to do so. then the workflow will receive realtime updates when a user enters or exits the audience. for each user to export, regardless if the user is being added as part of the backfill, or theyre enteringexiting the audience in realtime, it will: lyticsaudiencename fields blueshift supports fields listed in their user documentation. lytics allows you to map user profile fields with the corresponding blueshift field as part of the workflow configuration. configuration follow these steps to set up and configure an export job for blueshift in the lytics platform. updated almost 2 years ago scraped from: https:docs.lytics.comdocsbraze braze quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy braze is a customer engagement platform that delivers messaging experiences across push, email, inapp. integrating lytics to braze helps you import email, sms, and push activity from braze and then export lytics crosschannel, behavioraldriven audiences to build and send personalized marketing campaigns. if you havent already done so, you will need to set up a braze account before you begin the process described below. the braze credentials necessary to integrate with lytics are rest api key and braze instance. follow these steps to get your braze credentials: create braze rest api key. when creating the braze rest api key, you must grant permissions for each app group you would like to use with lytics. if you are new to creating authorizations in lytics, see the authorizations documentation for more information. connect braze to lytics to import email, sms, and push activity to enrich lytics user profiles. integration details fields the following fields are included in the default mapping of the brazeusers stream: brazeusers unique id unique id unique id configuration braze has three ways to export data to other services: via brazes dashboard, you can export a csv with up to 500,000 rows. to export a segment with over 500,000 users, youll need to use the export api, which places no limit on how much data you can export. you can import csv files to lytics via alytics file service. export api allows large dataset to be exported to s3 buckets, which requires your s3 credentials to be added to braze. more information can be found at users by segment endpoint. this can then be imported into lytics via amazon s3 import. webhooks triggered by events in braze can be sent to lytics representing realtime events within campaigns across multiple channels as email, push notification, and more. see brazes documentation for creating a webhook. https:api.lytics.iocbrazeusers?key the webhook template is expected to include the following fields: city country firstname gender language lastname dateofbirth phonenumber timezone twitterhandle email brazeid if other fields need to be included, please speak to your lytics account manager. export crosschannel, behavioraldriven audiences from lytics to braze to power personalized marketing campaigns. integration details lyticssegments this integration utilizes the braze user track api to attach lytics audience membership data to braze as a user attribute. once the export is started the job will: fields one of the following following fields must be sent to braze as an identifier: configuration follow these steps to set up and configure an export job for braze in the lytics platform. at least one of external user id, braze id, email, phone, or the combination of user alias label and user alias field must be set in the configuration below to be used as the identifier for the user in braze. at least one of external user id, braze id, email, phone, or the combination of user alias label and user alias field must be set in the configuration below to be used as the identifier for the user in braze. updated 5 months ago scraped from: https:docs.lytics.comdocsdeveloperquickstart3installlytics 1. install the lytics tag quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy in the web sdk, lytics provides a tag that can be placed on your site to collect behavioral data and surface the materialized profile back to your browser in real time. manual installation instructions are available from within the app at data pipeline sdk web sdk. if your site is a drupal or wordpress site, or you use google tag manager we recommend using one of these turnkey methods to install the lytics tag: visit the configuration page either from the module details in the extend section of drupal or directly from the configuration section enadminconfigsystemlytics. enadminconfigsystemlytics add a valid api token for your lytics account. be sure to give that token admin privileges, as that will unlock account verification and direct access to all lytics systems responsible for surfacing and managing profiles and personalization. this token should also have no expiration, as it will need continued access to your lytics account. for more information visit our access token documentation. with your token entered. ensure the enable tab setting is selected. this will automatically place the lytics javascript sdk onto all public pages of your site. upon saving configuration, your account details will be surfaced. be sure to verify that your token is for the lytics account you are currently configuring. thats it! once the tag has been installed, validate a successful installation via one of the three following methods: the lytics dev tools chrome extension makes validation and exploration simple. once youve installed the extension as outlined above: open the extension and ensure it has been activated. the interface will display a message to confirm the correct installation of the lytics tag and relevant configuration details. weve built a simple tool to validate that data flows into your lytics account. please note that the collected events may take a few minutes to reflect in the ui. if you prefer to verify the installation manually, you can do so by opening the chrome developer console and ensuring jstag is accessible. in addition, by viewing the network tab, you can monitor data collection requests being sent tolytics.ioc and personalization requests being loaded from lytics.ioapipersonalize jstag lytics.ioc lytics.ioapipersonalize the final step to verify installation is ensuring you can access your visitor profile. this profile is built and delivered in realtime as you engage with content. the lytics dev tools chrome extension makes validation and exploration simple. once youve installed the extension as outlined above: open the extension and ensure it has been activated. access profile from the bottom menu in the chrome extension. this section of the extension profiles two views into your profile. the first is the summary showcasing available attributes and affinities. the second is the details, which profiles a raw json dump of all available attributes and insights accessible via the personalization engine web sdk. if you prefer to verify the installation manually, you can do so by opening the chrome developer console and ensuring jstag is accessible. in addition, by viewing the network tab, you can monitor data collection requests being sent tolytics.ioc and personalization requests being loaded from lytics.ioapipersonalize jstag lytics.ioc lytics.ioapipersonalize updated 5 months ago scraped from: https:docs.lytics.comdocsdeveloperquickstart2contentsetup 2. content setup quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy lytics interest engines effectively analyze your content with minimal effort required. however, to guarantee that the classification results align with your expectations, conducting a quick test is advisable, as poor metadata can sometimes lead to lessthanoptimal outcomes. the most reliable method to ensure comprehensive and satisfactory content classification is to classify a selection of your urls manually. updated 5 months ago scraped from: https:docs.lytics.comdocsdeveloperquickstart4personalizedmessage 3. surface personalized message quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy the lytics personalization engine profiles realtime access to a comprehensive visitor profile. before we jump into the weeds of how you can fully build and leverage this profile, lets create our first experience. lytics comes with our personalization sdk called pathfora. pathfora allows you to easily surface simple lead capture and messaging modals or content directly inline. full documentation for pathfora is available, but initially, lets surface a welcome message to our anonymous visitor audience. jstag profiles a helper function to ensure that the pathfora library has been loaded before triggering the experience. jstag.onpathfora.publish.done, functiontopic, event here we initialize a new pathfora message experience var module new pathfora.message id: samplemessagecampaign, this value will be collected along side all interactions and used in reporting layout: slideout, for layout well use a small slide out position: bottomleft, the model will enter and sit at the bottom left theme: dark, css can be customized to brand but well use the default dark theme headline: hello world!, this will be the headline of our message msg: congratulations on setting up your first targetted campaign using the lytics personalization engine!, the body of the message ; var modules target: segment: anonymousprofiles, target only visitors with the anonymousprofile attribute widgets: module ; pathfora.initializewidgetsmodules; initialize the campaign ; alter the pathfora configuration to your liking. install the pathfora configuration onto your site via your preferred tag management method. refresh the page and be greeted with your new welcome message targeted at anonymous visitors! lytics and pathfora provide a great deal of flexibility. if you are ready to dive deeper, please explore some of our other popular use cases: updated 6 months ago scraped from: https:docs.lytics.comdocsdeveloperattributes default attributes quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy lytics offers a wide range of prepackaged user attributes, including automatically generated and customizable ones. additionally, lytics employs predictive modeling and machine learning algorithms to provide insights and scores, allowing users to gain a deeper understanding of their audience. the guide below provides an overview of all available attributes and examples to enhance your profiling efforts. if you havent already reviewed our documentation on collecting events via our javascript sdk, we highly recommend doing so first. this will give you a better understanding of how data collection works at a high level before delving into the specifics of what can be collected. the following attributes are all available out of the box with no customization necessary in all lytics pricing tiers. do note that any attributes flagged as computed can not be edited directly but are computed based on various factors, including other noncomputed attributes. for a more comprehensive example of how any of the following attributes can be collected and used for your visitors click the name of the identifier to access the code examples below. identifiers default attributes that are used to stitch profiles together. for instance, if you pass an email along with the uid, all events that have only been associated with either identifier will be merged into a single comprehensive profile. id uid uids email email protected uuid details details encompass all default attributes related to user demographics and general information, including name, phone number, status, etc. it serves as a catchall for attributes not specifically tied to interactions or behaviors. name firstname lastname title phone cell origin language age companies gender status userattributes timezone city country state meta meta encompasses all systemlevel information that provides insights into the health and breadth of the profile. this includes data such as creation date, last update timestamp, source information, and other metadata associated with the profiles management and maintenance. metadata offers a behindthescenes view of the profiles overall status and administration. created lastscored modified numaliases numdays numevents numstreams streamnames isbot behavior behavioral attributes typically cannot be directly managed but represent a set of insights derived from a users behavior over time. these insights are invaluable when personalizing experiences based on changes in behavior or behaviors indicative of high likelihood. for instance, you might want to present a premium offer to users exhibiting higher momentum than usual. behavioral attributes enable targeted and timely interventions tailored to user actions and patterns. scoreconsistency scorefrequency scoreintensity scorematurity scoremomentum scorepropensity scorequantity scorerecency scorevolatility interests interests entail understanding the topics a user is interested in based on their interactions, crossreferenced by deep programmatic analysis of their online activities. this allows for tailored content recommendations and targeted messaging aligned with the users preferences and engagement history. lyticscontent intelligence attributes classified as intelligence encompass diverse, highly valuable information to facilitate relevant and highvalue personalized experiences. within this category, youll discover realtime segment membership, values crucial for split testing and experimentation, and direct correlation to our realtime machine learning modeling. these attributes empower dynamic and datadriven decisionmaking, enhancing the efficacy of personalized marketing strategies. segments split split2 needsmessage nextevent segmentprediction segmentpredictionpercentile activity activity encompasses the users engagement across different channels and campaigns, including clicks and conversions. it provides valuable insights into recent interactions, aiding campaign optimization and channel effectiveness assessment. general eventfirstseen eventlastseen channels devices hourly hourofweek lastactivets lastchannelactivities web domains firstvisitts lastvisitts pageviewct refdomain useragent visitct visitcity visitcountry visitregion formdata formssubmitted utmcampaignlast utmcampaigns utmcontentlast utmcontents utmmediumlast utmmediums utmsourcelast utmsources utmtermlast utmterms campaign lyhover lyimpressions lycloses lyconversions lymilestones lygoals identifiers id this is an automatically generated canonical id managed by lytics. it refers to the materialized profile and cannot be customized or overridden. uid uids uid represents the lytics anonymous 1st party cookie. this value is automatically captured with every jstag.send call from the javascript tag. the only way to customize this value is to explicitly set the value of uid, which we do not recommend. jstag.send jstag.setidsomecustomvalue; jstag.send; email jstag.send email:email protected ; uuid jstag.send uuid:someuniqueuserid ; details firstname jstag.send firstname:john, ; lastname jstag.send lastname:doe, ; title jstag.send title:president, ; phone jstag.send phone:5555555555, ; cell jstag.send cell:5555555555, ; origin jstag.send origin:loyalty2022, ; language by default, the lytics javascript sdk will collect language information based on the browser, but this can be overridden. jstag.send ul:enus, ; age jstag.send age:25, ; companies jstag.send companies:lytics, pantheon, ; gender jstag.send gender: na, meta created lytics automatically generate this and represents the oldest event associated with the user. modified this is automatically generated by lytics and represents the last time the user was modified. lastscored this is automatically generated by lytics and represents the last time the users scores were updated. numaliases this is automatically generated by lytics and represents the number of aliases associated with the user. numdays this is automatically generated by lytics and represents the number of days the user has existed. numevents this is automatically generated by lytics and represents the number of events associated with the user. numstreams this is automatically generated by lytics and represents the number of streams associated with the user. streamnames this is automatically generated by lytics and represents the names of the streams associated with the user. isbot this is automatically generated by lytics and represents whether the user has been flagged as a bot or not. behavior the following attributes are all computed in realtime as the profile evolves. each of the behavioral attributes are surfaced as a score between 0 and 100. these scores represent an aggregate summary of the users behavior across various dimensions: consistency, frequency, intensity, maturity, momentum, propensity, quantity, recency, and volatility. interests lyticscontent the interest attributes are computed in realtime and represent the users interest in various topics. these topics are generated as a result of the analysis done by the lytics interest engine and then associated with the user based upon their interaction with content on your site. intelligence segments this attribute displays an array of all segments the user is currently a member of. it updates in realtime based on various audience definitions. lytics offers a range of useful segments out of the box, requiring no additional setup. for detailed information on these audiences, refer to our developer tier audiences documentation. split split2 these attributes are automatically generated by lytics and represent a random value evenly distributed across users. they are useful for split testing and experimentation. needsmessage this attribute is computed in realtime and represents the relative distance between now and the next predicted event. it is stream specific and is useful for understanding when a user is likely to engage again. nextevent this attribute is computed in realtime and represents the next expected event. it is stream specific and is useful for understanding when a user is likely to engage again. segmentprediction segmentpredictionpercentile this attribute is computed in realtime and represents the scores resulting from lytics lookalike and segmentml models. outofthebox, lytics offers a range of useful models, requiring no additional setup. for detailed information on these models, refer to our developer tier models documentation. activity general eventfirstseen eventlastseen both of these attributes are automatically populated based upon the e value in the jstag.send payload. by default lytics will collect a pv event for each page view and this will automatically populate the firstseen and lastseen attributes. below is an example of collecting a custom event that would populate these attributes as well. e jstag.send pv firstseen lastseen jstag.send e:customevent ; channels jstag.send channel:web, ; devices jstag.send device:desktop, ; hourly this attribute is automatically populated with a count of events per hour for the user. hourofweek this attribute is automatically populated with a count of events per hour of the week for the user. lastactivets this attribute is automatically populated with the last time an event was received in any stream for the user. lastchannelactivities jstag.send channel:web, ; web domains this attribute is automatically populated with the domains the user has been active on. firstvisitts this attribute is automatically populated with the first time the user visited the site and sends data to the default stream. default lastvisitts this attribute is automatically populated with the last time the user visited the site and sends data to the default stream. default pageviewct this attribute is automatically populated with the number of pv events recieved for the user. pv jstag.send e:pv ; refdomain this attribute is automatically populated with the referring domain for the user. jstag.send ref:umami.lytics.com, ; useragent this attribute is automatically populated based on the user agent of the browser. this attribute must be turned on in your lytics account to be collected. visitct this attribute is automatically populated with the number of visits the user has had based on presence of the sesstart key in an event. sesstart jstag.send sesstart:1 ; visitcity this attribute is automatically populated with the city the user visited from based upon geoip. visitcountry this attribute is automatically populated with the country the user visited from based upon geoip. visitregion this attribute is automatically populated with the region the user visited from based upon geoip. formdata form data is a wildcard attribute that allows you to pass a number of key value pairs that all get stored under the formdata attribute. this is useful for capturing form submissions. formdata jstag.send formdatafn:john, formdataln:doe, formdatasomeotherkey:somevalue ; formssubmitted jstag.send formname:newsletter ; utmcampaignlast jstag.send utmcampaign:holiday ; utmcampaigns jstag.send utmcampaign:holiday ; utmcontentlast jstag.send utmcontent:recipe1 ; utmcontents jstag.send utmcontent:recipe1 ; utmmediumlast jstag.send utmmedium:article ; utmmediums jstag.send utmmedium:article ; utmsourcelast jstag.send utmsource:googleads ; utmsources jstag.send utmsource:googleads ; utmtermlast jstag.send utmterm:example ; utmterms jstag.send utmterm:example ; campaign lyhover jstag.send pfwidgetid: contentrecmodal, pfwidgetevent: hover ; lyimpressions jstag.send pfwidgetid: contentrecmodal, pfwidgetevent: show ; lycloses jstag.send pfwidgetid: contentrecmodal, pfwidgetevent: close ; lyconversions jstag.send pfwidgetid: contentrecmodal, pfwidgetevent: conversion ; lymilestones jstag.send pfwidgetid: engageddonationpage, pfwidgetevent: milestone ; lygoals jstag.send pfwidgetid: madedonation, pfwidgetevent: goal ; updated 12 months ago scraped from: https:docs.lytics.comdocsdevelopersegments default segments quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy categorizing users based on their behaviors and characteristics is pivotal for effective audience targeting and personalization strategies. lytics audience segmentation offers a powerful tool for organizing users into meaningful groups based on shared attributes or behaviors. this section provides a comprehensive overview of all audience segments that are provided outofthebox. lytics audience segments apply predefined rules to each user profile as they update. membership in these segments is maintained in realtime and can trigger subsequent actions when users enter or exit the segment. the following audience segments are all available out of the box, with no customization necessary in all lytics pricing tiers. all filter anonymousprofiles30days filter and modified now30d, numaliases 1, exists uids from user alias anonymousprofiles30days anonymousprofiles60days filter and modified now60d, numaliases 1, exists uids from user alias anonymousprofiles60days anonymousprofiles90days filter and modified now90d, numaliases 1, exists uids from user alias anonymousprofiles90days anonymousprofiles filter and numaliases 1, exists uids from user alias defaultanonseg connectedcustomersegment filter forceinvalidsegmentfield 1 from user alias connectedcustomersegment defaultunhealthyprofiles filter profileprocessingfailure true from user alias defaultunhealthyprofiles lyatrisk filter and scoremomentum 10, scoremomentum 30 lybingeuser filter and scorefrequency 50 lycasualvisitor filter scoreintensity 25 lydeeplyengagedusers filter scoreintensity 75 lyfirsttimevisitor filter visitct 1 lyfrequentuser filter scorefrequency 65 lyfromemail filter utmmediums intersects email lyfrompaid filter utmmediums intersects cpc, ppc lyfromsocial filter utmmediums intersects social, twitter, facebook, pinterest, instagram lyhasvisitedmobileweb filter exists ismobile lyhasvisitedweb filter channels intersects web lyinfrequentuser filter scorefrequency 35 lyinternationalvisitor filter and exists visitcountry, visitcountry not in us lyknownemail filter exists email lymoderatelyengagedvisitor filter and scoreintensity 24, scoreintensity 76 lymoderatelyfrequentuser filter and scorefrequency 34, scorefrequency 76 lymultisessionvisitor filter visitct 1 lyperuser filter and scorefrequency 70, scoreintensity 20 lyrepeatvisitor filter visitct 1 lyreportingcasualvisitors filter scoreintensity 25 lyreportingdeeplyengagedusers filter scoreintensity 75 lyreportingfrequentusers filter scorefrequency 65 lyreportingfromemail filter utmmediums intersects email lyreportingfromfacebook filter utmsources intersects facebook, facebook lyreportingfromgoogle filter utmsources intersects google, google search, google ads, gads, google, googlesearch lyreportingfrompaid filter utmmediums intersects cpc, ppc lyreportingfromsocial filter utmmediums intersects social, twitter, facebook, pinterest, instagram lyreportinghasvisitedmobileweb filter exists ismobile lyreportinghasvisitedweb filter channels intersects web lyreportinginfrequentusers filter scorefrequency 35 lyreportinglastvisitwithin3months filter lastvisitts now3m lyreportinglastvisitwithinday filter lastvisitts now1d lyreportinglastvisitwithinmonth filter lastvisitts now1m lyreportinglastvisitwithinweek filter lastvisitts now1w lyreportingmultisessionvisitor filter visitct 1 lyreportingsinglepagevisitor filter pageviewct 1 lysinglepagevisitor filter pageviewct 1 lyunknownemail filter not exists email lyusvisitor filter visitcountry in us lyusesandroid filter devices intersects android lyusesdesktop filter devices intersects desktop lyusesios filter devices intersects ios lyusesmobile filter devices intersects android, blackberry, ios, winmobile lyusesother filter devices intersects blackberry, winmobile, other smtactive filter and scoremomentum 10, exists scoremomentum, 50, scorefrequency 50, scoreintensity 25, scoremomentum 40 from user smtdormant filter and and scorefrequency 5, scoreintensity 0, scoremomentum 0, scorequantity 3 , exists scoremomentum, now1w from user smtinactive filter and scoremomentum 10, exists scoremomentum, now1w, not and scorefrequency 5, scoreintensity 0, scoremomentum 0, scorequantity 3 from user smtnew filter now1w from user smtpower filter and and scorequantity 50, scorefrequency 50, scoreintensity 25, scoremomentum 40 , exists scoremomentum, 10 from user smtunscored filter and not exists scoremomentum, created now1w updated 7 months ago scraped from: https:docs.lytics.comdocsleadcapture lead capture quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy this document outlines how to leverage lytics realtime personalization engine to identify visitors who lack the strong identifiers necessary to link webbased behavioral insights with other activation channels such as email. what is lead capture? traditionally, lead capture involves gathering information from individuals interested in your products or services, such as names, email addresses, and phone numbers. this occurs when people visit your website, subscribe to your newsletter, download resources, or interact with your content. however, in recent years, the role of lead capture within your global consumer engagement strategy has undergone significant transformation. why is it important? lytics realtime personalization engine lets you first identify the current web visitor, programmatically review their profile, and ultimately decide if a campaign should be delivered to their browser to close gaps in that profile. below, youll find a detailed guide on how to execute this use case. at the end, youll find additional reading, which will help you learn how to extend and customize this use case further. requirements install lytics core sdk before executing this use case, your website must be properly configured with the core lytics sdk javascript tag. please refer to our installation documentation to confirm the tag is present before continuing. activate required attributes as an admin user of lytics, you can enable or disable any attributes from being surfaced to the web via our javascript sdk. depending on how your account has been configured, you may need to allow some of the required attributes for this particular use case. required attributes: configure web experience to execute this use case, we will launch a simple lead capture form at the bottom left of your site. the lead capture will focus on collecting email addresses for the user and use a generic dark theme along with some boilerplate messaging. for best results, we recommend leveraging an offer of some sort that will resonate with your customers to entice them to sign up and identify themselves. in return, youll gain much knowledge about this visitor, strengthen your consumer record, and unlock the ability to connect in channels outside of just the web. jstag.onpathfora.publish.done, functiontopic, event var module new pathfora.form id: leadcaptureform, layout: slideout, theme: dark, headline: sign up!, msg: submit this form to get updates, formelements: type: email, required: true, label: email, name: email ; var modules target: segment: anonymousprofiles, widgets: module ; pathfora.initializewidgetsmodules; ; bonus idea you can extend this use case to progressively improve your visitor profiles by surfacing data capture forms specific to gaps in their profile data. for instance, when they are unknown, only ask for email. as soon as you have an email, perhaps you want to know more about their preferences or learn how to improve a particular portion of your productoffering. configure reporting the lytics platform has a predefined schema to facilitate this use case. as such, we recommend building a custom report from within our ui to showcase your progress in converting unknown or anonymous visitors to known visitors with a strong identifier. there are many ways to configure this type of reporting, but here well guide you through a basic example to measure impact. create new report add anonymous vs. known size component step 1: once you have created your new report, add a component using the add new component button at the top of the report. step 2: select size as your component type. step 3: name your component. we recommend something simple like anonymous vs. known. step 4: add an optional description to explain the goal of this report to ease the consumption for others later. step 5: select the audiences youd like to highlight. in this use case, well select anonymous profiles and known profiles, which are both available by default. step 6: save your component. please note it may take some time for the report to populate once you have configured it, but as long as you see the empty not enough data state, you are good to go. add impression composition component step 1: following steps similar to the previous component, well add another component, but this time, we use the composition component type. step 2: name your component campaign impressions, as this component will focus on showcasing impression information to validate that the campaign is being shown. step 3: select the all audience to ensure all visitors, regardless if they are known or unknown, are included in the report. step 4: select the field lyimpressions. this field represents an array of all campaigns each visitor has interacted with and is available by default. the values returned for custom campaigns will match the id you set above in the campaign configuration. step 5: finally, replace mycampaignid in the example below with the id of your campaign above to highlight only interactions for that campaign. add conversion composition component following the same steps as above, configure another composition component, but this time, use the field lyconversions as it will be an array of all campaigns your users have converted on instead of the impressions used previously. thats it. your report has been configured. as you begin to gain impressions and conversions, this report will populate. generally, every hour or two, to highlight the impact. bonus! get additional guidance on how to pass interaction information related to your campaign to the following common sources for additional reporting and insight discovery: execute your experience once you have configured and tested your experience, activation is as simple as publishing the javascript code to your site. this may be done via a tag manager like google tag manager or using another mechanism such as drupal blocks or wordpress. updated 12 months ago scraped from: https:docs.lytics.comdocsguidecontentrecommendations content recommendations quick start account management key concepts pipeline profiles warehouse access audiences activation tutorials sdks tools integrations partners legacy this document details the process of utilizing lytics realtime personalization engine to showcase content that resonates with a visitors interests. this can be implemented directly inline or through a popup modal on your website to boost engagement. how does lytics make content recommendations? lytics interest engines provides a sophisticated approach for businesses to gain and leverage insights into which content is most likely to resonate based on behavioral patterns. this is achieved through a detailed enrichment and affinity generation process in realtime. while full details are available in lytics comprehensive documentation, the core process can be distilled into four key steps: this streamlined framework enables realtime personalization, enhancing user experience by continuously adapting to evolving visitor interests. why is it important? relevant content for individual users boosts marketing effectiveness and roi by increasing engagement and conversion rates. personalized content meets users interests, leading to better resource utilization and higher customer retention. this targeted strategy enhances sales opportunities and brand perception, directly improving return on investment. lytics realtime personalization engine lets you first identify the current web visitor, programmatically review their profile, and deliver one or more pieces of content based on their unique interests to maximize engagement. below, youll find a detailed guide on how to execute this use case. at the end, youll find additional reading, which will help you learn how to extend and customize this use case further. requirements install lytics core sdk before executing this use case, your website must be properly configured with the core lytics sdk javascript tag. please refer to our installation documentation to confirm the tag is present before continuing. interest scores content collections your content must be classified before lytics can offer content recommendations or insights. typically, this classification process takes 24 to 48 hours, though it may extend if your catalog is particularly large. to ensure the health of your content graph and interest engines, there are a few key areas to focus on: verify interest scores on profile verify content has been classified if you have not already verified the quality of classification and metadata, please refer to our quickstart documentation. from the lytics interface, navigate to decision engine from the product switcher at the top left. using the lefthand menu, navigate to content collections. from the list of collections, there is likely only one; look for all content in the list and the rows size. this refers to the number of documents classified; it should be greater than one and reflect your total catalog. for greater detail, you can also visit content web classification. at the top of that page will be a summary dashboard of all content that has been successfully classified and cataloged. most notably, the all documents count under the url path component. select a content collection. we will use the default collection outlined below for this exercise, but you may also create a custom collection with a subset of your content to recommend. ready to go a step farther and build a custom collection of your content to ensure recommendations are selected from a more currated set of documents? check out our full documentation. activate required attributes segments as an admin user of lytics, you can enable or disable any attributes from being surfaced to the web via our javascript sdk. depending on how your account has been configured, you may need to allow some of the required attributes for this particular use case. required attributes: configure web experience to execute this use case, we will demonstrate two examples. the first will be a single content recommendation in a popup style modal, while the second will leverage pathfora to add a set of recommendations inline to your website. example 1: content recommendation modal jstag.onpathfora.publish.done, functiontopic, event var module new pathfora.message id: contentrecsample, layout: slideout, theme: dark, headline: yummy content!, msg: we suspect you are going to want to check this out., recommend: collection: allcontent , cancelshow: false, okshow: false, variant: 3 ; var modules target: segment: all, widgets: module ; pathfora.initializewidgetsmodules; ; jstag.onpathfora.publish.done, functiontopic, event var module new pathfora.message id: contentrecsample, layout: slideout, theme: dark, headline: yummy content!, msg: we suspect you are going to want to check this out., recommend: collection: allcontent , cancelshow: false, okshow: false, variant: 3 this variant shows the title and image for the recommendations ; var modules target: segment: all, widgets: module ; pathfora.initializewidgetsmodules; ; example 2: inline content recommendation in this example, well use a generic drupal block with pure html and css, but the same approach can be taken using wordpress or a thirdparty tag manager such as google tag manager. .reccontainer display: flex; justifycontent: spacebetween; width: 100; .recitem boxsizing: borderbox; flex: 1; padding: 10px; .rectitle, .recdescription fontsize: 14px; marginbottom: 5px; wordwrap: breakword; .recdescription fontsize: 12px; .recimg height: auto; marginbottom: 5px; maxwidth: 100; recommendation title loading... the recommendation description is loading... recommendation title loading... the recommendation description is loading... recommendation title loading... the recommendation description is loading... updated 12 months ago"
}